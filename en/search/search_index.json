{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>EIPL (Embodied Intelligence with Deep Predictive Learning) is a library for robot motion generation using deep predictive learning developed at Tetsuya Ogata Laboratory, Waseda University. Deep predictive learning is a method that enables flexible motion generation for unlearned environments and work targets by predicting in real time the appropriate motion for the real world based on past learning experience. In this study, using the smart robot AIREC (AIREC\uff1aAI-driven Robot for Embrace and Care) andOpen Manpulator as real robots, it is possible to learn systematically from model implementation to learning and real-time motion generation using collected visual and joint angle data. The EIPL system enables users to learn systematically from model implementation to learning and real-time motion generation using the collected visual and joint angle data. In the future, newly developed motion generation models using EIPL will be released on Model Zoo. The following is an overview of each chapter.</p> <ol> <li> <p>Deep Predictive Learning</p> <p>The concept of deep predictive learning and the three steps toward robot implementation, motion teaching, learning, and motion generation, are described.</p> </li> <li> <p>Setup</p> <p>Describes how to install EIPL and check the program using pre-trained weights.</p> </li> <li> <p>Motion Teaching</p> <p>Describes how to extract data from ROSbag data and create dataset. EIPL provides an object grasping motion data set using AIREC as sample data.</p> </li> <li> <p>Train Model</p> <p>Using the motion generation model with attention mechanism as an example, describes a series of implementation methods from model training to inference.</p> </li> <li> <p>Real Robot Application</p> <p>Describes a series of procedures from motion teaching to real robot control using Open Manpulator.</p> </li> <li> <p>ModelZoo</p> <p>The motion generation models developed using EIPL will be released in a phased manner.</p> </li> <li> <p>Tips</p> <p>Describes the know-how of motion learning.</p> </li> </ol> <p>Acknowledgements</p> <p>This work was supported by JST Moonshot-type R&amp;D Project JPMJMS2031. We would like to express our gratitude.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#q-motion-is-not-smoothly","title":"Q. Motion is not smoothly.","text":"<ol> <li> <p>Mixing predictive data     In order to generate stable/smooth motion against real-world noise, the sensor information at time $t$ is mixed with the predicted value of RNN at the previous time $t-1$ in a specific ratio and then input to RNN.     This process is equivalent to a low-pass filter, and even if the robot's sensor values are noisy, the predicted values from the previous time can be used as a supplement to predict stable motion commands.     Note that if the mixing factor (<code>input_param</code>) is too small, it becomes difficult to modify the motion based on real-world sensor information, and the robustness against position changes decreases.     If <code>input_param=0.0</code>, the motion will be generated using only the sensor information acquired at the initial time.     The following is an example implementation, in which data is mixed with the robot's camera image and joint angles.</p> <pre><code>x_image, x_joint = robot.get_sensor_data()\nif loop_ct &gt; 1:\nx_image = x_image * input_param + y_image * (1.0-input_param)\nx_joint = x_joint * input_param + y_joint * (1.0-input_param)\ny_image , y_joint, state = mode(x_image, x_joint, state)\n</code></pre> </li> </ol>"},{"location":"faq/#q-predicted-image-is-abnormal","title":"Q. Predicted image is abnormal","text":"<ol> <li>Fix camera parameters     When the trained model is applied to a real robot, the problem occurs that no objects are seen in the predicted image or the predicted image is noisy.     This may be due to the fact that camera parameters (e.g. white balance) are automatically changed, fix the camera parameters at the time of <code>motion teaching</code>.     Or adjust the <code>inference</code> camera parameters to get the same visual image as the <code>motion teaching</code>.</li> </ol>"},{"location":"faq/#q-not-focusing-attention-on-the-object","title":"Q. Not focusing attention on the object.","text":"<ol> <li> <p>Adjusting the camera position</p> <p>It is recommended that the robot's body (hand or arm) and the object be constantly displayed in the image in order to acquire stable attention and motion. When attention is directed to the robot's body and the object part, it is easier to learn the time-series relationship between the both.</p> </li> <li> <p>Enlarge the object</p> <p>Therefore, either make the object physically larger, crop the image around the object, or move the camera closer to the object.</p> </li> <li> <p>Re-training the mode     The initial weights of the model may cause the objects to be inattentive.     Training multiple times with the same parameters yields good results 3 out of 5 times.</p> </li> </ol>"},{"location":"faq/#q-customize-the-data-loader","title":"Q. Customize the data loader","text":"<p>It is possible to add/remove any sensor by changing the number of data to pass the <code>MultimodalDataset</code> class or by changing some of the input/output definitions. The following shows an example of adding a new torque sensor.</p> <pre><code>class MultimodalDataset(Dataset):\ndef __init__(self, images, joints, torque, stdev=0.02):\npass\ndef __getitem__(self, idx):\nreturn [[x_img, x_joint, x_torque], [y_img, y_joint, y_torque]]\n</code></pre>"},{"location":"license/","title":"License","text":"<p>This software, EIPL (Embodied Intelligence with Deep Predictive Learning), is licensed for redistribution and/or modification under the terms of the  GNU Affero General Public License version 3: GNU AGPLv3 published by the Free Software Foundation.</p>"},{"location":"overview/","title":"Deep Predictive Learning","text":""},{"location":"overview/#dpl-overview","title":"Overview","text":"<p>Deep Predictive Learning (Deep Predictive Learning) is a robot motion generation method developed with reference to the free energy principle, which unifies and explains the various functions of the brain1. A recurrent coupled neural network (RNN) model is trained to minimize the prediction error between the sensory-motor information at time (t) and the next time (t+1) using the time-series information of the robot's motion and sensation when the robot experiences motion in the real world using teleoperation, etc. At runtime, it is possible to predict the robot's near-future sensations and actions in real time from its sensory-motor information, and to execute actions such that the resulting prediction error is minimized by the attracting action of attractors in the RNN. The figure below shows a robot implementation of deep predictive learning, which consists of three steps: motion teaching, learning, and motion generation.</p> <p></p>"},{"location":"overview/#dpl-teach","title":"Motion Teaching","text":"<p>In deep predictive learning, motion generation models are acquired in a data-driven manner using the robot's sensorimotor information (time-series data comprising motion and sensor information) as training data. Therefore, the training data must contain information about the interaction between the robot's body and the environment. In addition, since the quantity and quality of the training data influence model performance, high-quality demonstration data must be collected efficiently. </p> <p>In Phase 1, the training data is collected by performing the desired motions on the robot and recording motion information, such as joint angles, and sensor information, such as camera images, at a constant sampling rate. Typical teaching methods for robot motion include program description2, direct teaching3, and teleoperation1. Although describing the robot's motion in advance using a robot programming language is simple, it may not be feasible due to the complexity of the description when a robot needs to perform a long-term motion. In contrast, training data without precise modeling and parameter adjustment can be obtained by teaching movements through human manipulation of the robot. Among them, remotely controlling the manipulator from the actual robot's perspective (Wizard of Oz4) is desirable because it can intuitively teach the robot human manipulation skills for a task. The operator interacts naturally with the environment by controlling the robot as if he were controlling his body. In addition, since the operator makes decisions about actions based on information obtained from sensor information, the acquired teaching dataset is expected to contain information necessary for motion learning and be effective in acquiring data for model training.</p>"},{"location":"overview/#dpl-train","title":"Training","text":"<p>In deep predictive learning, the learning target is the time-series relationships between sensorimotor information in a system in which the environment and the body interact dynamically. The training data are not labeled with correct answers, and the model is trained to predict the robot state ($\\hat i_t, \\hat s_{t+1}$) in the next step using the current robot state as input ($i_t, s_t$). This autoregressive learning eliminates the need for the detailed design of a physical model of the environment, as required in conventional robotics. In addition, the model can be represented as a dynamic system integrating environment recognition and motion generation functions across multiple modalities.</p> <p>The model consists of feature extraction and time series learning parts to learn the robot's sensorimotor information. The feature extraction part extracts features from sensor information values acquired by the robot, and the time series learning part learns sensorimotor information that integrates the extracted features and robot motion information ( e.g., joint angles and torque). Although each part is connected end-to-end (CNNRNN, SARNN) or learned independently (CAE-RNN), the roles of each part are explicitly separated in this manual.</p>"},{"location":"overview/#dpl-execute","title":"Motion Generation","text":"<p>When performing the task, RNN performs three processes sequentially: (1) Acquire sensor information from the robot, (2) predict the next state based on the sensor information, and (3) send control commands to the robot based on the predicted values. By performing the forward computation of the model at each step, RNN predicts the robot's state for the next time-step based on the context information and inputs it holds internally. The RNN output is then used as the target state to control each joint. By repeating the above process online, the RNN predicts the sensorimotor motion of the robot while sequentially changing the state of each neuron in the context layer. Based on the results of this prediction and the prediction error in the real environment, the robot generates motions that dynamically respond to the input.</p> <p>Another advantage of using deep learning models for motion generation is the online motion generation speed. The proposed framework comprises lightweight models, and the computational time and cost required for motion generation are low. Moreover, implementing each of the previous functions as components makes it possible to easily reuse the implemented system when tasks or robot hardware changes, or devices are added5.</p> <ol> <li> <p>Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Efficient multitask learning with an embodied predictive model for door opening and entry with whole-body control. Science Robotics, 7(65):eaax8177, 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kanata Suzuki, Momomi Kanamura, Yuki Suga, Hiroki Mori, and Tetsuya Ogata. In-air knotting of rope using dual-arm robot based on deep learning. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 6724\u20136731. IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Hideyuki Ichiwara, Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Contact-rich manipulation of a flexible object based on deep predictive learning using vision and tactility. In 2022 International Conference on Robotics and Automation (ICRA), 5375\u20135381. IEEE, 2022.\u00a0\u21a9</p> </li> <li> <p>Pin-Chu Yang, Kazuma Sasaki, Kanata Suzuki, Kei Kase, Shigeki Sugano, and Tetsuya Ogata. Repeatable folding task by humanoid robot worker using deep learning. IEEE Robotics and Automation Letters, 2(2):397\u2013403, 2016.\u00a0\u21a9</p> </li> <li> <p>Momomi Kanamura, Kanata Suzuki, Yuki Suga, and Tetsuya Ogata. Development of a basic educational kit for robotic system with deep neural networks. Sensors, 21(11):3804, 2021.\u00a0\u21a9</p> </li> </ol>"},{"location":"install/install-software/","title":"Overview","text":"<p>EPIL officially supports Linux (Ubuntu 20.04) and Python 3.8. Pytorch is used as the deep learning framework and it is recommended to install the latest version of pytorch. In particular, Pytorch 2.0 and above can perform training of large models faster because it is precompiled, which improves training speed and reduces GPU memory usage. Note that CUDA and Nvidia drivers must be installed according to the version of Pytorch used.</p>"},{"location":"install/install-software/#files","title":"Files","text":"<p>This library is composed of the following:</p> <ul> <li>data: A sample data downloader and a Dataloader for model training are implemented.</li> <li>layer: A layered model (Hierarchical RNNs, spatial attention mechanisms, etc.) are implemented.</li> <li>model: Multiple motion generation models are implemented, and inputs support joint angles (arbitrary degrees of freedom) and color images (128x128 pixels).</li> <li>test: Test programs.</li> <li>utils: Functions for normalization, visualization, arguments processing, etc.</li> </ul>"},{"location":"install/install-software/#pip_install","title":"Install from pip","text":"<p>Clone the EIPL repository from Github and install the environment using the pip command.</p> <pre><code>mkdir ~/work/\ncd ~/work/\ngit clone https://github.com/ogata-lab/eipl.git\ncd eipl\npip install -r requirements.txt\npip install -e .\n</code></pre>"},{"location":"install/install-software/#docker","title":"docker","text":"<p>Note</p> <p>Coming soon.</p>"},{"location":"install/quick-start/","title":"Quick Start","text":"<p>In this section, a test program will be run using the pre-trained weights and the motion generation model with spatial attention mechanism (SARNN: Spatial Attention with Recurrent Neural Network) to verify that the EIPL environment has been properly installed.  Please refer to the next and subsequent chapters for specific model training methods and model details.</p>"},{"location":"install/quick-start/#inference","title":"Inference","text":"<p>The following shows how to infer SARNN using the pre-trained weights and running <code>test.py</code> in the tutorial folder will save the inference results in the output folder. At this time, by specifying the --pretrained argument, the pre-trained weights and sample data are automatically downloaded.</p> <pre><code>$ cd eipl/tutorials/sarnn\n$ python3 ./bin/test.py --pretrained\n$ ls ./output/\nSARNN_20230514_2312_17_4_1.0.gif\n</code></pre>"},{"location":"install/quick-start/#results","title":"Results","text":"<p>The following figure shows the result of inference. The blue dots in the figure are the points of attention extracted from the CNN (Convolutional Neural Network), and the red dots are the points of attention predicted by the RNN (Recurrent Neural Network), indicating that the joint angles are predicted while focusing on the robot hand and grasped object.</p> <p></p>"},{"location":"install/quick-start/#help","title":"Help","text":"<p>If an error occurs, there are three possible causes:</p> <ol> <li> <p>Installation error</p> <p>Since the libraries may not be properly installed, use the <code>pip freeze</code> command to verify that they are installed. If the library is installed, its version information will be displayed. If not, the package may not have been installed, so please double check the installation procedure.</p> <pre><code>pip freeze | grep eipl\n</code></pre> </li> <li> <p>Download error</p> <p>If you are unable to perform inference using sample data or trained models due to a proxy or other reason, manually download the weights file and dataset, save them in the <code>~/.eipl/</code> folder, and then extract them.</p> <pre><code>$ cd ~/\n$ mkdir .eipl\n$ cd .eipl\n$ # copy grasp_bottle.tar and pretrained.tar to ~/.eipl/ directory\n$ tar xvf grasp_bottle.tar &amp;&amp; tar xvf pretrained.tar\n$ ls grasp_bottle/*\ngrasp_bottle/joint_bounds.npy\n...\n$ ls pretrained/*\npretrained/CAEBN:\nargs.json  model.pth\n...\n</code></pre> </li> <li> <p>Drawing error</p> <p>If the following error message appears after program execution, the animation file may have failed to be generated. In this case, changing the WRITER when drawing the animation will solve the problem.</p> <pre><code>File \"/usr/lib/python3/dist-packages/matplotlib/animation.py\", line 410, in cleanup\n    raise subprocess.CalledProcessError(\nsubprocess.CalledProcessError: Command '['ffmpeg', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-s', '720x300', '-pix_fmt', 'rgba', '-r', '52.63157894736842', '-loglevel', 'error', '-i', 'pipe:', '-vcodec', 'h264', '-pix_fmt', 'yuv420p', '-y', './output/CAE-RNN-RT_20230510_0134_03_0_0.8.gif']' returned non-zero exit status 1.\n</code></pre> <p>First, install imagemagick and ffmpeg using the <code>apt</code> command.</p> <pre><code>$ sudo apt install imagemagick\n$ sudo apt install ffmpeg\n</code></pre> <p>Next, edit the code at the bottom of <code>test.py</code> as follows:</p> <pre><code># Using imagemagick\nani.save( './output/SARNN_{}_{}_{}.gif'.format(params['tag'], idx, args.input_param), writer=\"imagemagick\")\n\n# Using ffmpeg\nani.save( './output/SARNN_{}_{}_{}.gif'.format(params['tag'], idx, args.input_param), writer=\"ffmpeg\")\n</code></pre> </li> </ol>"},{"location":"model/SARNN/","title":"Model Overview","text":"<p>SARNN \"explicitly\" extracts the spatial coordinates of task-critical positions (work objects and arms) from images and learns the coordinates and joint angles of the robot, thereby significantly improving robustness to changes in object position1. The figure below shows the network structure of SARNN, which consists of an Encoder that extracts image features $f_t$ and object position coordinates $p_t$ from camera images $i_t$, a Recurrent that learns time-series changes in the robot's joint angles $a_t$ and object position coordinates $p_t$, and a Decoder that reconstructs images based on image features $f_t$ and heat maps $\\hat h_{t+1}$.</p> <p>The CNN layers (Convolution layer and Transposed convolutional layer) of the upper part of the Encoder and Decoder extract information about the color and shape of objects by extracting and reconstructing image features. The lower CNN extracts 2D position information of objects by using the Spatial Softmax layer. The Recurrent part predicts only the position information $p_{t+1}$ of the object, which does not contain enough information to reconstruct the image by Decoder. Therefore, a heat map $\\hat h_{t+1}$ centered on the predicted coordinate information $p_{t+1}$ is generated, and by multiplying it with the image features extracted by the CNN in the upper section, a predicted image $\\hat i_{t+1}$ is generated based on the information around the predicted attention point.</p> <p>Here, we show the implementation method and model classes of SARNN's characteristic features: spatial attention mechanism, heat-map generator, loss scheduler, and back-propagation method.</p> <p></p>"},{"location":"model/SARNN/#spatial_softmax","title":"Spatial Attention Mechanism","text":"<p>The spatial attention mechanism emphasizes important information (large pixel values) by multiplying the feature map by Softmax, and then extracts the position informations of the emphasized pixels using Position-Encoding. The following figure shows the processing results of the spatial attention mechanism, where important position informations (red dots) is extracted by multiplying a \"pseudo\" feature map generated using two randomly generated gaussian distributions by Softmax. Since CNN feature maps contain a wide variety of information, they are not enhanced by simply multiplying Softmax. In order to further enhance the features, it is important to use Softmax with temperature. You can check the effect of Softmax with temperature by adjusting the parameter <code>temperature</code> in the sample program below. The red dots in the figure indicate the positions extracted by spatial softmax, and since they are generated at the center of one of the gaussian distributions, the position informations can be extracted appropriately.</p> <p></p> [SOURCE] SpatialSoftmax.py<pre><code>class SpatialSoftmax(nn.Module):\ndef __init__(self, width: int, height: int, temperature=1e-4, normalized=True):\nsuper(SpatialSoftmax, self).__init__()\nself.width = width\nself.height = height\nif temperature is None:\nself.temperature = torch.nn.Parameter(torch.ones(1))\nelse:\nself.temperature = temperature\n_, pos_x, pos_y = create_position_encoding(width, height, normalized=normalized)\nself.register_buffer(\"pos_x\", pos_x)\nself.register_buffer(\"pos_y\", pos_y)\ndef forward(self, x):\nbatch_size, channels, width, height = x.shape\nassert height == self.height\nassert width == self.width\n# flatten, apply softmax\nlogit = x.reshape(batch_size, channels, -1)\natt_map = torch.softmax(logit / self.temperature, dim=-1)\n# compute expectation\nexpected_x = torch.sum(self.pos_x * att_map, dim=-1, keepdim=True)\nexpected_y = torch.sum(self.pos_y * att_map, dim=-1, keepdim=True)\nkeys = torch.cat([expected_x, expected_y], -1)\n# keys [[x,y], [x,y], [x,y],...]\nkeys = keys.reshape(batch_size, channels, 2)\natt_map = att_map.reshape(-1, channels, width, height)\nreturn keys, att_map\n</code></pre>"},{"location":"model/SARNN/#heatmap","title":"Heatmap Generator","text":"<p>The heatmap generator generates a heatmap centered on the location information (specific pixel coordinates). The following figure shows a heatmap generated by the heatmap generator centered on the location extracted by the spatial attention mechanism (red dot in the figure). The size of the heatmap can be set using the <code>heatmap_size</code> parameter. If the heatmap size is small, only the information near the attention point is considered, and if the heatmap size is large, the image is generated with some of the surrounding information. Note that if the heatmap is too small, the corresponding predictive image $\\hat i_{t+1}$ cannot be generated, and if it is too large, the parameter must be adjusted to be sensitive to changes in the environment (background and obstacles).</p> <p></p> [SOURCE] InverseSpatialSoftmax.py<pre><code>class InverseSpatialSoftmax(nn.Module):\ndef __init__(self, width: int, height: int, heatmap_size=0.1, normalized=True):\nsuper(InverseSpatialSoftmax, self).__init__()\nself.width = width\nself.height = height\nself.normalized = normalized\nself.heatmap_size = heatmap_size\npos_xy, _, _ = create_position_encoding(width, height, normalized=normalized)\nself.register_buffer(\"pos_xy\", pos_xy)\ndef forward(self, keys):\nsquared_distances = torch.sum(\ntorch.pow(self.pos_xy[None, None] - keys[:, :, :, None, None], 2.0), axis=2\n)\nheatmap = torch.exp(-squared_distances / self.heatmap_size)\nreturn heatmap\n</code></pre>"},{"location":"model/SARNN/#loss_scheduler","title":"Loss Scheduler","text":"<p>The loss scheduler is a <code>callback</code> function that gradually weights the prediction error of the attention point according to the epoch, and is an important feature for training SARNN. The following figure shows the weighting curve for each <code>curve_name</code> argument, where the horizontal axis is the number of epochs and the vertical axis is the weighting value. Loss weighting starts from 0 and returns the maximum weighting value (e.g. 0.1) at the epoch set by <code>decay_end</code> (e.g. 100). Note that the maximum weighting value is specified by the <code>__call__</code> method. This class supports the five types of curves shown in the figure (linear, S-curve, inverse S-curve, deceleration, and acceleration interpolation).</p> <p></p> <p>The reason for using the error scheduler for training SARNN is to allow the filters of the CNN to be \"freely\" trained in the early stages of training. Since the Encoder and Decoder weights of SARNN are initialized randomly, visual features are not properly extracted/learned in the early learning phase.</p> <p>If the attention prediction error obtained in such a situation is propagated backward, the attention point will not be correctly directed to the work object, and the attention point that minimizes the \"prediction image error\" will be learned. Therefore, it is possible to obtain an attention point that focuses only on the work object by ignoring the prediction error of the attention point in the initial stage of learning, and learning the error of the attention point prediction when the filter of the CNN finishes learning the features. The <code>decay_end</code> adjusts the learning timing of the CNN, which is usually set around 1000 epochs, but may need to be adjusted depending on the task.</p> [SOURCE] callback.py<pre><code>class LossScheduler:\ndef __init__(self, decay_end=1000, curve_name=\"s\"):\ndecay_start = 0\nself.counter = -1\nself.decay_end = decay_end\nself.interpolated_values = self.curve_interpolation(\ndecay_start, decay_end, decay_end, curve_name\n)\ndef linear_interpolation(self, start, end, num_points):\nx = np.linspace(start, end, num_points)\nreturn x\ndef s_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (t - np.sin(2 * np.pi * t) / (2 * np.pi))\nreturn x\ndef inverse_s_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (t + np.sin(2 * np.pi * t) / (2 * np.pi))\nreturn x\ndef deceleration_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (1 - np.cos(np.pi * t / 2))\nreturn x\ndef acceleration_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (np.sin(np.pi * t / 2))\nreturn x\ndef curve_interpolation(self, start, end, num_points, curve_name):\nif curve_name == \"linear\":\ninterpolated_values = self.linear_interpolation(start, end, num_points)\nelif curve_name == \"s\":\ninterpolated_values = self.s_curve_interpolation(start, end, num_points)\nelif curve_name == \"inverse_s\":\ninterpolated_values = self.inverse_s_curve_interpolation(start, end, num_points)\nelif curve_name == \"deceleration\":\ninterpolated_values = self.deceleration_curve_interpolation(start, end, num_points)\nelif curve_name == \"acceleration\":\ninterpolated_values = self.acceleration_curve_interpolation(start, end, num_points)\nelse:\nassert False, \"Invalid curve name. {}\".format(curve_name)\nreturn interpolated_values / num_points\ndef __call__(self, loss_weight):\nself.counter += 1\nif self.counter &gt;= self.decay_end:\nreturn loss_weight\nelse:\nreturn self.interpolated_values[self.counter] * loss_weight\n</code></pre>"},{"location":"model/SARNN/#bptt","title":"Backpropagation Through Time","text":"<p>We use Backpropagation Through Time (BPTT) to learn the time series of the model2. In RNN, the internal state $h_{t}$ at each time depends on the internal state $h_{t-1}$ at the previous time $t-1$. In BPTT, the parameters are updated at each time by calculating the loss at each time and then calculating the gradient backward. Specifically, image $i_t$ and joint angle $a_{t}$ are input to the model, and the next state ($\\hat i_{t+1}$, $ \\hat a_{t+1}$) is output (predicted). The mean squared error <code>nn.MSELoss</code> of the predictions and true values ($f_{t+1}$, $a_{t+1}$) for all sequences is calculated and error propagation is performed based on the loss value <code>loss</code>. The parameters at each time are used at all times after that time, so back propagation is performed with temporal expansion.</p> <p>Lines 47-54 show that SARNN computes not only the image loss and joint angle loss, but also the prediction loss of the attention point. Since the true value of the attention point does not exist, the bi-directional loss3 is used to learn the attention point. Specifically, the model updates the weights to minimize the loss between the attention point $\\hat p_{t+1}$ predicted by the RNN at time $t$ and the attention point $p_{t+1}$ extracted by the CNN at time $t+1$.  Based on this bidirectional loss, LSTM learns the time-series relationship between attention points and joint angles, which not only eliminates redundant image predictions, but also induces the CNN to predict attention points that are important for motion prediction.</p> <p>Also, <code>loss_weights</code> weights each modality loss and determines which modality to focus on for learning.  In deep prediction learning, the joint angles are intensively learned because the predicted joint angles are directly related to the robot's motion commands. However, if image information is not sufficiently learned, integrated learning of images and joint angles cannot be performed properly (joint angle prediction corresponding to image information becomes difficult), thus the weighting coefficients must be adjusted according to the model and task. In our experience, the weighting factor is often set to 1.0 for all or 0.1 for images only.</p> [SOURCE] fullBPTT.py<pre><code>class fullBPTTtrainer:\ndef __init__(self, model, optimizer, loss_weights=[1.0, 1.0], device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.loss_weights = loss_weights\nself.scheduler = LossScheduler(decay_end=1000, curve_name=\"s\")\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, ((x_img, x_joint), (y_img, y_joint)) in enumerate(data):\nx_img = x_img.to(self.device)\ny_img = y_img.to(self.device)\nx_joint = x_joint.to(self.device)\ny_joint = y_joint.to(self.device)\nstate = None\nyi_list, yv_list = [], []\ndec_pts_list, enc_pts_list = [], []\nT = x_img.shape[1]\nfor t in range(T - 1):\n_yi_hat, _yv_hat, enc_ij, dec_ij, state = self.model(\nx_img[:, t], x_joint[:, t], state\n)\nyi_list.append(_yi_hat)\nyv_list.append(_yv_hat)\nenc_pts_list.append(enc_ij)\ndec_pts_list.append(dec_ij)\nyi_hat = torch.permute(torch.stack(yi_list), (1, 0, 2, 3, 4))\nyv_hat = torch.permute(torch.stack(yv_list), (1, 0, 2))\nimg_loss = nn.MSELoss()(yi_hat, y_img[:, 1:]) * self.loss_weights[0]\njoint_loss = nn.MSELoss()(yv_hat, y_joint[:, 1:]) * self.loss_weights[1]\n# Gradually change the loss value using the LossScheluder class.\npt_loss = nn.MSELoss()(\ntorch.stack(dec_pts_list[:-1]), torch.stack(enc_pts_list[1:])\n) * self.scheduler(self.loss_weights[2])\nloss = img_loss + joint_loss + pt_loss\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / (n_batch + 1)\n</code></pre> <ol> <li> <p>Hideyuki Ichiwara, Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Contact-rich manipulation of a flexible object based on deep predictive learning using vision and tactility. In 2022 International Conference on Robotics and Automation (ICRA), 5375\u20135381. IEEE, 2022.\u00a0\u21a9</p> </li> <li> <p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.\u00a0\u21a9</p> </li> <li> <p>Hyogo Hiruma, Hiroshi Ito, Hiroki Mori, and Tetsuya Ogata. Deep active visual attention for real-time robot motion generation: emergence of tool-body assimilation and adaptive tool-use. IEEE Robotics and Automation Letters, 7(3):8550\u20138557, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"model/SARNN/#model.SARNN","title":"<code>model.SARNN</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>SARNN: Spatial Attention with Recurrent Neural Network. This model \"explicitly\" extracts positions from the image that are important to the task, such as the work object or arm position, and learns the time-series relationship between these positions and the robot's joint angles. The robot is able to generate robust motions in response to changes in object position and lighting.</p> <p>Parameters:</p> Name Type Description Default <code>rec_dim</code> <code>int</code> <p>The dimension of the recurrent state in the LSTM cell.</p> required <code>k_dim</code> <code>int</code> <p>The dimension of the attention points.</p> <code>5</code> <code>joint_dim</code> <code>int</code> <p>The dimension of the joint angles.</p> <code>14</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for the softmax function.</p> <code>0.0001</code> <code>heatmap_size</code> <code>float</code> <p>The size of the heatmap in the InverseSpatialSoftmax layer.</p> <code>0.1</code> <code>kernel_size</code> <code>int</code> <p>The size of the convolutional kernel.</p> <code>3</code> <code>activation</code> <code>str</code> <p>The name of activation function.</p> <code>'lrelu'</code> <code>im_size</code> <code>list</code> <p>The size of the input image [height, width].</p> <code>[128, 128]</code> Source code in <code>en/docs/model/src/model.py</code> <pre><code>class SARNN(nn.Module):\n#:: SARNN\n\"\"\"SARNN: Spatial Attention with Recurrent Neural Network.\n    This model \"explicitly\" extracts positions from the image that are important to the task, such as the work object or arm position,\n    and learns the time-series relationship between these positions and the robot's joint angles.\n    The robot is able to generate robust motions in response to changes in object position and lighting.\n    Arguments:\n        rec_dim (int): The dimension of the recurrent state in the LSTM cell.\n        k_dim (int, optional): The dimension of the attention points.\n        joint_dim (int, optional): The dimension of the joint angles.\n        temperature (float, optional): The temperature parameter for the softmax function.\n        heatmap_size (float, optional): The size of the heatmap in the InverseSpatialSoftmax layer.\n        kernel_size (int, optional): The size of the convolutional kernel.\n        activation (str, optional): The name of activation function.\n        im_size (list, optional): The size of the input image [height, width].\n    \"\"\"\ndef __init__(\nself,\nrec_dim,\nk_dim=5,\njoint_dim=14,\ntemperature=1e-4,\nheatmap_size=0.1,\nkernel_size=3,\nactivation=\"lrelu\",\nim_size=[128, 128],\n):\nsuper(SARNN, self).__init__()\nself.k_dim = k_dim\nif isinstance(activation, str):\nactivation = get_activation_fn(activation, inplace=True)\nsub_im_size = [im_size[0] - 3 * (kernel_size - 1), im_size[1] - 3 * (kernel_size - 1)]\nself.temperature = temperature\nself.heatmap_size = heatmap_size\n# Positional Encoder\nself.pos_encoder = nn.Sequential(\nnn.Conv2d(3, 16, 3, 1, 0),  # Convolutional layer 1\nactivation,\nnn.Conv2d(16, 32, 3, 1, 0),  # Convolutional layer 2\nactivation,\nnn.Conv2d(32, self.k_dim, 3, 1, 0),  # Convolutional layer 3\nactivation,\nSpatialSoftmax(\nwidth=sub_im_size[0], height=sub_im_size[1], temperature=self.temperature, normalized=True\n),  # Spatial Softmax layer\n)\n# Image Encoder\nself.im_encoder = nn.Sequential(\nnn.Conv2d(3, 16, 3, 1, 0),  # Convolutional layer 1\nactivation,\nnn.Conv2d(16, 32, 3, 1, 0),  # Convolutional layer 2\nactivation,\nnn.Conv2d(32, self.k_dim, 3, 1, 0),  # Convolutional layer 3\nactivation,\n)\nrec_in = joint_dim + self.k_dim * 2\nself.rec = nn.LSTMCell(rec_in, rec_dim)  # LSTM cell\n# Joint Decoder\nself.decoder_joint = nn.Sequential(nn.Linear(rec_dim, joint_dim), activation)  # Linear layer and activation\n# Point Decoder\nself.decoder_point = nn.Sequential(\nnn.Linear(rec_dim, self.k_dim * 2), activation\n)  # Linear layer and activation\n# Inverse Spatial Softmax\nself.issm = InverseSpatialSoftmax(\nwidth=sub_im_size[0], height=sub_im_size[1], heatmap_size=self.heatmap_size, normalized=True\n)\n# Image Decoder\nself.decoder_image = nn.Sequential(\nnn.ConvTranspose2d(self.k_dim, 32, 3, 1, 0),  # Transposed Convolutional layer 1\nactivation,\nnn.ConvTranspose2d(32, 16, 3, 1, 0),  # Transposed Convolutional layer 2\nactivation,\nnn.ConvTranspose2d(16, 3, 3, 1, 0),  # Transposed Convolutional layer 3\nactivation,\n)\ndef forward(self, xi, xv, state=None):\n\"\"\"\n        Forward pass of the SARNN module.\n        Predicts the image, joint angle, and attention at the next time based on the image and joint angle at time t.\n        Predict the image, joint angles, and attention points for the next state (t+1) based on\n        the image and joint angles of the current state (t).\n        By inputting the predicted joint angles as control commands for the robot,\n        it is possible to generate sequential motion based on sensor information.\n        Arguments:\n            xi (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n            xv (torch.Tensor): Input vector tensor of shape (batch_size, input_dim).\n            state (tuple, optional): Initial hidden state and cell state of the LSTM cell.\n        Returns:\n            y_image (torch.Tensor): Decoded image tensor of shape (batch_size, channels, height, width).\n            y_joint (torch.Tensor): Decoded joint prediction tensor of shape (batch_size, joint_dim).\n            enc_pts (torch.Tensor): Encoded points tensor of shape (batch_size, k_dim * 2).\n            dec_pts (torch.Tensor): Decoded points tensor of shape (batch_size, k_dim * 2).\n            rnn_hid (tuple): Tuple containing the hidden state and cell state of the LSTM cell.\n        \"\"\"\n# Encode input image\nim_hid = self.im_encoder(xi)\nenc_pts, _ = self.pos_encoder(xi)\n# Reshape encoded points and concatenate with input vector\nenc_pts = enc_pts.reshape(-1, self.k_dim * 2)\nhid = torch.cat([enc_pts, xv], -1)\nrnn_hid = self.rec(hid, state)  # LSTM forward pass\ny_joint = self.decoder_joint(rnn_hid[0])  # Decode joint prediction\ndec_pts = self.decoder_point(rnn_hid[0])  # Decode points\n# Reshape decoded points\ndec_pts_in = dec_pts.reshape(-1, self.k_dim, 2)\nheatmap = self.issm(dec_pts_in)  # Inverse Spatial Softmax\nhid = torch.mul(heatmap, im_hid)  # Multiply heatmap with image feature `im_hid`\ny_image = self.decoder_image(hid)  # Decode image\nreturn y_image, y_joint, enc_pts, dec_pts, rnn_hid\n</code></pre>"},{"location":"model/SARNN/#model.SARNN.forward","title":"<code>forward(xi, xv, state=None)</code>","text":"<p>Forward pass of the SARNN module. Predicts the image, joint angle, and attention at the next time based on the image and joint angle at time t. Predict the image, joint angles, and attention points for the next state (t+1) based on the image and joint angles of the current state (t). By inputting the predicted joint angles as control commands for the robot, it is possible to generate sequential motion based on sensor information.</p> <p>Parameters:</p> Name Type Description Default <code>xi</code> <code>torch.Tensor</code> <p>Input image tensor of shape (batch_size, channels, height, width).</p> required <code>xv</code> <code>torch.Tensor</code> <p>Input vector tensor of shape (batch_size, input_dim).</p> required <code>state</code> <code>tuple</code> <p>Initial hidden state and cell state of the LSTM cell.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_image</code> <code>torch.Tensor</code> <p>Decoded image tensor of shape (batch_size, channels, height, width).</p> <code>y_joint</code> <code>torch.Tensor</code> <p>Decoded joint prediction tensor of shape (batch_size, joint_dim).</p> <code>enc_pts</code> <code>torch.Tensor</code> <p>Encoded points tensor of shape (batch_size, k_dim * 2).</p> <code>dec_pts</code> <code>torch.Tensor</code> <p>Decoded points tensor of shape (batch_size, k_dim * 2).</p> <code>rnn_hid</code> <code>tuple</code> <p>Tuple containing the hidden state and cell state of the LSTM cell.</p> Source code in <code>en/docs/model/src/model.py</code> <pre><code>def forward(self, xi, xv, state=None):\n\"\"\"\n    Forward pass of the SARNN module.\n    Predicts the image, joint angle, and attention at the next time based on the image and joint angle at time t.\n    Predict the image, joint angles, and attention points for the next state (t+1) based on\n    the image and joint angles of the current state (t).\n    By inputting the predicted joint angles as control commands for the robot,\n    it is possible to generate sequential motion based on sensor information.\n    Arguments:\n        xi (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n        xv (torch.Tensor): Input vector tensor of shape (batch_size, input_dim).\n        state (tuple, optional): Initial hidden state and cell state of the LSTM cell.\n    Returns:\n        y_image (torch.Tensor): Decoded image tensor of shape (batch_size, channels, height, width).\n        y_joint (torch.Tensor): Decoded joint prediction tensor of shape (batch_size, joint_dim).\n        enc_pts (torch.Tensor): Encoded points tensor of shape (batch_size, k_dim * 2).\n        dec_pts (torch.Tensor): Decoded points tensor of shape (batch_size, k_dim * 2).\n        rnn_hid (tuple): Tuple containing the hidden state and cell state of the LSTM cell.\n    \"\"\"\n# Encode input image\nim_hid = self.im_encoder(xi)\nenc_pts, _ = self.pos_encoder(xi)\n# Reshape encoded points and concatenate with input vector\nenc_pts = enc_pts.reshape(-1, self.k_dim * 2)\nhid = torch.cat([enc_pts, xv], -1)\nrnn_hid = self.rec(hid, state)  # LSTM forward pass\ny_joint = self.decoder_joint(rnn_hid[0])  # Decode joint prediction\ndec_pts = self.decoder_point(rnn_hid[0])  # Decode points\n# Reshape decoded points\ndec_pts_in = dec_pts.reshape(-1, self.k_dim, 2)\nheatmap = self.issm(dec_pts_in)  # Inverse Spatial Softmax\nhid = torch.mul(heatmap, im_hid)  # Multiply heatmap with image feature `im_hid`\ny_image = self.decoder_image(hid)  # Decode image\nreturn y_image, y_joint, enc_pts, dec_pts, rnn_hid\n</code></pre>"},{"location":"model/dataloader/","title":"Dataloader","text":"<p>EIPL provides a <code>MultimodalDataset</code> class for learning robot motions, which inherits from the <code>Dataset</code> class provided by Pytorch as standard. This class returns every epoch, a pair of input data <code>x_data</code> and a true value <code>y_data</code>. The input data <code>x_data</code> is a pair of images and joint angles, and data augmentation is applied every epoch. The input image is randomly assigned brightness, contrast, etc. to improve robustness against lighting changes, and the input joint angles are added gaussian noise to improve robustness against position errors. On the other hand, no noise is added to the true data. The model learns a noise-neglected situation (internal representation) from input data mixed with noise, which enables robust motion generation against real-world noise during inference.</p> <p>The following source code shows how to use the <code>MultimodalDataset</code> class using an object grasping task collected using AIREC as an example. By passing 5-dimensional image time-series data [number of data, time-series length, channel, height, width] and 3-dimensional joint angle time-series data [number of data, time-series length, number of joints] to the <code>MultimodalDataset</code> class, data expansion and other operations are automatically performed. Note that <code>SampleDownloader</code> downloading sample dataset and is not necessarily required. You may use the <code>numpy.load</code> function or other functions to directly load your own data sets.</p> How to use dataloader<pre><code>from eipl.data import SampleDownloader, MultimodalDataset\n# Download and normalize sample data\ngrasp_data = SampleDownloader(\"airec\", \"grasp_bottle\", img_format=\"CHW\")\nimages, joints = grasp_data.load_norm_data(\"train\", vmin=0.1, vmax=0.9)\n# Give the image and joint angles to the Dataset class\nmulti_dataset = MultimodalDataset(images, joints)\n# Return input/true data as return value.\nx_data, y_data = multi_dataset[1]\n</code></pre> <p>The following figure shows the robot camera images returned by the <code>MultimodalDataset</code> class, From left to right are the noiseless image, the image with noise and the robot joint angles. Since random noise is added to the image every epoch, the model learns a wide variety of visual situations. The black dotted lines in the robot joint angles are the original joint angles without noise, and the colored lines are the joint angles with gaussian noise.</p> <p></p> <p>Note</p> <p>If you are unable to get the dataset due to a proxy or some other reason, download the data set manually from here and save it in the ~/.eipl/ folder.     <pre><code>$ cd ~/\n$ mkdir .eipl\n$ cd .eipl\n$ # copy grasp_bottle.tar to ~/.eipl/ directory\n$ tar xvf grasp_bottle.tar\n$ ls grasp_bottle/*\ngrasp_bottle/joint_bounds.npy\n</code></pre></p>"},{"location":"model/dataloader/#dataloader.MultimodalDataset","title":"<code>dataloader.MultimodalDataset</code>","text":"<p>         Bases: <code>Dataset</code></p> <p>This class is used to train models that deal with multimodal data (e.g., images, joints), such as CNNRNN/SARNN.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>numpy array</code> <p>Set of images in the dataset, expected to be a 5D array [data_num, seq_num, channel, height, width].</p> required <code>joints</code> <code>numpy array</code> <p>Set of joints in the dataset, expected to be a 3D array [data_num, seq_num, joint_dim].</p> required <code>stdev</code> <code>float</code> <p>Set the standard deviation for normal distribution to generate noise.</p> <code>0.02</code> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>class MultimodalDataset(Dataset):\n#:: MultimodalDataset\n\"\"\"\n    This class is used to train models that deal with multimodal data (e.g., images, joints), such as CNNRNN/SARNN.\n    Args:\n        images (numpy array): Set of images in the dataset, expected to be a 5D array [data_num, seq_num, channel, height, width].\n        joints (numpy array): Set of joints in the dataset, expected to be a 3D array [data_num, seq_num, joint_dim].\n        stdev (float, optional): Set the standard deviation for normal distribution to generate noise.\n    \"\"\"\ndef __init__(self, images, joints, stdev=0.02):\n\"\"\"\n        The constructor of Multimodal Dataset class. Initializes the images, joints, and transformation.\n        Args:\n            images (numpy array): The images data, expected to be a 5D array [data_num, seq_num, channel, height, width].\n            joints (numpy array): The joints data, expected to be a 3D array [data_num, seq_num, joint_dim].\n            stdev (float, optional): The standard deviation for the normal distribution to generate noise. Defaults to 0.02.\n        \"\"\"\nself.stdev = stdev\nself.images = images\nself.joints = joints\nself.transform = transforms.ColorJitter(contrast=0.5, brightness=0.5, saturation=0.1)\ndef __len__(self):\n\"\"\"\n        Returns the number of the data.\n        \"\"\"\nreturn len(self.images)\ndef __getitem__(self, idx):\n\"\"\"\n        Extraction and preprocessing of images and joints at the specified indexes.\n        Args:\n            idx (int): The index of the element.\n        Returns:\n            dataset (list): A list containing lists of transformed and noise added image\n                            and joint (x_img, x_joint) and the original image and joint (y_img, y_joint).\n        \"\"\"\ny_img = self.images[idx]\ny_joint = self.joints[idx]\nx_img = self.transform(self.images[idx])\nx_img = x_img + torch.normal(mean=0, std=self.stdev, size=x_img.shape)\nx_joint = self.joints[idx] + torch.normal(mean=0, std=self.stdev, size=y_joint.shape)\nreturn [[x_img, x_joint], [y_img, y_joint]]\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Extraction and preprocessing of images and joints at the specified indexes.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the element.</p> required <p>Returns:</p> Name Type Description <code>dataset</code> <code>list</code> <p>A list containing lists of transformed and noise added image             and joint (x_img, x_joint) and the original image and joint (y_img, y_joint).</p> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>def __getitem__(self, idx):\n\"\"\"\n    Extraction and preprocessing of images and joints at the specified indexes.\n    Args:\n        idx (int): The index of the element.\n    Returns:\n        dataset (list): A list containing lists of transformed and noise added image\n                        and joint (x_img, x_joint) and the original image and joint (y_img, y_joint).\n    \"\"\"\ny_img = self.images[idx]\ny_joint = self.joints[idx]\nx_img = self.transform(self.images[idx])\nx_img = x_img + torch.normal(mean=0, std=self.stdev, size=x_img.shape)\nx_joint = self.joints[idx] + torch.normal(mean=0, std=self.stdev, size=y_joint.shape)\nreturn [[x_img, x_joint], [y_img, y_joint]]\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset.__init__","title":"<code>__init__(images, joints, stdev=0.02)</code>","text":"<p>The constructor of Multimodal Dataset class. Initializes the images, joints, and transformation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>numpy array</code> <p>The images data, expected to be a 5D array [data_num, seq_num, channel, height, width].</p> required <code>joints</code> <code>numpy array</code> <p>The joints data, expected to be a 3D array [data_num, seq_num, joint_dim].</p> required <code>stdev</code> <code>float</code> <p>The standard deviation for the normal distribution to generate noise. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>def __init__(self, images, joints, stdev=0.02):\n\"\"\"\n    The constructor of Multimodal Dataset class. Initializes the images, joints, and transformation.\n    Args:\n        images (numpy array): The images data, expected to be a 5D array [data_num, seq_num, channel, height, width].\n        joints (numpy array): The joints data, expected to be a 3D array [data_num, seq_num, joint_dim].\n        stdev (float, optional): The standard deviation for the normal distribution to generate noise. Defaults to 0.02.\n    \"\"\"\nself.stdev = stdev\nself.images = images\nself.joints = joints\nself.transform = transforms.ColorJitter(contrast=0.5, brightness=0.5, saturation=0.1)\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of the data.</p> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>def __len__(self):\n\"\"\"\n    Returns the number of the data.\n    \"\"\"\nreturn len(self.images)\n</code></pre>"},{"location":"model/test/","title":"Inference","text":""},{"location":"model/test/#off-line-inference","title":"Off-line inference","text":"<p>Check that SARNN has been properly trained using the test program <code>test.py</code>. The argument <code>filename</code> is the path of the trained weights file and <code>idx</code> is the index of the data to be visualized. The <code>input_param</code> is a mixing coefficient to generate stable behavior against real-world noise. The sensor information at time $t$ is mixed with the predictions of the model at the previous time $t-1$ in a certain ratio and input to the model. This process is equivalent to a low-pass filter, and even if the robot's sensor values are noisy, the predicted values from the previous time can be used as a supplement to predict stable motion commands. Note that if the mixing coefficient is too small, it becomes difficult to modify the motion based on real-world sensor information, and the robustness against position changes decreases.</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ python3 bin/test.py --filename ./log/20230521_1247_41/SARNN.pth --idx 4 --input_param 1.0\n\nimages shape:(187, 128, 128, 3), min=0, max=255\njoints shape:(187, 8), min=-0.8595600128173828, max=1.8292399644851685\nloop_ct:0, joint:[ 0.00226304 -0.7357931  -0.28175825  1.2895856   0.7252841   0.14539993\n-0.0266939   0.00422328]\nloop_ct:1, joint:[ 0.00307412 -0.73363686 -0.2815826   1.2874944   0.72176594  0.1542334\n-0.02719587  0.00325996]\n.\n.\n.\n\n$ ls output/\nSARNN_20230521_1247_41_4_1.0.gif\n</code></pre> <p>The following figure shows the inference results at the unlearned position (point D). From left to right, the input image, the predicted image, and the predicted joint angles (dotted lines are true values). The blue points in the input image are the points of interest extracted from the image, and the red points are the points of interest predicted by the RNN, indicating that the joint angle is predicted while focusing on the robot hand and the grasped object.</p> <p></p>"},{"location":"model/test/#pca","title":"Principal Component Analysis","text":"<p>In deep predictive learning, it is recommended to visualize the internal representation using Principal Component Analysis (PCA)1 in order to preliminarily examine whether the trained model has generalization performance. In order to acquire generalization motion with small data, it is necessary to embed the motion in the RNN's internal state, and the internal state should be self-organized (structured) for each teaching motion. Hereafter, we use PCA to compress the internal state of the RNN to a lower dimension, and visualize the elements (first through third principal components) that represent the characteristics of the data to verify how the sensorimotor information (images and joint angles) are represented.</p> <p>The following program is a partial excerpt of the inference and PCA process. First, input test data into the model and store the internal state <code>state</code> of the RNN at each time as a list. In the case of LSTM, <code>hidden state</code> and <code>cell state</code> are returned as <code>state</code>. Here we use <code>hidden state</code> for visualization and analysis. Next, we transform the shape of state, from [number of data, time series length, number of dimensions of state] to [number of data x time series length, number of dimensions of state] in order to compare the internal state at each object position. Finally, the high-dimensional state is compressed into low-dimensional information (3 dimensions) by applying PCA as shown in line 12. By restoring the compressed principal component <code>pca_val</code> to its original shape [number of data, time series length, 3 dim], we can visualize the relationship between object position and internal state by coloring each object position and plotting it in 3D space.</p> [SOURCE] test_pca_rnn.py<pre><code>states = tensor2numpy( states )\n# Reshape the state from [N,T,D] to [-1,D] for PCA of RNN.\n# N is the number of datasets\n# T is the sequence length\n# D is the dimension of the hidden state\nN,T,D  = states.shape\nstates = states.reshape(-1,D)\n# plot pca\nloop_ct = float(360)/T\npca_dim = 3\npca     = PCA(n_components=pca_dim).fit(states)\npca_val = pca.transform(states)\n# Reshape the states from [-1, pca_dim] to [N,T,pca_dim] to\n# visualize each state as a 3D scatter.\npca_val = pca_val.reshape( N, T, pca_dim )\nfig = plt.figure(dpi=60)\nax = fig.add_subplot(projection='3d')\ndef anim_update(i):\nax.cla()\nangle = int(loop_ct * i)\nax.view_init(30, angle)\nc_list = ['C0','C1','C2','C3','C4']\n</code></pre> <p>Use <code>test_pca_sarnn.py</code> for the program to visualize the internal state using PCA. The argument filename is the path of the weights file.</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ python3 ./bin/test_pca_sarnn.py --filename log/20230521_1247_41/SARNN.pth\n$ ls output/\nPCA_SARNN_20230521_1247_41.gif\n</code></pre> <p>The figure below shows the inference result of SARNN. Each dotted line shows the time series change of the internal state. The color of each attractor corresponds to object position, with blue, orange, and green corresponding to taught positions A, C,and E, and red and purple to untrained positions B and D. Since the attractors are self-organized (aligned) according to the object position, it can be said that the behavior is learned (memorized) according to the object position. In particular, since the attractors at unlearned positions are generated between taught positions, it is possible to generate unlearned interpolated motions by simply teaching and learning grasping motions with different object positions multiple times.</p> <p></p>"},{"location":"model/test/#online","title":"Online Motion Generation","text":"<p>The following describes an online motion generation method using a real robot with pseudo code. The robot can generate sequential motions based on sensor information by repeating steps 2-5 at a specified sampling rate.</p> <ol> <li> <p>Model loading (line 23)</p> <p>After defining the model, load the trained weights.</p> </li> <li> <p>Get and normalize sensor information (line 38)</p> <p>Get the robot sensor information and perform the normalization process. For example, if you are using ROS, the Subscribed image and joint angles as <code>raw_image</code> and <code>raw_joint</code>.</p> </li> <li> <p>Inference (line 51)</p> <p>Predict the image <code>y_image</code> and joint angle <code>y_joint</code> at the next time using the normalized image <code>x_img</code> and joint angle <code>x_joint</code>.</p> </li> <li> <p>Send command (line 61)</p> <p>By using the predicted joint angle <code>pred_joint</code> as the robot's motor command, the robot can generate sequential motions. In the case of ROS, by publishing the joint angles to the motors, the robot controls each motor based on the motor command.</p> </li> <li> <p>Sleep (line 65)</p> <p>Finally, timing is adjusted by inserting a sleep process to perform inference at the specified sampling rate. The sampling rate should be the same as during training data collection.</p> </li> </ol> online.py<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(\"--model_pth\", type=str, default=None)\nparser.add_argument(\"--input_param\", type=float, default=0.8)\nargs = parser.parse_args()\n# restore parameters\ndir_name = os.path.split(args.model_pth)[0]\nparams = restore_args(os.path.join(dir_name, \"args.json\"))\n# load dataset\nminmax = [params[\"vmin\"], params[\"vmax\"]]\njoint_bounds = np.load(os.path.join(os.path.expanduser(\"~\"), \".eipl/grasp_bottle/joint_bounds.npy\"))\n# define model\nmodel = SARNN(\nrec_dim=params[\"rec_dim\"],\njoint_dim=8,\nk_dim=params[\"k_dim\"],\nheatmap_size=params[\"heatmap_size\"],\ntemperature=params[\"temperature\"],\n)\n# load weight\nckpt = torch.load(args.model_pth, map_location=torch.device(\"cpu\"))\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\n# Inference\n# Set the inference frequency; for a 10-Hz in ROS system, set as follows.\nfreq = 10  # 10Hz\nrate = rospy.Rate(freq)\nimage_list, joint_list = [], []\nstate = None\nnloop = 200  # freq * 20 sec\nfor loop_ct in range(nloop):\nstart_time = time.time()\n# load data and normalization\nraw_images, raw_joint = robot.get_sensor_data()\nx_img = raw_images[loop_ct].transpose(2, 0, 1)\nx_img = torch.Tensor(np.expand_dims(x_img, 0))\nx_img = normalization(x_img, (0, 255), minmax)\nx_joint = torch.Tensor(np.expand_dims(raw_joint, 0))\nx_joint = normalization(x_joint, joint_bounds, minmax)\n# closed loop\nif loop_ct &gt; 0:\nx_img = args.input_param * x_img + (1.0 - args.input_param) * y_image\nx_joint = args.input_param * x_joint + (1.0 - args.input_param) * y_joint\n# predict rnn\ny_image, y_joint, state = rnn_model(x_img, x_joint, state)\n# denormalization\npred_image = tensor2numpy(y_image[0])\npred_image = deprocess_img(pred_image, cae_params[\"vmin\"], cae_params[\"vmax\"])\npred_image = pred_image.transpose(1, 2, 0)\npred_joint = tensor2numpy(y_joint[0])\npred_joint = normalization(pred_joint, minmax, joint_bounds)\n# send pred_joint to robot\n# send_command(pred_joint)\npub.publish(pred_joint)\n# Sleep to infer at set frequency.\n# ROS system\nrate.sleep()\n</code></pre> <ol> <li> <p>Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933.\u00a0\u21a9</p> </li> </ol>"},{"location":"model/train/","title":"Train","text":""},{"location":"model/train/#files","title":"Files","text":"<p>Use the programs in the tutorial/SARNN folder in the EIPL repository for training SARNN. The roles of each folder and program are as follows:</p> <ul> <li>bin/train.py: Programs to load data, train, and save models.</li> <li>bin/test.py: Program to perform off-line inference of models using test data (images and joint angles) and visualize inference results.</li> <li>bin/test_pca_sarnn.py: Program to visualize the internal state of RNN using Principal Component Analysis.</li> <li>libs/fullBPTT.py: Back propagation class for time series learning.</li> <li>log: Folder to store weights, learning curves, and parameter information.</li> <li>output: Save inference results.</li> </ul>"},{"location":"model/train/#train","title":"Trainig","text":"<p>The main program <code>train.py</code> is used to train SARNN. When the program is run, the weights (pth) and Tensorboard log files are saved in the <code>log</code> folder. The program allows the user to specify the necessary parameters for training, such as model type, number of epochs, batch size, training rate, and optimization method, using command line arguments. It also uses the EarlyStopping library to determine when to terminate training early as well as to save weights when the test error is minimized. For a detailed description of how the program works, please see the comments in the code.</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ python3 ./bin/train.py\n[INFO] Set tag = \"20230521_1247_41\"\n================================\nbatch_size : 5\ndevice : 0\nepoch : 100000\nheatmap_size : 0.1\nimg_loss : 0.1\njoint_loss : 1.0\nk_dim : 5\nlog_dir : log/\nlr : 0.001\nmodel : sarnn\noptimizer : adam\npt_loss : 0.1\nrec_dim : 50\nstdev : 0.02\ntag : \"20230521_1247_41\"\ntemperature : 0.0001\nvmax : 1.0\nvmin : 0.0\n================================\n12%|\u2588\u2588\u2588\u2588          | 11504/100000 [14:46:53&lt;114:10:44,  4.64s/it, train_loss=0.000251, test_loss=0.000316]\n</code></pre>"},{"location":"model/train/#tensorboard","title":"Learning Curves","text":"<p>Check the training status of the model using TensorBoard. By specifying the log folder where the weights are stored in the argument <code>logdir</code>, you can see the learning curve in your browser as shown in the figure below. If there is a tendency toward over-learning in the early phase of training, it may be due to an anomaly in the training data or model, or the initial weights (seeds). Countermeasures include checking the normalization range of the training data, checking the model structure, and retraining with different seed values. For specific information on how to use TensorBoard, please refer to here.</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ tensorboard --logdir=./log/\nTensorFlow installation not found - running with reduced feature set.\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.12.1 at http://localhost:6006/ (Press CTRL+C to quit)\n</code></pre> <p></p>"},{"location":"robot/overview/","title":"Overview","text":"<p>Here, we describe a sequence of procedures from motion teaching to motion generation using Open Manpulator. The following five points will be described.</p> <ol> <li>Hardware Configuration</li> <li>ROS Environment</li> <li>Data Collection</li> <li>Generate Dataset</li> <li>Model Training</li> <li>Online Motion Generation</li> </ol> <p>Note</p> <p>The packages will be available in end of July.</p>"},{"location":"teach/dataset/","title":"Generate dataset","text":""},{"location":"teach/dataset/#download","title":"Download","text":"<p>Here, we generate a dataset for deep predictive learning using sensor information from teaching object grasping movements using AIREC. In this section, we describe how to extract only specific files from multiple rosbag data and save them in npz format using the collected sample data and scripts. Follow the commands below to download and extract the files.</p> <pre><code>$ mkdir ~/tmp\n$ cd tmp\n$ wget https://dl.dropboxusercontent.com/s/90wkfttf9w0bz0t/rosbag.tar\n$ tar xvf rosbag.tar\n$ cd rosbag\n$ ls\n1_rosbag2npz.py  2_make_dataset.py  3_check_data.py  bag  data  output  utils.py\n</code></pre>"},{"location":"teach/dataset/#files","title":"Files","text":"<p>The contents of the download file consist of the following files. Users can generate training data from rosbag data simply by executing the programs in the order of program number 1.</p> <ul> <li>1_rosbag2npy.py: Extracts only the specified information (topic data) from rosbag data and converts it to npz format.</li> <li>2_make_dataset.py: This program performs three processes: First, formatting the data length. Even if the <code>--duration</code> argument is set at <code>rosbag record</code>, the time-series length of the data differs depending on the timing of program execution, so it is necessary to align the time-series length of all the data. The second is to sort and save training and test data based on a specified index. The third is to calculate the normalization parameters (upper and lower limits) for the joint angles. For details of this process, please click here.</li> <li>3_check_data.py: A program to visualize the collected data, this program saves the image and joint angles of the robot as gifs. Before executing the training program, be sure to check the cropping range of the image and the normalized range of the joint angles.</li> <li>utils.py: Pre-processing programs (e.g., normalization) required for the data set are stored.</li> <li>bag: The collected <code>rosbag</code> data are stored.</li> <li>data: After running <code>2_make_dataset.py</code>, the training and test data and the normalization parameters for the joint angles are saved.</li> <li>output: The visualization results are saved. The number at the end of the file name indicates the index of the training data.</li> </ul>"},{"location":"teach/dataset/#data-extraction","title":"Data extraction","text":"<p>The following command can extract only the specified information (topic data) from rosbag data. The details of the arguments are as follows:</p> <ul> <li>bag_dir: Specify the directory where rosbag data are stored.</li> <li>freq: Since the sampling rate (Hz) varies by sensor, the data is extracted and stored at the specified sampling rate.</li> </ul> <pre><code>$ python3 1_rosbag2npz.py ./bag/ --freq 10\nFailed to load Python extension for LZ4 support. LZ4 compression will not be available.\n./bag/rollout_001.bag\n1664630681.9616075\n1664630682.0616074\n1664630682.1616075\n1664630682.2616074\n</code></pre> <p>Since saving all topics in the npz file consumes a huge amount of memory, this script saves the robot sensor information (camera image, joint angle, and gripper state) as an example. In line 31-35, the names of the topics to be saved are listed, and in lines 50-87, data is extracted from the messages of each topic and saved in a list prepared in advance. Note that saving the camera image as it is requires a huge amount of space, so it is recommended to resize or crop the image in advance. Even if sampling is performed at regular intervals, the data length of the topics may differ depending on the start and end timing of the rosbag record, so the time series length is adjusted after line 95. The program can be applied to the user's own robot by changing the topic name and data extraction method.</p> [SOURCE] 1_rosbag2npz.py<pre><code>import os\nimport cv2\nimport glob\nimport rospy\nimport rosbag\nimport argparse\nimport numpy as np\nparser = argparse.ArgumentParser()\nparser.add_argument(\"bag_dir\", type=str)\nparser.add_argument(\"--freq\", type=float, default=10)\nargs = parser.parse_args()\nfiles = glob.glob(os.path.join(args.bag_dir, \"*.bag\"))\nfiles.sort()\nfor file in files:\nprint(file)\nsavename = file.split(\".bag\")[0] + \".npz\"\n# Open the rosbag file\nbag = rosbag.Bag(file)\n# Get the start and end times of the rosbag file\nstart_time = bag.get_start_time()\nend_time = bag.get_end_time()\n# Get the topics in the rosbag file\n# topics = bag.get_type_and_topic_info()[1].keys()\ntopics = [\n\"/torobo/joint_states\",\n\"/torobo/head/see3cam_left/camera/color/image_repub/compressed\",\n\"/torobo/left_hand_controller/state\",\n]\n# Create a rospy.Time object to represent the current time\ncurrent_time = rospy.Time.from_sec(start_time)\njoint_list = []\nfinger_list = []\nimage_list = []\nfinger_state_list = []\nprev_finger = None\nfinger_state = 0\n# Loop through the rosbag file at regular intervals (args.freq)\nfreq = 1.0 / float(args.freq)\nwhile current_time.to_sec() &lt; end_time:\nprint(current_time.to_sec())\n# Get the messages for each topic at the current time\nfor topic in topics:\nfor topic_msg, msg, time in bag.read_messages(topic):\nif time &gt;= current_time:\nif topic == \"/torobo/joint_states\":\njoint_list.append(msg.position[7:14])\nif topic == \"/torobo/head/see3cam_left/camera/color/image_repub/compressed\":\nnp_arr = np.frombuffer(msg.data, np.uint8)\nnp_img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\nnp_img = np_img[::2, ::2]\nimage_list.append(np_img[150:470, 110:430].astype(np.uint8))\nif topic == \"/torobo/left_hand_controller/state\":\nfinger = np.array(msg.desired.positions[3])\nif prev_finger is None:\nprev_finger = finger\nif finger - prev_finger &gt; 0.005 and finger_state == 0:\nfinger_state = 1\nelif prev_finger - finger &gt; 0.005 and finger_state == 1:\nfinger_state = 0\nprev_finger = finger\nfinger_list.append(finger)\nfinger_state_list.append(finger_state)\nbreak\n# Wait for the next interval\ncurrent_time += rospy.Duration.from_sec(freq)\nrospy.sleep(freq)\n# Close the rosbag file\nbag.close()\n# Convert list to array\njoints = np.array(joint_list, dtype=np.float32)\nfinger = np.array(finger_list, dtype=np.float32)\nfinger_state = np.array(finger_state_list, dtype=np.float32)\nimages = np.array(image_list, dtype=np.uint8)\n# Get shorter lenght\nshorter_length = min(len(joints), len(images), len(finger), len(finger_state))\n# Trim\njoints = joints[:shorter_length]\nfinger = finger[:shorter_length]\nimages = images[:shorter_length]\nfinger_state = finger_state[:shorter_length]\n# Save\nnp.savez(savename, joints=joints, finger=finger, finger_state=finger_state, images=images)\n</code></pre>"},{"location":"teach/dataset/#generate-traintest-data","title":"Generate train/test data","text":"<p>The following command generates training and test data from the npz file converted in the previous section.</p> <pre><code>$ python3 2_make_dataset.py\n./bag/rollout_001.npz\n./bag/rollout_002.npz\n./bag/rollout_003.npz\n./bag/rollout_004.npz\n./bag/rollout_005.npz\n</code></pre> <p>This program consists of the following three steps, and each generated data is stored in the <code>data</code> folder. First, all data are loaded using the <code>load_data</code> function. Lines 21, 22, 28, and 29 perform the following operations.</p> <ul> <li>resize_img: Resizes the image to the specified size. Based on the <code>cv2.resize</code> function, this function supports time-series images.</li> <li>cos_interpolation: To facilitate learning and prediction of sharply changing 0/1 binary data, such as robot hand open/close commands, cos interpolation are used to reshape the data into smooth open/close commands. For more information, see here.</li> <li>list_to_numpy: Even if you specify a storage time <code>--duration</code> for <code>rosbag record</code>, the sequence length of all rosbag data is not always the same due to the execution timing of the ROS system. Therefore, the data length is standardized and formatted by performing padding processing according to the longest sequence.</li> </ul> <p>Lines 43-46 then sort the training and test data based on the indexes specified by the user (lines 36 and 37). The relationship between the teaching position and the index is shown in the table below. Positions A-E in the table are object position. 4 training data were collected for each teaching position and 1 test data for all positions. In other words, a total of 15 data were collected. When the model is evaluated using only the test data collected at the teaching positions, it is difficult to acquire generalization behavior at unlearned positions due to overlearning at the teaching positions. Therefore, it is important to include even a small amount of untrained positions in the test data in order to acquire generalization performance in a variety of positions.</p> <p>Finally, in lines 49-50, the upper and lower limits of each joint angle are calculated and stored as normalization parameters for the joint angles. For more information on why the upper and lower limits of the joint angles are calculated, see here.</p> Position A B C D E train 0,1,2,3 None 5,6,7,8 None 10,11,12,13 test 4 15 9 16 14 [SOURCE] 2_make_dataset.py<pre><code>import os\nimport cv2\nimport glob\nimport argparse\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom eipl.utils import resize_img, calc_minmax, list_to_numpy, cos_interpolation\ndef load_data(dir):\njoints = []\nimages = []\nseq_length = []\nfiles = glob.glob(os.path.join(dir, \"*.npz\"))\nfiles.sort()\nfor filename in files:\nprint(filename)\nnpz_data = np.load(filename)\nimages.append(resize_img(npz_data[\"images\"], (128, 128)))\nfinger_state = cos_interpolation(npz_data[\"finger_state\"])\n_joints = np.concatenate((npz_data[\"joints\"], finger_state), axis=-1)\njoints.append(_joints)\nseq_length.append(len(_joints))\nmax_seq = max(seq_length)\nimages = list_to_numpy(images, max_seq)\njoints = list_to_numpy(joints, max_seq)\nreturn images, joints\nif __name__ == \"__main__\":\n# dataset index\ntrain_list = [0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13]\ntest_list = [4, 9, 14, 15, 16]\n# load data\nimages, joints = load_data(\"./bag/\")\n# save images and joints\nnp.save(\"./data/train/images.npy\", images[train_list].astype(np.uint8))\nnp.save(\"./data/train/joints.npy\", joints[train_list].astype(np.float32))\nnp.save(\"./data/test/images.npy\", images[test_list].astype(np.uint8))\nnp.save(\"./data/test/joints.npy\", joints[test_list].astype(np.float32))\n# save joint bounds\njoint_bounds = calc_minmax(joints)\nnp.save(\"./data/joint_bounds.npy\", joint_bounds)\n</code></pre>"},{"location":"teach/dataset/#visualization-of-datasets","title":"Visualization of datasets","text":"<p>The following command will save the robot's image and joint angles as a gif animation. The argument <code>idx</code> is the index of the data to be visualized. The result shows that the joint angles range from [-0.92, 1.85] to [0.1, 0.9], which is within the normalized range specified by the user. The following figure shows the actually generated GIF animation, from left to right: camera image, robot joint angles, and robot joint angles after normalization. If the cropping of the image or the normalization range of the joint angle is different from the expected range, it is highly likely that errors occurred in the <code>resize_img</code> and <code>calc_minmax</code> processes in the previous section.</p> <pre><code>$ python3 3_check_data.py --idx 4\nload test data, index number is 4\nJoint: shape=(5, 187, 8), min=-0.92, max=1.85\nNorm joint: shape=(5, 187, 8), min=0.1, max=0.9\n</code></pre> <p></p> [SOURCE] 3_check_data.py<pre><code>import argparse\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib.animation as anim\nfrom eipl.utils import normalization\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--idx\", type=int, default=0)\nargs = parser.parse_args()\nidx = int(args.idx)\njoints = np.load(\"./data/test/joints.npy\")\njoint_bounds = np.load(\"./data/joint_bounds.npy\")\nimages = np.load(\"./data/test/images.npy\")\nN = images.shape[1]\n# normalized joints\nminmax = [0.1, 0.9]\nnorm_joints = normalization(joints, joint_bounds, minmax)\n# print data information\nprint(\"load test data, index number is {}\".format(idx))\nprint(\"Joint: shape={}, min={:.3g}, max={:.3g}\".format(joints.shape, joints.min(), joints.max()))\nprint(\n\"Norm joint: shape={}, min={:.3g}, max={:.3g}\".format(\nnorm_joints.shape, norm_joints.min(), norm_joints.max()\n)\n)\n# plot images and normalized joints\nfig, ax = plt.subplots(1, 3, figsize=(14, 5), dpi=60)\ndef anim_update(i):\nfor j in range(3):\nax[j].cla()\n# plot image\nax[0].imshow(images[idx, i, :, :, ::-1])\nax[0].axis(\"off\")\nax[0].set_title(\"Image\")\n# plot joint angle\nax[1].set_ylim(-1.0, 2.0)\nax[1].set_xlim(0, N)\nax[1].plot(joints[idx], linestyle=\"dashed\", c=\"k\")\nfor joint_idx in range(8):\nax[1].plot(np.arange(i + 1), joints[idx, : i + 1, joint_idx])\nax[1].set_xlabel(\"Step\")\nax[1].set_title(\"Joint angles\")\n# plot normalized joint angle\nax[2].set_ylim(0.0, 1.0)\nax[2].set_xlim(0, N)\nax[2].plot(norm_joints[idx], linestyle=\"dashed\", c=\"k\")\nfor joint_idx in range(8):\nax[2].plot(np.arange(i + 1), norm_joints[idx, : i + 1, joint_idx])\nax[2].set_xlabel(\"Step\")\nax[2].set_title(\"Normalized joint angles\")\nani = anim.FuncAnimation(fig, anim_update, interval=int(N / 10), frames=N)\nani.save(\"./output/check_data_{}.gif\".format(idx))\n</code></pre>"},{"location":"teach/environment/","title":"Setup","text":""},{"location":"teach/environment/#ros","title":"ROS","text":"<p>Here, we use the rospy and rosbag packages to extract data from <code>rosbag</code>. If you are generating data sets in a ROS installed environment, the following process is not necessary, so please go to the next chapter.</p>"},{"location":"teach/environment/#pyenv","title":"pyenv","text":"<p>On the other hand, one approach to using rospy and other software on a PC without ROS installed is rospypi/simple. This package enables the use of binary packages such as rospy and tf2 without installing ROS. Furthermore, since it is compatible with Linux, Windows, and MacOS, the collected data can be easily analyzed on one's own PC environment. In order to prevent conflicts with existing python environments, it is recommended to create a virtual environment using venv. The following is the procedure for creating an environment for rospypi/simple library using venv.</p> <pre><code>$ python3 -m venv ~/.venv/rosbag\n$ source ~/.venv/rosbag/bin/activate\n$ pip install -U pip\n$ pip install --extra-index-url https://rospypi.github.io/simple/ rospy rosbag\n$ pip install matplotlib numpy opencv-python\n</code></pre> <p>Note</p> <p>The authors have not been able to verify that the rospypi/simple library can handle all message data. Especially custom ROS messages have not been tested, so if a program cannot be executed correctly in a virtual environment, it should be executed in a ROS environment.</p>"},{"location":"teach/overview/","title":"Overview","text":"<p>This section describes how to create a dataset for deep predictive learning from robot sensor data <code>rosbag</code> obtained from motion instruction using ROS. For better understanding, it is recommended to download (1.3GB) the collected data and scripts and run them according to the manual.</p>"},{"location":"teach/overview/#task","title":"Experimental Task","text":"<p>The AIREC (AI-driven Robot for Embrace and Care), a smart robot manufactured by Tokyo Robotics, is used to teach object grasping. The following figure shows an overview of the task. Based on the object grasping experience at the teaching positions shown in the figure (three circled points), the generalization performance at the unlearned positions (two points) is evaluated. The training data was collected 4 times for each teaching position for a total of 12 data, and the test data was collected once for each of the 5 locations, including the unlearned positions, for a total of 5 data.</p> <p></p>"},{"location":"teach/overview/#teaching","title":"Motion Teaching","text":"<p>AIREC is a robot system that enables bilateral teleoperation as shown below. The operator can teach a multi-degree-of-freedom robot more intuitively by teaching its motion based on the robot visual image displayed on the monitor and force feedback from the robot. Here, the sensor information (joint angle, camera image, torque information, etc.) of the robot when teaching a task using the teleoperation device is saved in 'rosbag' format, and a dataset is created for the machine learning model in the following sections.</p> <p>Note that it is possible to teach motion to a robot without such a specialized device. In the Real Robot Application section, two types of motion teaching methods using OpenManipulator are described: leader-follower system and joystick.</p> <p> <p></p>"},{"location":"tips/augmentation/","title":"Image Augmentation","text":""},{"location":"tips/augmentation/#transformsrandomaffine","title":"transforms.RandomAffine","text":"<p>transforms.RandomAffine is a function for applying a random affine transformation to an image. Affine transforms can transform an image by translating, rotating, scaling, or distorting it. The figure below shows the result of translating an image vertically and horizontally. When affine transforms are used for training AutoEncoder, the position information of objects is expressed (extracted) as image features, so that even unlearned positions can be reconstructed appropriately.</p> <p></p>"},{"location":"tips/augmentation/#transformsrandomverticalflip","title":"transforms.RandomVerticalFlip","text":"<p>[transforms.RandomVerticalFlip](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomVerticalFlip.html is a function that randomly flips the input image upside down to increase data diversity.</p> <p></p>"},{"location":"tips/augmentation/#transformsrandomhorizontalflip","title":"transforms.RandomHorizontalFlip","text":"<p>transforms.RandomHorizontalFlip is a function that randomly flips the input image left and right, and can be combined with <code>RandomVerticalFlip</code> to improve the generalization performance of the model.</p> <p></p>"},{"location":"tips/augmentation/#transformscolorjitter","title":"transforms.ColorJitter","text":"<p>transforms.ColorJitter is a function that performs a random color transformation on an input image, allowing the brightness, contrast, saturation, and hue of the image to be changed, as shown in the figure below.</p> <p></p>"},{"location":"tips/augmentation/#gridmask","title":"GridMask","text":"<p>GridMask is a method to increase the diversity of training data by hiding parts of the image using a grid-like pattern1. As shown in the figure below, the model is expected to improve generalization performance by learning image data with some part missing. When applied to a SARNN model, the missing parts of the image will not attract attention, and as a result, the model can explore (learn) spatial attention that is important for motion prediction. Source code is available here.</p> <p></p> <ol> <li> <p>Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Gridmask data augmentation. arXiv preprint arXiv:2001.04086, 2020.\u00a0\u21a9</p> </li> </ol>"},{"location":"tips/normalization/","title":"Pre-process","text":""},{"location":"tips/normalization/#joint_norm","title":"Joint Angle Normalization","text":"<p>Since the robot camera image is represented by an integer value of 256 shades (0-255 = 8 bits), normalization should be performed to fit within an arbitrary range (e.g., [0.0, 1.0]) from [0, 255]. On the other hand, the possible range of robot joint angles varies depending on the joint structure, range of movement, and teaching task. The simplest normalization method is to normalize based on the maximum and minimum values of the training data, but the normalization range is pulled down by the joints with large movements, and it is difficult to learn fine movements. Therefore, by normalizing based on the maximum and minimum values of each joint, it is possible to emphasize and learn fine movements.</p> <p>The following figure shows the results of normalization of joint angles during object grasping operation. From the left are the results of normalization based on the maximum and minimum values of the collected raw data and training data (overall normalization) and normalization based on the maximum and minimum values of each joint angle (joint normalization). In the case of overall normalization, there is no significant change in the waveform after normalization compared to the raw data, and the range of joint angles is simply converted from 0.0 to 1.0. On the other hand, when joint normalization is performed, both rough motion (e.g., gray waveform) and fine motion (blue waveform) are more pronounced because normalization is performed for each joint. This makes it possible to learn robot motions with higher accuracy than with global normalization.</p> <p></p> <p>Note</p> <p>Depending on the task, some joints may not move at all or change only slightly. If joint normalization is applied to such joints, the waveforms after normalization will be greatly distorted, which may adversely affect learning. It is recommended to check the waveforms after normalization and manually adjust the normalization range if such joint waveforms exist. Note that joint normalization is not suitable for originally noisy data such as torque and current values.</p>"},{"location":"tips/normalization/#cos-interpolation","title":"Cosine Interpolation","text":"<p>When a model learns data expressed as ON/OFF, such as opening/closing commands of a robot hand or a PSD (Position Sensitive Detector) sensor, it is possible to facilitate learning by performing smoothing in advance. The following is the result of applying smoothing to the original data (blue square wave) using cosin interpolation, and the smoothness can be adjusted according to the argument (<code>step</code> size).</p> <p></p>"},{"location":"zoo/CAE-RNN/","title":"CAE-RNN","text":"<p>CAE-RNN is a motion generation model consisting of an image feature extraction part and a time series learning part to learn the robot's sensory-motor information1 2. The following figure shows the network structure of the CAE-RNN model, which consists of a Convolutional Auto-Encoder (CAE) that extracts image features from the robot's visual information, and a Recurrent Neural Network (RNN) that learns the time series information of robot's joint angles and image features. CAE-RNN features independent training of the image feature extraction part and the time series learning part, which are trained in the order of CAE and RNN. By learning a variety of sensory-motor information, it is possible to extract image features such as the position and shape of flexible objects that are conventionally difficult to recognize, and to learn and generate corresponding motions. This section describes a series of processes from the CAE and RNN model implementation, training, inference, internal representation analysis.</p> <p></p>"},{"location":"zoo/CAE-RNN/#cae","title":"CAE","text":""},{"location":"zoo/CAE-RNN/#cae_overview","title":"Overview","text":"<p>Since visual images are high-dimensional information compared to the robot's motion information, it is necessary to align the dimensions of each modal in order to properly learn sensory-motor information.  Furthermore, in order to learn the relationship between the position and motion of the object, it is necessary to extract low-dimensional image features (e.g., position, color, shape, etc.) of the object or robot's body from the high-dimensional visual image. Therefore, Convolutional Auto-Encoder (CAE) is used to extract image features. The following figure highlights only the CAE network structure in CAE-RNN, which consists of an Encoder that extracts image features from the robot's visual information ($i_t$) and a Decoder that reconstructs the image ($\\hat i_t$) from the image features. By updating the weights of each layer to minizize the error between input and output values, the layer with the fewest number of neurons (bottleneck layer) in the middle layer is able to extract an abstract representation of the input information. </p> <p></p>"},{"location":"zoo/CAE-RNN/#cae_files","title":"Files","text":"<p>The programs and folders used in CAE are as follows:</p> <ul> <li>bin/train.py: Programs to load data, train, and save models.</li> <li>bin/test.py: Program to perform off-line inference of models using test data (images and joint angles) and visualize inference results.</li> <li>bin/extract.py\uff1aProgram to calculate and store the image features extracted by the CAE and the upper and lower limits for normalization.</li> <li>libs/trainer.py\uff1aBack propagation class for CAE.</li> <li>log: Folder to store weights, learning curves, and parameter information.</li> <li>output: Save inference results.</li> <li>data\uff1aStore RNN training data (joint angles, image features, normalization information, etc.).</li> </ul>"},{"location":"zoo/CAE-RNN/#cae_model","title":"CAE Model","text":"<p>CAE consists of a convolution layer, transposed convolution layer, and a linear layer. By using the Convolution layer (CNN) to extract image features, CAE can handle high-dimensional information with fewer parameters compared to AutoEncoder3, which consists of only a Linear layer. Furthermore, CNN can extract a variety of image features by convolving with shifting filters. The Pooling layer, which is generally applied after CNN, is often used in image recognition and other fields to compress the dimensionality of input data. However, while position invariance and information compression can be achieved simultaneously, there is a problem that information on the spatial structure of the image is lost 4. Since spatial position information of manipulated objects and robot hands is essential for robot motion generation, dimensional compression is performed using the convolution application interval (stride) of the CNN filter instead of the Pooling Layer.</p> <p>The following is a program of the CAE model, which can extract image features of the dimension specified by <code>feat_dim</code> from a 128x128 pixel color image. This model is a simple network structure to understand the outline and implementation of CAE.</p> [SOURCE] BasicCAE.py<pre><code>class BasicCAE(nn.Module):\ndef __init__(self, feat_dim=10):\nsuper(BasicCAE, self).__init__()\n# encoder\nself.encoder = nn.Sequential(\nnn.Conv2d(3, 64, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(64, 32, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(32, 16, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(16, 12, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(12, 8, 3, 2, 1),\nnn.Tanh(),\nnn.Flatten(),\nnn.Linear(8 * 4 * 4, 50),\nnn.Tanh(),\nnn.Linear(50, feat_dim),\nnn.Tanh(),\n)\n# decoder\nself.decoder = nn.Sequential(\nnn.Linear(feat_dim, 50),\nnn.Tanh(),\nnn.Linear(50, 8 * 4 * 4),\nnn.Tanh(),\nnn.Unflatten(1, (8, 4, 4)),\nnn.ConvTranspose2d(8, 12, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(12, 16, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(16, 32, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(32, 64, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(64, 3, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\n)\ndef forward(self, x):\nreturn self.decoder(self.encoder(x))\n</code></pre> <p>By using the <code>ReLU</code> function and <code>Batch Normalization</code>5, it is possible to improve the expressiveness of each layer, prevent gradient loss, and furthermore make learning more efficient and stable. In this library, CAE models using <code>Batch Normalization</code> have already been implemented and can be loaded as follows. The difference between BasicCAENE and CAEBN is the structure of the model (parameter size), see source code for details. Note that the input format of the implemented model is a color image of 128x128 pixels; if you want to input any other image size, you need to modify the parameters.</p> <pre><code>from eipl.model import BasicCAENE, CAEBN\n</code></pre>"},{"location":"zoo/CAE-RNN/#cae_bp","title":"Back Propagation","text":"<p>In the CAE learning process, input camera images of the robot ($i_t$) and generate the reconstructed images ($\\hat i_t$). Here, the parameters of the model are updated using the back propagation method6 to minimize the error between the input and reconstructed images. In lines 27-33, the batch size image <code>xi</code> is input to the model to obtain the reconstructed image <code>yi_hat</code>. Then, the mean square error <code>nn.MSELoss</code> between the reconstructed image and the true value <code>yi</code> is calculated, and error propagation is performed based on the error value <code>loss</code>. This autoregressive learning eliminates the need for detailed model design for images, which is required in conventional robotics. Note that in order to extract image features that are robust against a variety of real-world noise, data extension is used to train the model on images with randomly varying brightness, contrast, and position.</p> [SOURCE] trainer.py<pre><code>class Trainer:\ndef __init__(self, model, optimizer, device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, (xi, yi) in enumerate(data):\nxi = xi.to(self.device)\nyi = yi.to(self.device)\nyi_hat = self.model(xi)\nloss = nn.MSELoss()(yi_hat, yi)\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / n_batch\n</code></pre>"},{"location":"zoo/CAE-RNN/#training","title":"Training","text":"<p>We will use <code>Model</code>, <code>Trainer Class</code> and the already implemented main program <code>train.py</code> to train CAE. When the program is run, a folder name (e.g. 20230427_1316_29) is created in the <code>log</code> folder indicating the execution date and time. The folder will contain the trained weights (pth) and the TensorBoard log file. The program can use command line arguments to specify parameters necessary for training, such as model type, number of epochs, batch size, training rate, and optimization method. It also uses the EarlyStopping library to determine when to end training early as well as to save weights when the test error is minimized (<code>save_ckpt=True</code>). Please see the comments in the code for a detailed description of how the program works.</p> <pre><code>$ cd eipl/tutorials/cae/\n$ python3 train.py\n[INFO] Set tag = 20230427_1316_29\n================================\nbatch_size : 128\ndevice : 0\nepoch : 100000\nfeat_dim : 10\nlog_dir : log/\nlr : 0.001\nmodel : CAE\noptimizer : adam\nstdev : 0.02\ntag : 20230427_1316_29\nvmax : 1.0\nvmin : 0.0\n================================\n0%|               | 11/100000 [00:40&lt;101:55:18,  3.67s/it, train_loss=0.0491, test_loss=0.0454]\n</code></pre>"},{"location":"zoo/CAE-RNN/#inference","title":"Inference","text":"<p>Check that CAE has been properly trained using the test program <code>test.py</code>. The argument <code>filename</code> is the path of the trained weights file and <code>idx</code> is the index of the data to be visualized. The lower (top) figure shows the inference results of the CAEBN model using this program, with the input image on the left and the reconstructed image on the right. Since the robot hand and the grasping object in the \"unlearned position\" are reconstructed, which is important for generating robot motion, it can be assumed that the image features represent information such as the object's position and shape. The lower figure (bottom) is also an example of failure, showing that the object is not adequately predicted by the Basic CAE model with a simple network structure. In this case, it is necessary to adjust the method of the optimization algorithm, the learning rate, the loss function, and the structure of the model.</p> <pre><code>$ cd eipl/tutorials/cae/\n$ python3 test.py --filename ./log/20230424_1107_01/CAEBN.pth --idx 4\n$ ls output/\nCAEBN_20230424_1107_01_4.gif\n</code></pre> <p></p> <p></p>"},{"location":"zoo/CAE-RNN/#extract-image-features","title":"Extract image features","text":"<p>Extract image features of CAE as a preparation for time series learning of image features and robot joint angles with RNN. Executing the following program, image features and joint angles of training and test data are stored in the <code>data</code> folder in npy format. At this time, confirm that the number of data and time series length of the extracted image features and joint angles are the same. The reason for storing the joint angles again is to make it easier to load the dataset when training RNN.</p> <pre><code>$ cd eipl/tutorials/cae/\n$ python3 extract.py ./log/20230424_1107_01/CAEBN.pth\n[INFO] train data\n==================================================\nShape of joints angle: torch.Size([12, 187, 8])\nShape of image feature: (12, 187, 10)\n==================================================\n[INFO] test data\n==================================================\nShape of joints angle: torch.Size([5, 187, 8])\nShape of image feature: (5, 187, 10)\n==================================================\n$ ls ./data/*\ndata/test:\nfeatures.npy  joints.npy\n\ndata/train:\nfeatures.npy  joints.npy\n</code></pre> <p>The following code is part of the source code of <code>extract.py</code>, which extracts and saves image features. In the fourth line, the Encoder process of CAE is performed and the extracted low-dimensional image features are returned as the return value. The image features extracted by CAE are normalized to within the range specified by the user, and then used for training RNN. When <code>tanh</code> is used as the activation function of the model, the upper and lower bounds of the image features (<code>feat_bounds</code>) are constant (-1.0 to 1.0). However, CAEBN uses <code>ReLU</code> for the activation function, so the upper and lower bounds of the image features are undetermined. Therefore, in line 25, the upper and lower bounds of the image features are determined by calculating the maximum and minimum values from the extracted image features of the training and test data.</p> [SOURCE] extract.py<pre><code>    # extract image feature\nfeature_list = []\nfor i in range(N):\n_features = model.encoder(images[i])\nfeature_list.append( tensor2numpy(_features) )\nfeatures = np.array(feature_list)\nnp.save('./data/joint_bounds.npy', joint_bounds )\nnp.save('./data/{}/features.npy'.format(data_type), features )\nnp.save('./data/{}/joints.npy'.format(data_type), joints )\nprint_info('{} data'.format(data_type))\nprint(\"==================================================\")\nprint('Shape of joints angle:',  joints.shape)\nprint('Shape of image feature:', features.shape)\nprint(\"==================================================\")\nprint()\n# save features minmax bounds\nfeat_list = []\nfor data_type in ['train', 'test']:\nfeat_list.append( np.load('./data/{}/features.npy'.format(data_type) ) )\nfeat = np.vstack(feat_list)\nfeat_minmax = np.array( [feat.min(), feat.max()] )\nnp.save('./data/feat_bounds.npy', feat_minmax )\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn","title":"RNN","text":""},{"location":"zoo/CAE-RNN/#rnn_overview","title":"Overview","text":"<p>A Recurrent Neural Network (RNN) is used to integrate and learn the robot's sensory-motor information. The following figure highlights only the network structure of RNN among CAE-RNNs, which inputs image features ($f_t$) and joint angles ($a_t$) at time $t$ and predicts them at the next time $t+1$.</p> <p></p>"},{"location":"zoo/CAE-RNN/#rnn_files","title":"Files","text":"<p>The programs and folders used in RNN are as follows:</p> <ul> <li>bin/train.py: Programs to load data, train, and save models.</li> <li>bin/test.py: Program to perform off-line inference of models using test data (images and joint angles) and visualize inference results.</li> <li>bin/test_pca_cnnrnn.py: Program to visualize the internal state of RNN using Principal Component Analysis.</li> <li>libs/fullBPTT.py: Back propagation class for time series learning.</li> <li>bin/rt_predict.py: Program that integrates trained CAE and RNN model to predict motor command based on images and joint angles.</li> <li>libs/dataloader.py: DataLoader for RNN, returning image features and joint angles.</li> <li>log: Folder to store weights, learning curves, and parameter information.</li> <li>output: Save inference results.</li> </ul>"},{"location":"zoo/CAE-RNN/#rnn_model","title":"RNN Model","text":"<p>RNN is a neural network that can learn and infer time-series data, and it can perform time-series prediction by sequentially changing states based on input values. However, Vanilla RNN is prone to gradient loss during bask propagation, to solve this problem, Long Short-Term Memory (LSTM) and Multiple Timescales RNN (MTRNN) have been proposed.</p> <p>Here, we describe a method for learning integrated sensory-motor information of a robot using LSTM. LSTM has three gates (input gate, forget gate, and output gate), each with its own weight and bias. The $h_{t-1}$ gate learns detailed changes in the time series as short-term memory, and the $c_{t-1}$ gate learns features of the entire time series as long-term memory, allowing retention and forgetting of past information through each gate. The following shows an example of implementation. Input value <code>x</code>, which is a combination of low-dimensional image features and robot joint angles extracted by CAE in advance, is input to LSTM. LSTM then outputs the predicted value <code>y_hat</code> of the image features and robot joint angles at the next time based on the internal state.</p> BasicRNN.py<pre><code>class BasicLSTM(nn.Module):\ndef __init__(self, in_dim, rec_dim, out_dim, activation=\"tanh\"):\nsuper(BasicLSTM, self).__init__()\nif isinstance(activation, str):\nactivation = get_activation_fn(activation)\nself.rnn = nn.LSTMCell(in_dim, rec_dim)\nself.rnn_out = nn.Sequential(nn.Linear(rec_dim, out_dim), activation)\ndef forward(self, x, state=None):\nrnn_hid = self.rnn(x, state)\ny_hat = self.rnn_out(rnn_hid[0])\nreturn y_hat, rnn_hid\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_bptt","title":"Backpropagation Through Time","text":"<p>Backpropagation Through Time (BPTT) is used as the error back propagation algorithm for time series learning. The details of BPTT have already been described in SARNN, please refer to here.</p> [SOURCE] fullBPTT.py<pre><code>class fullBPTTtrainer:\ndef __init__(self, model, optimizer, device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, (x, y) in enumerate(data):\nx = x.to(self.device)\ny = y.to(self.device)\nstate = None\ny_list = []\nT = x.shape[1]\nfor t in range(T - 1):\ny_hat, state = self.model(x[:, t], state)\ny_list.append(y_hat)\ny_hat = torch.permute(torch.stack(y_list), (1, 0, 2))\nloss = nn.MSELoss()(y_hat, y[:, 1:])\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / (n_batch + 1)\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_dataloader","title":"Dataloader","text":"<p>We describe DataLoader for learning image features and robot joint angles extracted by CAE with RNN. As shown in lines 15 and 16, gaussian noise is added to the input data. By training the model to minimize the error between the prediction values and the original data, the robot can predict appropriate motion commands even if noise is added in the real world.</p> [SOURCE] dataloader.py<pre><code>class TimeSeriesDataSet(Dataset):\ndef __init__(self, feats, joints, minmax=[0.1, 0.9], stdev=0.02):\nself.stdev = stdev\nself.feats = torch.from_numpy(feats).float()\nself.joints = torch.from_numpy(joints).float()\ndef __len__(self):\nreturn len(self.feats)\ndef __getitem__(self, idx):\ny_feat = self.feats[idx]\ny_joint = self.joints[idx]\ny_data = torch.concat((y_feat, y_joint), axis=-1)\nx_feat = self.feats[idx] + torch.normal(mean=0, std=self.stdev, size=y_feat.shape)\nx_joint = self.joints[idx] + torch.normal(mean=0, std=self.stdev, size=y_joint.shape)\nx_data = torch.concat((x_feat, x_joint), axis=-1)\nreturn [x_data, y_data]\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_train","title":"Training","text":"<p>The main program <code>train.py</code> is used to train RNN. When the program is run, the trained weights (pth) and Tensorboard log files are saved in the <code>log</code> folder. Please see the comments in the code for a detailed description of how the program works.</p> <pre><code>$ cd eipl/tutorials/rnn/\n$ python3 ./bin/train.py --device -1\n[INFO] Set tag = 20230510_0134_03\n================================\nbatch_size : 5\ndevice : -1\nepoch : 100000\nlog_dir : log/\nlr : 0.001\nmodel : LSTM\noptimizer : adam\nrec_dim : 50\nstdev : 0.02\ntag : 20230510_0134_03\nvmax : 1.0\nvmin : 0.0\n================================\n0%|               | 99/100000 [00:25&lt;7:05:03,  3.92it/s, train_loss=0.00379, test_loss=0.00244\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_inference","title":"Inference","text":"<p>Check that RNN has been properly trained using the test program <code>test.py</code>. The arguments <code>filename</code> is the path of the trained weights file, <code>idx</code> is the index of the data you want to visualize, To evaluate the generalization performance of the model, test data collected at untrained location are input and the true values are compared with the predicted values. The figure below shows the RNN prediction results, where the left figure is the robot joint angles and the right figure is the image features. The black dotted line in the figure represents the true value and the colored line represents the predicted value, and since they are almost identical, we can say that motion learning was done appropriately.</p> <pre><code>$ cd eipl/tutorials/rnn/\n$ python3 ./bin/test.py --filename ./log/20230510_0134_03/LSTM.pth --idx 4\n$ ls output/\nLSTM_20230510_0134_03_4.gif\n</code></pre> <p></p>"},{"location":"zoo/CAE-RNN/#rnn_pca","title":"Principal Component Analysis","text":"<p>For an overview and concrete implementation of PCA, see here.</p> <pre><code>$ cd eipl/tutorials/rnn/\n$ python3 ./bin/test_pca_rnn.py --filename log/20230510_0134_03/LSTM.pth\n$ ls output/\nPCA_LSTM_20230510_0134_03.gif\n</code></pre> <p>The figure figure shows the result of visualizing the internal state of RNN using PCA. Each dotted line shows the time-series change of the RNN's internal state, and the internal state transitions sequentially starting from the black circle. The transition trajectory of the internal state is called an attractor. The color of each attractor is indicated by object position, with blue, orange, and green corresponding to taught positions A, C, and E,  and red and purple corresponding to unlearned positions B and D. Since the attractors are self-organized (aligned) according to the object position, it can be said that the behavior is learned (memorized) according to the object position. In particular, since the attractors at the unlearned positions are generated between the taught positions, it is possible to generate unlearned interpolated motions by simply teaching and learning grasping motions with different object positions multiple times.</p> <p></p> <ol> <li> <p>Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Efficient multitask learning with an embodied predictive model for door opening and entry with whole-body control. Science Robotics, 7(65):eaax8177, 2022.\u00a0\u21a9</p> </li> <li> <p>Pin-Chu Yang, Kazuma Sasaki, Kanata Suzuki, Kei Kase, Shigeki Sugano, and Tetsuya Ogata. Repeatable folding task by humanoid robot worker using deep learning. IEEE Robotics and Automation Letters, 2(2):397\u2013403, 2016.\u00a0\u21a9</p> </li> <li> <p>Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.\u00a0\u21a9</p> </li> <li> <p>Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advances in neural information processing systems, 2017.\u00a0\u21a9</p> </li> <li> <p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448\u2013456. pmlr, 2015.\u00a0\u21a9</p> </li> <li> <p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.\u00a0\u21a9</p> </li> </ol>"},{"location":"zoo/CNNRNN/","title":"Overview","text":"<p>Because CAE-RNN trains the image feature extraction part (CAE) and the time series learning part (RNN) independently, parameter adjustment and model training time have been issues. Furthermore, CAE extracts image features that are specialized for dimensional compression of image information, not the image features that are necessarily appropriate for generating robot motions. Therefore, CNNRNN is a motion generation model that can automatically extract image features important for motion generation by simultaneously learning (end-to-end learning) the image feature extraction part (CAE) and the time series learning part (RNN). This allows the robot to focus only on objects that are important to the task and generate motions that are more robust to background changes than CAE-RNN1.</p> <p></p>"},{"location":"zoo/CNNRNN/#files","title":"Files","text":"<p>The programs and folders used in CNNRNN are as follows:</p> <ul> <li>bin/train.py: Programs to load data, train, and save models.</li> <li>bin/test.py: Program to perform off-line inference of models using test data (images and joint angles) and visualize inference results.</li> <li>bin/test_pca_cnnrnn.py: Program to visualize the internal state of RNN using Principal Component Analysis.</li> <li>libs/fullBPTT.py: Back propagation class for time series learning.</li> <li>log: Folder to store weights, learning curves, and parameter information.</li> <li>output: Save inference results.</li> </ul>"},{"location":"zoo/CNNRNN/#model","title":"Model","text":"<p>CNNRNN is a motion generation model capable of learning and inference of multimodal time series data. It predicts the image <code>y_image</code> and joint angle <code>y_joint</code> at the next time $t+1$ based on the image <code>xi</code>, joint angle <code>xv</code> and state <code>state</code> at the previous time $t$.</p> [SOURCE] CNNRNN.py<pre><code>class CNNRNN(nn.Module):\ndef __init__(self, rec_dim=50, joint_dim=8, feat_dim=10):\nsuper(CNNRNN, self).__init__()\n# Encoder\nself.encoder_image = nn.Sequential(\nnn.Conv2d(3, 64, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(64, 32, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(32, 16, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(16, 12, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(12, 8, 3, 2, 1),\nnn.Tanh(),\nnn.Flatten(),\nnn.Linear(8 * 4 * 4, 50),\nnn.Tanh(),\nnn.Linear(50, feat_dim),\nnn.Tanh(),\n)\n# Recurrent\nrec_in = feat_dim + joint_dim\nself.rec = nn.LSTMCell(rec_in, rec_dim)\n# Decoder for joint angle\nself.decoder_joint = nn.Sequential(nn.Linear(rec_dim, joint_dim), nn.Tanh())\n# Decoder for image\nself.decoder_image = nn.Sequential(\nnn.Linear(rec_dim, 8 * 4 * 4),\nnn.Tanh(),\nnn.Unflatten(1, (8, 4, 4)),\nnn.ConvTranspose2d(8, 12, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(12, 16, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(16, 32, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(32, 64, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(64, 3, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\n)\ndef forward(self, xi, xv, state=None):\n# Encoder\nim_feat = self.encoder_image(xi)\nhid = torch.concat([im_feat, xv], -1)\n# Recurrent\nrnn_hid = self.rec(hid, state)\n# Decoder\ny_joint = self.decoder_joint(rnn_hid[0])\ny_image = self.decoder_image(rnn_hid[0])\nreturn y_image, y_joint, rnn_hid\n</code></pre>"},{"location":"zoo/CNNRNN/#bptt","title":"Backpropagation Through Time","text":"<p>Backpropagation Through Time (BPTT) is used as the error back propagation algorithm for time series learning. The details of BPTT have already been described in SARNN, please refer to here.</p> [SOURCE] fullBPTT.py<pre><code>class fullBPTTtrainer:\ndef __init__(self, model, optimizer, loss_weights=[1.0, 1.0], device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.loss_weights = loss_weights\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, ((x_img, x_joint), (y_img, y_joint)) in enumerate(data):\nx_img = x_img.to(self.device)\ny_img = y_img.to(self.device)\nx_joint = x_joint.to(self.device)\ny_joint = y_joint.to(self.device)\nstate = None\nyi_list, yv_list = [], []\nT = x_img.shape[1]\nfor t in range(T - 1):\n_yi_hat, _yv_hat, state = self.model(x_img[:, t], x_joint[:, t], state)\nyi_list.append(_yi_hat)\nyv_list.append(_yv_hat)\nyi_hat = torch.permute(torch.stack(yi_list), (1, 0, 2, 3, 4))\nyv_hat = torch.permute(torch.stack(yv_list), (1, 0, 2))\nloss = self.loss_weights[0] * nn.MSELoss()(yi_hat, y_img[:, 1:]) \\\n                + self.loss_weights[1] * nn.MSELoss()(yv_hat, y_joint[:, 1:])\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / (n_batch + 1)\n</code></pre>"},{"location":"zoo/CNNRNN/#train","title":"Training","text":"<p>The main program <code>train.py</code> is used to train CNNRNN. When the program is run, the trained weights (pth) and Tensorboard log files are saved in the <code>log</code> folder. Please see the comments in the code for a detailed description of how the program works.</p> <pre><code>$ cd eipl/tutorials/cnnrnn/\n$ python3 ./bin/train.py\n[INFO] Set tag = 20230514_1958_07\n================================\nbatch_size : 5\ndevice : 0\nepoch : 100000\nfeat_dim : 10\nimg_loss : 1.0\njoint_loss : 1.0\nlog_dir : log/\nlr : 0.001\nmodel : CNNRNN\noptimizer : adam\nrec_dim : 50\nstdev : 0.02\ntag : 20230514_1958_07\nvmax : 1.0\nvmin : 0.0\n================================\n0%|               | 83/100000 [05:07&lt;99:16:42,  3.58s/it, train_loss=0.0213, test_loss=0.022\n</code></pre>"},{"location":"zoo/CNNRNN/#inference","title":"Inference","text":"<p>Check that CNNRNN has been properly trained using the test program <code>test.py</code>. The arguments <code>filename</code> is the path of the trained weights file, <code>idx</code> is the index of the data you want to visualize, <code>input_param</code> is the mixing coefficient for inference, and more info are here.</p> <pre><code>$ cd eipl/tutorials/cnnrnn/\n$ python3 bin/test.py --filename ./log/20230514_1958_07/CNNRNN.pth --idx 4 --input_param 1.0\n\nimages shape:(187, 128, 128, 3), min=0, max=255\njoints shape:(187, 8), min=-0.8595600128173828, max=1.8292399644851685\nloop_ct:0, joint:[ 0.00226304 -0.7357931  -0.28175825  1.2895856   0.7252841   0.14539993\n-0.0266939   0.00422328]\nloop_ct:1, joint:[ 0.00307412 -0.73363686 -0.2815826   1.2874944   0.72176594  0.1542334\n-0.02719587  0.00325996]\n.\n.\n.\n\n$ ls ./output/\nCNNRNN_20230514_1958_07_4_1.0.gif\n</code></pre> <p>The following figure shows the results of inference at unlearned position. From left to right are the input image, the predicted image, and the predicted joint angles (dotted lines are true values). CNNRNN predicts the next time based on the extracted image features and robot joint angles. It is expected that the image features include information such as the color and position of the grasped object, and it is also important that the predicted image and robot joint angle are predicted appropriately as a set. However, experiments show that while the joint angles are appropriately predicted, only the robot hand is generated in the predicted image. Therefore, it is difficult to generate flexible motions based on object positions because the image features contain \"only\" information on the robot hand.</p> <p></p>"},{"location":"zoo/CNNRNN/#pca","title":"Principal Component Analysis","text":"<p>The following figure shows the results of visualizing the internal state of CNNRNN using principal component analysis. Each dotted line shows the time-series change of the internal state of CNNRNN, and the internal state transitions sequentially starting from the black circle. The color of each attractor is object position, where blue, orange, and green correspond to teaching positions A, C, and E, and red and purple correspond to unlearned positions B and D. Since the attractors are self-organized (aligned) for each teaching position, it can be said that properly learned movements can be generated at the teaching positions. On the other hand, the attractors at the unteaching position are attracted to the attractors at the teaching position, and thus cannot generate interpolated motions. This is due to the fact that the position information of the grasped object could not be extracted as image features.</p> <p></p>"},{"location":"zoo/CNNRNN/#improvement","title":"Model Improvement","text":"<p>In CAE-RNN, generalization performance was ensured by learning various object position information using data augmentation. On the other hand, since CNNRNN learns images and joint angle information at the same time, the data augmentation method for robot joint angles corresponding to changes in image position is a challenge. The following three solutions can be proposed to obtain position generalization performance with CNNRNNs.</p> <ol> <li> <p>Pre-training</p> <p>Only the CAE portion of the CNNRNN is extracted and pre-trained. By learning only the image information using data augmentation, CAE can extract a variety of object location information. Then, end-to-end learning is performed using the pre-trained weights to map images to joint angles. However, since CAE needs to be trained in advance, the training man-hours are equivalent to those of CAE-RNN, so the benefit of CNNRNN is small.</p> </li> <li> <p>Layer Normalization</p> <p>CAE-RNN used <code>BatchNormalization</code>2 as a normalization method to make CAE training stable and fast. However, BatchNormalization has the issues that learning becomes unstable when the batch of dataset is small and it is difficult to apply to recursive neural networks. Therefore, we will improve generalization performance by using <code>Layer Normalization</code>3, which can stably train on small batches of data sets and time-series data.</p> <p>The following figure visualizes the internal state of CNNRNNLN using principal component analysis. The self-organization (alignment) of attractors for each object position allows the robot to properly generate motion even at unlearned positions.</p> <p></p> </li> <li> <p>Spatial Attention</p> <p>Because CAE-RNN and CNNRNN were learning motions based on image features containing various information in the image (position, color, shape, background, lighting conditions, etc.), robustness during motion generation was an issue. Therefore, it is possible to improve robustness by learning spatial coordinates and robot joint angles using a spatial attention mechanism that \"explicitly\" extracts spatial coordinates of locations (work objects and arms) important to the task from images. For more information on the spatial attention mechanism, see here.</p> </li> </ol> <ol> <li> <p>Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, Shuki Goto, and Tetsuya Ogata. Visualization of focal cues for visuomotor coordination by gradient-based methods: a recurrent neural network shifts the attention depending on task requirements. In 2020 IEEE/SICE International Symposium on System Integration (SII), 188\u2013194. IEEE, 2020.\u00a0\u21a9</p> </li> <li> <p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448\u2013456. pmlr, 2015.\u00a0\u21a9</p> </li> <li> <p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\u00a0\u21a9</p> </li> </ol>"},{"location":"zoo/MTRNN/","title":"Overview","text":"<p>MTRNN is a type of RNN consisting of a hierarchical neuron group with different firing rates1. It consists of three layers: an IO layer and context layers (Cf and Cs layers) with different firing rates (time constants), each with recursive inputs. The time constants increase in value from the Cf layer to the Cs layer, and the response speed to the input becomes slower. The input information is output at the Output layer via the Cf and Cs layers. There is no direct coupling between the IO and Cs layers, and they interact through the Cf layer. The MTRNN enables the robot to learn behaviors, with the Cf layer representing the behavior primitives and the Cs layer representing (learning) the combination of these primitives. Compared to LSTM, MTRNN is more interpretable and is often used in our laboratory.</p> <p></p> <ol> <li> <p>Yuichi Yamashita and Jun Tani. Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment. PLoS computational biology, 4(11):e1000220, 2008.\u00a0\u21a9</p> </li> </ol>"},{"location":"zoo/MTRNN/#MTRNN.MTRNNCell","title":"<code>MTRNN.MTRNNCell</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Multiple Timescale RNN.</p> <p>Implements a form of Recurrent Neural Network (RNN) that operates with multiple timescales. This is based on the idea of hierarchical organization in human cognitive functions.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>fast_dim</code> <code>int</code> <p>Number of fast context neurons.</p> required <code>slow_dim</code> <code>int</code> <p>Number of slow context neurons.</p> required <code>fast_tau</code> <code>float</code> <p>Time constant value of fast context.</p> required <code>slow_tau</code> <code>float</code> <p>Time constant value of slow context.</p> required <code>activation</code> <code>string</code> <p>If you set <code>None</code>, no activation is applied (ie. \"linear\" activation: <code>a(x) = x</code>).</p> <code>'tanh'</code> <code>use_bias</code> <code>Boolean</code> <p>whether the layer uses a bias vector. The default is False.</p> <code>False</code> <code>use_pb</code> <code>Boolean</code> <p>whether the recurrent uses a pb vector. The default is False.</p> <code>False</code> <p>Yuichi Yamashita, Jun Tani, \"Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment.\", NeurIPS 2018. https://arxiv.org/abs/1807.03247v2</p> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>class MTRNNCell(nn.Module):\n#:: MTRNNCell\n\"\"\"Multiple Timescale RNN.\n    Implements a form of Recurrent Neural Network (RNN) that operates with multiple timescales.\n    This is based on the idea of hierarchical organization in human cognitive functions.\n    Arguments:\n        input_dim (int): Number of input features.\n        fast_dim (int): Number of fast context neurons.\n        slow_dim (int): Number of slow context neurons.\n        fast_tau (float): Time constant value of fast context.\n        slow_tau (float): Time constant value of slow context.\n        activation (string, optional): If you set `None`, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n        use_bias (Boolean, optional): whether the layer uses a bias vector. The default is False.\n        use_pb (Boolean, optional): whether the recurrent uses a pb vector. The default is False.\n    Yuichi Yamashita, Jun Tani,\n    \"Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment.\", NeurIPS 2018.\n    https://arxiv.org/abs/1807.03247v2\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nfast_dim,\nslow_dim,\nfast_tau,\nslow_tau,\nactivation=\"tanh\",\nuse_bias=False,\nuse_pb=False,\n):\nsuper(MTRNNCell, self).__init__()\nself.input_dim = input_dim\nself.fast_dim = fast_dim\nself.slow_dim = slow_dim\nself.fast_tau = fast_tau\nself.slow_tau = slow_tau\nself.use_bias = use_bias\nself.use_pb = use_pb\n# Legacy string support for activation function.\nif isinstance(activation, str):\nself.activation = get_activation_fn(activation)\nelse:\nself.activation = activation\n# Input Layers\nself.i2f = nn.Linear(input_dim, fast_dim, bias=use_bias)\n# Fast context layer\nself.f2f = nn.Linear(fast_dim, fast_dim, bias=False)\nself.f2s = nn.Linear(fast_dim, slow_dim, bias=use_bias)\n# Slow context layer\nself.s2s = nn.Linear(slow_dim, slow_dim, bias=False)\nself.s2f = nn.Linear(slow_dim, fast_dim, bias=use_bias)\ndef forward(self, x, state=None, pb=None):\n\"\"\"Forward propagation of the MTRNN.\n        Arguments:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n            state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n                   If None, initialize states to zeros.\n            pb (bool): pb vector. Used if self.use_pb is set to True.\n        Returns:\n            new_h_fast (torch.Tensor): Updated fast context state.\n            new_h_slow (torch.Tensor): Updated slow context state.\n            new_u_fast (torch.Tensor): Updated fast internal state.\n            new_u_slow (torch.Tensor): Updated slow internal state.\n        \"\"\"\nbatch_size = x.shape[0]\nif state is not None:\nprev_h_fast, prev_h_slow, prev_u_fast, prev_u_slow = state\nelse:\ndevice = x.device\nprev_h_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_h_slow = torch.zeros(batch_size, self.slow_dim).to(device)\nprev_u_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_u_slow = torch.zeros(batch_size, self.slow_dim).to(device)\n# Update of fast internal state\nnew_u_fast = (1.0 - 1.0 / self.fast_tau) * prev_u_fast + 1.0 / self.fast_tau * (\nself.i2f(x) + self.f2f(prev_h_fast) + self.s2f(prev_h_slow)\n)\n# Update of slow internal state\n_input_slow = self.f2s(prev_h_fast) + self.s2s(prev_h_slow)\nif pb is not None:\n_input_slow += pb\nnew_u_slow = (1.0 - 1.0 / self.slow_tau) * prev_u_slow + 1.0 / self.slow_tau * _input_slow\n# Compute the activation for both fast and slow context states\nnew_h_fast = self.activation(new_u_fast)\nnew_h_slow = self.activation(new_u_slow)\nreturn new_h_fast, new_h_slow, new_u_fast, new_u_slow\n</code></pre>"},{"location":"zoo/MTRNN/#MTRNN.MTRNNCell.forward","title":"<code>forward(x, state=None, pb=None)</code>","text":"<p>Forward propagation of the MTRNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <code>state</code> <code>list</code> <p>Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).    If None, initialize states to zeros.</p> <code>None</code> <code>pb</code> <code>bool</code> <p>pb vector. Used if self.use_pb is set to True.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_h_fast</code> <code>torch.Tensor</code> <p>Updated fast context state.</p> <code>new_h_slow</code> <code>torch.Tensor</code> <p>Updated slow context state.</p> <code>new_u_fast</code> <code>torch.Tensor</code> <p>Updated fast internal state.</p> <code>new_u_slow</code> <code>torch.Tensor</code> <p>Updated slow internal state.</p> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>def forward(self, x, state=None, pb=None):\n\"\"\"Forward propagation of the MTRNN.\n    Arguments:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n        state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n               If None, initialize states to zeros.\n        pb (bool): pb vector. Used if self.use_pb is set to True.\n    Returns:\n        new_h_fast (torch.Tensor): Updated fast context state.\n        new_h_slow (torch.Tensor): Updated slow context state.\n        new_u_fast (torch.Tensor): Updated fast internal state.\n        new_u_slow (torch.Tensor): Updated slow internal state.\n    \"\"\"\nbatch_size = x.shape[0]\nif state is not None:\nprev_h_fast, prev_h_slow, prev_u_fast, prev_u_slow = state\nelse:\ndevice = x.device\nprev_h_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_h_slow = torch.zeros(batch_size, self.slow_dim).to(device)\nprev_u_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_u_slow = torch.zeros(batch_size, self.slow_dim).to(device)\n# Update of fast internal state\nnew_u_fast = (1.0 - 1.0 / self.fast_tau) * prev_u_fast + 1.0 / self.fast_tau * (\nself.i2f(x) + self.f2f(prev_h_fast) + self.s2f(prev_h_slow)\n)\n# Update of slow internal state\n_input_slow = self.f2s(prev_h_fast) + self.s2s(prev_h_slow)\nif pb is not None:\n_input_slow += pb\nnew_u_slow = (1.0 - 1.0 / self.slow_tau) * prev_u_slow + 1.0 / self.slow_tau * _input_slow\n# Compute the activation for both fast and slow context states\nnew_h_fast = self.activation(new_u_fast)\nnew_h_slow = self.activation(new_u_slow)\nreturn new_h_fast, new_h_slow, new_u_fast, new_u_slow\n</code></pre>"},{"location":"zoo/MTRNN/#MTRNN.BasicMTRNN","title":"<code>MTRNN.BasicMTRNN</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>MTRNN Wrapper Module.</p> <p>This module encapsulates the MTRNNCell, adding an output layer to it.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Number of input features.</p> required <code>fast_dim</code> <code>int</code> <p>Number of fast context neurons.</p> required <code>slow_dim</code> <code>int</code> <p>Number of slow context neurons.</p> required <code>fast_tau</code> <code>float</code> <p>Time constant value of fast context.</p> required <code>slow_tau</code> <code>float</code> <p>Time constant value of slow context.</p> required <code>out_dim</code> <code>int</code> <p>Number of output features. If None, set equal to in_dim.</p> <code>None</code> <code>activation</code> <code>string</code> <p>If you set <code>None</code>, no activation is applied (ie. \"linear\" activation: <code>a(x) = x</code>).</p> <code>'tanh'</code> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>class BasicMTRNN(nn.Module):\n#:: BasicMTRNN\n\"\"\"MTRNN Wrapper Module.\n    This module encapsulates the MTRNNCell, adding an output layer to it.\n    Arguments:\n        in_dim (int):  Number of input features.\n        fast_dim (int): Number of fast context neurons.\n        slow_dim (int): Number of slow context neurons.\n        fast_tau (float): Time constant value of fast context.\n        slow_tau (float): Time constant value of slow context.\n        out_dim (int, optional): Number of output features. If None, set equal to in_dim.\n        activation (string, optional): If you set `None`, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n    \"\"\"\ndef __init__(\nself, in_dim, fast_dim, slow_dim, fast_tau, slow_tau, out_dim=None, activation=\"tanh\"\n):\nsuper(BasicMTRNN, self).__init__()\nif out_dim is None:\nout_dim = in_dim\n# Legacy string support for activation function.\nif isinstance(activation, str):\nactivation = get_activation_fn(activation)\nself.mtrnn = MTRNNCell(\nin_dim, fast_dim, slow_dim, fast_tau, slow_tau, activation=activation\n)\n# Output of RNN\nself.rnn_out = nn.Sequential(nn.Linear(fast_dim, out_dim), activation)\ndef forward(self, x, state=None):\n\"\"\"Forward propagation of the BasicMTRNN.\n        Arguments:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n            state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n                   If None, initialize states to zeros.\n        Returns:\n            y_hat (torch.Tensor): Output tensor of shape (batch_size, out_dim).\n            rnn_hid (list): Updated states (h_fast, h_slow, u_fast, u_slow).\n        \"\"\"\nrnn_hid = self.mtrnn(x, state)\ny_hat = self.rnn_out(rnn_hid[0])\nreturn y_hat, rnn_hid\n</code></pre>"},{"location":"zoo/MTRNN/#MTRNN.BasicMTRNN.forward","title":"<code>forward(x, state=None)</code>","text":"<p>Forward propagation of the BasicMTRNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <code>state</code> <code>list</code> <p>Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).    If None, initialize states to zeros.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_hat</code> <code>torch.Tensor</code> <p>Output tensor of shape (batch_size, out_dim).</p> <code>rnn_hid</code> <code>list</code> <p>Updated states (h_fast, h_slow, u_fast, u_slow).</p> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>def forward(self, x, state=None):\n\"\"\"Forward propagation of the BasicMTRNN.\n    Arguments:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n        state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n               If None, initialize states to zeros.\n    Returns:\n        y_hat (torch.Tensor): Output tensor of shape (batch_size, out_dim).\n        rnn_hid (list): Updated states (h_fast, h_slow, u_fast, u_slow).\n    \"\"\"\nrnn_hid = self.mtrnn(x, state)\ny_hat = self.rnn_out(rnn_hid[0])\nreturn y_hat, rnn_hid\n</code></pre>"},{"location":"zoo/overview/","title":"Overview","text":"<p>The newly implemented layers and models will be released sequentially.</p> <ul> <li> Multiple Timescale Recurrent Neural Network</li> <li> CAE-RNN</li> <li> CNNRNN</li> <li> SARNN with image feature (comming soon)</li> <li> Stochastic Recurrent Neural Network (comming soon)</li> </ul>"}]}