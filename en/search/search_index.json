{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>EIPL (Embodied Intelligence with Deep Predictive Learning) is a library for robot motion generation using deep predictive learning developed at the  Ogata Laboratory, Waseda University. Deep predictive learning is a method that enables flexible motion generation for unlearned environments and work goals by predicting the appropriate motion for the real world in real time based on past learning experience. In this study, we use the humanoid robot AIREC AIREC (AIREC\uff1aAI-driven Robot for Embrace and Care) and Open Manpulator as real robots, which enables systematic learning from model implementation to learning and real-time motion generation. In the future, newly developed motion generation models using EIPL will be published in the Model Zoo. Below is an overview of each chapter.</p> <ol> <li> <p>Deep Predictive Learning</p> <p>This section explains the concept of deep predictive learning and outlines the three steps towards robot implementation: motion teaching, learning, and motion generation.</p> </li> <li> <p>Set Up</p> <p>This section provides instructions on how to install EIPL and verify the program using pre-trained weights.</p> </li> <li> <p>Motion Teaching</p> <p>This section describes the process of extracting data from ROSbag files and creating datasets. EIPL provides a sample dataset of object grasping motion using AIREC.</p> </li> <li> <p>Teaching Model</p> <p>Using the attention mechanism based motion generation model as an example, this section explains the implementation steps for training the model and performing inference.</p> </li> <li> <p>Simulator Application</p> <p>This section describes motion learning using a robot simulator (robosuite).</p> </li> <li> <p>Real Robot Application</p> <p>This section provides a detailed explanation of the procedures involved in applying motion learning to real robot control using Open Manpulator.</p> </li> <li> <p>Model Zoo</p> <p>The motion generation models developed with EIPL will be gradually released in the ModelZoo.</p> </li> <li> <p>Tips and Tricks</p> <p>This section provides valuable insights and tips on motion learning techniques.</p> </li> </ol> <p>Acknowledgements</p> <p>This work was supported by JST Moonshot-type R&amp;D Project JPMJMS2031. We would like to express our gratitude.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#q-motion-is-not-smooth","title":"Q. Motion is not smooth.","text":"<p>To generate stable and smooth motion against real-world noise, the sensor information at time $t$ is mixed with the predicted value from the previous time $t-1$ in a certain ratio and then fed into the RNN. This process acts as a low-pass filter, allowing the predicted values from the previous time to supplement the motion prediction even in the presence of noisy sensor values. It is important to note that if the mixing factor (<code>input_param</code>) is too small, it becomes difficult to modify the motion based on real sensor information, resulting in reduced robustness to position changes. When <code>input_param=0.0</code>, the motion is generated based solely on the sensor information obtained at the initial time. Below is an example implementation that shows data mixing using the robot's camera image and joint angles.</p> <pre><code>x_image, x_joint = robot.get_sensor_data()\nif loop_ct &gt; 1:\nx_image = x_image * input_param + y_image * (1.0-input_param)\nx_joint = x_joint * input_param + y_joint * (1.0-input_param)\ny_image , y_joint, state = mode(x_image, x_joint, state)\n</code></pre>"},{"location":"faq/#q-the-predicted-image-looks-abnormal","title":"Q. The predicted image looks abnormal.","text":"<p>When applying a trained model to a real robot, there may be problems such as objects not being visible in the predicted image, or the image being noisy. This could be due to automatic changes in camera parameters (e.g. white balance). To avoid this, it is recommended to fix the camera parameters during motion teaching, or to adjust the inference camera parameters to match the visual image during motion teaching.</p>"},{"location":"faq/#q-the-model-does-not-focus-on-the-object","title":"Q. The model does not focus on the object.","text":"<ol> <li> <p>Adjust the camera position</p> <p>It is recommended to make sure that the robot's body (hand or arm) and the object are consistently displayed in the image to facilitate stable attention and movement. When attention is directed to both the robot's body and the object, it becomes easier to learn the temporal relationship between them.</p> </li> <li> <p>Enlarging the object</p> <p>Consider physically enlarging the object, cropping the image around the object, or moving the camera closer to the object.</p> </li> <li> <p>Retrain the model</p> <p>The initial weights of the model may cause inattention to the objects. Training the model multiple times with the same parameters has yielded positive results 3 out of 5 times.</p> </li> </ol>"},{"location":"faq/#q-how-can-i-customize-the-data-loader","title":"Q. How can I customize the data loader?","text":"<p>It is possible to add or remove any sensor by adjusting the amount of data passed to the <code>MultimodalDataset</code> class or by modifying the input/output definitions. Here is an example that shows how to add a new torque sensor.</p> <pre><code>class MultimodalDataset(Dataset):\ndef __init__(self, images, joints, torque, stdev=0.02):\npass\ndef __getitem__(self, idx):\nreturn [[x_img, x_joint, x_torque], [y_img, y_joint, y_torque]]\n</code></pre>"},{"location":"license/","title":"License","text":"<p>This software, EIPL (Embodied Intelligence with Deep Predictive Learning), is licensed for redistribution and/or modification under the terms of the GNU Affero General Public License version 3 (GNU AGPLv3), as published by the Free Software Foundation.</p>"},{"location":"overview/","title":"Deep Predictive Learning","text":""},{"location":"overview/#dpl-overview","title":"Overview","text":"<p>Deep Predictive Learning (Deep Predictive Learning) is a robot motion generation method developed with reference to the free energy principle, which unifies and explains the various functions of the brain1. A recurrent coupled neural network (RNN) model is trained to minimize the prediction error between the sensory-motor information at time (t) and the next time (t+1) using the time-series information of the robot's motion and sensation when the robot experiences motion in the real world using teleoperation, etc. At runtime, it is possible to predict the robot's near-future sensations and actions in real time from its sensory-motor information, and to execute actions such that the resulting prediction error is minimized by the attracting action of attractors in the RNN. The figure below shows a robot implementation of deep predictive learning, which consists of three steps: motion teaching, learning, and motion generation.</p> <p></p>"},{"location":"overview/#dpl-teach","title":"Motion Teaching","text":"<p>In deep predictive learning, motion generation models are acquired in a data-driven manner using the robot's sensorimotor information (time-series data comprising motion and sensor information) as training data. Therefore, the training data must contain information about the interaction between the robot's body and the environment. In addition, since the quantity and quality of the training data influence model performance, high-quality demonstration data must be collected efficiently. </p> <p>In Phase 1, the training data is collected by performing the desired motions on the robot and recording motion information, such as joint angles, and sensor information, such as camera images, at a constant sampling rate. Typical teaching methods for robot motion include program description2, direct teaching3, and teleoperation1. Although describing the robot's motion in advance using a robot programming language is simple, it may not be feasible due to the complexity of the description when a robot needs to perform a long-term motion. In contrast, training data without precise modeling and parameter adjustment can be obtained by teaching movements through human manipulation of the robot. Among them, remotely controlling the manipulator from the actual robot's perspective (Wizard of Oz4) is desirable because it can intuitively teach the robot human manipulation skills for a task. The operator interacts naturally with the environment by controlling the robot as if he were controlling his body. In addition, since the operator makes decisions about actions based on information obtained from sensor information, the acquired teaching dataset is expected to contain information necessary for motion learning and be effective in acquiring data for model training.</p>"},{"location":"overview/#dpl-train","title":"Training","text":"<p>In deep predictive learning, the learning target is the time-series relationships between sensorimotor information in a system in which the environment and the body interact dynamically. The training data are not labeled with correct answers, and the model is trained to predict the robot state ($\\hat i_t, \\hat s_{t+1}$) in the next step using the current robot state as input ($i_t, s_t$). This autoregressive learning eliminates the need for the detailed design of a physical model of the environment, as required in conventional robotics. In addition, the model can be represented as a dynamic system integrating environment recognition and motion generation functions across multiple modalities.</p> <p>The model consists of feature extraction and time series learning parts to learn the robot's sensorimotor information. The feature extraction part extracts features from sensor information values acquired by the robot, and the time series learning part learns sensorimotor information that integrates the extracted features and robot motion information ( e.g., joint angles and torque). Although each part is connected end-to-end (CNNRNN, SARNN) or learned independently (CAE-RNN), the roles of each part are explicitly separated in this manual.</p>"},{"location":"overview/#dpl-execute","title":"Motion Generation","text":"<p>When performing the task, RNN performs three processes sequentially: (1) Acquire sensor information from the robot, (2) predict the next state based on the sensor information, and (3) send control commands to the robot based on the predicted values. By performing the forward computation of the model at each step, RNN predicts the robot's state for the next time-step based on the context information and inputs it holds internally. The RNN output is then used as the target state to control each joint. By repeating the above process online, the RNN predicts the sensorimotor motion of the robot while sequentially changing the state of each neuron in the context layer. Based on the results of this prediction and the prediction error in the real environment, the robot generates motions that dynamically respond to the input.</p> <p>Another advantage of using deep learning models for motion generation is the online motion generation speed. The proposed framework comprises lightweight models, and the computational time and cost required for motion generation are low. Moreover, implementing each of the previous functions as components makes it possible to easily reuse the implemented system when tasks or robot hardware changes, or devices are added5.</p> <ol> <li> <p>Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Efficient multitask learning with an embodied predictive model for door opening and entry with whole-body control. Science Robotics, 7(65):eaax8177, 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kanata Suzuki, Momomi Kanamura, Yuki Suga, Hiroki Mori, and Tetsuya Ogata. In-air knotting of rope using dual-arm robot based on deep learning. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 6724\u20136731. IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p>Hideyuki Ichiwara, Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Contact-rich manipulation of a flexible object based on deep predictive learning using vision and tactility. In 2022 International Conference on Robotics and Automation (ICRA), 5375\u20135381. IEEE, 2022.\u00a0\u21a9</p> </li> <li> <p>Pin-Chu Yang, Kazuma Sasaki, Kanata Suzuki, Kei Kase, Shigeki Sugano, and Tetsuya Ogata. Repeatable folding task by humanoid robot worker using deep learning. IEEE Robotics and Automation Letters, 2(2):397\u2013403, 2016.\u00a0\u21a9</p> </li> <li> <p>Momomi Kanamura, Kanata Suzuki, Yuki Suga, and Tetsuya Ogata. Development of a basic educational kit for robotic system with deep neural networks. Sensors, 21(11):3804, 2021.\u00a0\u21a9</p> </li> </ol>"},{"location":"install/install-software/","title":"Overview","text":"<p>EPIL officially supports Linux (Ubuntu 20.04), Python 3.8 and PyTorch with the latest version. In particular, PyTorch 2.0 allows faster training of large models through precompilation, resulting in improved training speed and reduced GPU memory usage. Please make sure that CUDA and Nvidia drivers are installed according to the PyTorch version you are using.</p>"},{"location":"install/install-software/#software-files","title":"Software Files","text":"<p>This library consists of the following components:</p> <ul> <li>data: Implements a sample dataset downloader and a Dataloader for model training.</li> <li>layer: Implements layered models such as(Hierarchical RNNs and spatial attention mechanisms, etc.</li> <li>model: Implements multiple motion generation models, with support for inputs including joint angles (with arbitrary degrees of freedom) and color images (128x128 pixels).</li> <li>test: Contains test programs.</li> <li>utils: Provides functions for normalization, visualization, argument processing, etc.</li> </ul>"},{"location":"install/install-software/#pip_install","title":"Install via pip","text":"<p>To set up the environment, clone the EIPL repository from GitHub and install it using the pip command.</p> <pre><code>mkdir ~/work/\ncd ~/work/\ngit clone https://github.com/ogata-lab/eipl.git\ncd eipl\npip install -r requirements.txt\npip install -e .\n</code></pre>"},{"location":"install/quick-start/","title":"Quick Start","text":"<p>In this section, we run a test program to verify the proper installation of the EIPL environment. We will use the pre-trained weights and the motion generation model with spatial attention mechanism (SARNN: Spatial Attention with Recurrent Neural Network). For specific details on model training methods and model specifications, please refer to the following chapters.</p>"},{"location":"install/quick-start/#inference","title":"Inference","text":"<p>To perform inference using SARNN and the pre-trained weights, execute the <code>test.py</code> file in the tutorial folder. The resulting inferences are saved in the output folder. Specifying the <code>--pretrained</code> argument will automatically download the pre-trained weights and sample data.</p> <pre><code>$ cd eipl/tutorials/sarnn\n$ python3 ./bin/test.py --pretrained\n$ ls ./output/\nSARNN_20230514_2312_17_4_1.0.gif\n</code></pre>"},{"location":"install/quick-start/#results","title":"Results","text":"<p>The figure below shows the inference results. The blue dots in the figure represent the attention points extracted by the Convolutional Neural Network (CNN), while the red dots indicate the attention points predicted by the Recurrent Neural Network (RNN). This visualization shows the prediction of joint angles with a focus on the robot hand and the grasped object.</p> <p></p>"},{"location":"install/quick-start/#help","title":"Help","text":"<p>If an error occurs, there are three possible causes:</p> <ol> <li> <p>Installation error</p> <p>To ensure proper installation, use the \"pip freeze\" command to verify that the libraries are installed correctly. If the library is installed, its version information will be displayed. If not, it is possible that the package was not installed properly, so please check the installation procedure again.</p> <pre><code>pip freeze | grep eipl\n</code></pre> </li> <li> <p>Download error</p> <p>If you have problems downloading the sample data or the pre-trained weights file due to a proxy or other reason, you can manually download the weights file and the data set. Save them to the <code>~/.eipl/</code> folder and then extract the files.</p> <pre><code>$ cd ~/\n$ mkdir -p .eipl/airec/\n$ cd .eipl/airec/\n$ # copy grasp_bottle.tar and pretrained.tar to ~/.eipl/airec/ directory\n$ tar xvf grasp_bottle.tar &amp;&amp; tar xvf pretrained.tar\n$ ls grasp_bottle/*\ngrasp_bottle/joint_bounds.npy\n...\n$ ls pretrained/*\npretrained/CAEBN:\nargs.json  model.pth\n...\n</code></pre> </li> <li> <p>Drawing error</p> <p>If you see the following error message after running the program, it may indicate an error in generating the animation file. In such cases, modifying the code at the end of test.py will solve the problem.</p> <pre><code>File \"/usr/lib/python3/dist-packages/matplotlib/animation.py\", line 410, in cleanup\n    raise subprocess.CalledProcessError(\nsubprocess.CalledProcessError: Command '['ffmpeg', '-f', 'rawvideo', '-vcodec', 'rawvideo', '-s', '720x300', '-pix_fmt', 'rgba', '-r', '52.63157894736842', '-loglevel', 'error', '-i', 'pipe:', '-vcodec', 'h264', '-pix_fmt', 'yuv420p', '-y', './output/CAE-RNN-RT_20230510_0134_03_0_0.8.gif']' returned non-zero exit status 1.\n</code></pre> <p>First, use the <code>apt</code> command to install imagemagick and ffmpeg.</p> <pre><code>$ sudo apt install imagemagick\n$ sudo apt install ffmpeg\n</code></pre> <p>Next, make the following changes to the code at the bottom of <code>test.py</code>:</p> <pre><code># Using imagemagick\nani.save( './output/SARNN_{}_{}_{}.gif'.format(params['tag'], idx, args.input_param), writer=\"imagemagick\")\n\n# Using ffmpeg\nani.save( './output/SARNN_{}_{}_{}.gif'.format(params['tag'], idx, args.input_param), writer=\"ffmpeg\")\n</code></pre> </li> </ol>"},{"location":"model/SARNN/","title":"SARNN","text":"<p>SARNN \"explicitly\" extracts the spatial coordinates of critical positions in the task, such as target objects and arms, from images, and learns the coordinates along with the robot's joint angles of the robot1. This greatly improves robustness to changes in the object's position. The figure below illustrates the network structure of SARNN, which consists of an encoder responsible for extracting image features $f_t$ and object position coordinates $p_t$ from camera images $i_t$, a recurrent module that learns the temporal changes in the robot's joint angles and object position coordinates $p_t$, and a decoder that reconstructs images based on the image features $f_t$ and heat maps $\\hat h_{t+1}$.</p> <p>The upper part of the encoder and decoder consists of CNN layers, including Convolutional and Transposed Convolutional layers, which extract and reconstruct color and shape information of objects from image features. The lower part of the CNN uses the Spatial Softmax layer to extract 2D position information of objects. The Recurrent module only predicts the position information $p_{t+1}$ of the object, which alone is not sufficient to reconstruct the image using the decoder. Therefore, a heat map $\\hat h_{t+1}$ centered on the predicted coordinate information $p_{t+1}$ is generated. By multiplying it with the image features extracted by the CNN in the upper part, a predicted image $\\hat i_{t+1}$ is generated based on the information around the predicted attention point.</p> <p>Here, we show the implementation method and model classes for the distinctive features of SARNN: Spatial Attention Mechanism, Heatmap Generator, Loss Scheduler, and Backpropagation Through Time.</p> <p></p>"},{"location":"model/SARNN/#spatial_softmax","title":"Spatial Attention Mechanism","text":"<p>The spatial attention mechanism emphasizes important information (pixels with large values) by multiplying the feature map by softmax. It then extracts the position information of the highlighted pixels using position encoding. The figure below illustrates the results of the spatial attention mechanism, where important position information (represented by red dots) is extracted by multiplying a \"pseudo\" feature map generated by two randomly generated Gaussian distributions with Softmax. Since CNN feature maps contain diverse information, they are not effectively enhanced by a simple softmax multiplication. To further enhance the features, it is critical to use Softmax with temperature. The effect of Softmax with temperature can be observed by adjusting the <code>temperature</code> parameter in the provided example program. The red dots in the figure indicate the positions extracted by spatial Softmax, and since they are generated at the center of one of the Gaussian distributions, the position information can be extracted accurately.</p> <p></p> [SOURCE] SpatialSoftmax.py<pre><code>class SpatialSoftmax(nn.Module):\ndef __init__(self, width: int, height: int, temperature=1e-4, normalized=True):\nsuper(SpatialSoftmax, self).__init__()\nself.width = width\nself.height = height\nif temperature is None:\nself.temperature = torch.nn.Parameter(torch.ones(1))\nelse:\nself.temperature = temperature\n_, pos_x, pos_y = create_position_encoding(width, height, normalized=normalized)\nself.register_buffer(\"pos_x\", pos_x)\nself.register_buffer(\"pos_y\", pos_y)\ndef forward(self, x):\nbatch_size, channels, width, height = x.shape\nassert height == self.height\nassert width == self.width\n# flatten, apply softmax\nlogit = x.reshape(batch_size, channels, -1)\natt_map = torch.softmax(logit / self.temperature, dim=-1)\n# compute expectation\nexpected_x = torch.sum(self.pos_x * att_map, dim=-1, keepdim=True)\nexpected_y = torch.sum(self.pos_y * att_map, dim=-1, keepdim=True)\nkeys = torch.cat([expected_x, expected_y], -1)\n# keys [[x,y], [x,y], [x,y],...]\nkeys = keys.reshape(batch_size, channels, 2)\natt_map = att_map.reshape(-1, channels, width, height)\nreturn keys, att_map\n</code></pre>"},{"location":"model/SARNN/#heatmap","title":"Heatmap Generator","text":"<p>The Heatmap Generator generates a heatmap centered on specific pixel coordinates that represent the position information. The figure below illustrates a heatmap generated by the heatmap generator, centered on the position extracted by the spatial attention mechanism (indicated by the red dot in the figure). The size of the heatmap can be adjusted using the <code>heatmap_size</code> parameter. A smaller heatmap size considers only the information near the attention point, while a larger size includes some surrounding information in the generated image. It is important to note that if the heatmap is too small, the corresponding predictive image $\\hat i_{t+1}$ may not be generated, while if it is too large, adjustments to the sensitivity parameter may be required to account for changes in the environment, such as background and obstacles.</p> <p></p> [SOURCE] InverseSpatialSoftmax.py<pre><code>class InverseSpatialSoftmax(nn.Module):\ndef __init__(self, width: int, height: int, heatmap_size=0.1, normalized=True):\nsuper(InverseSpatialSoftmax, self).__init__()\nself.width = width\nself.height = height\nself.normalized = normalized\nself.heatmap_size = heatmap_size\npos_xy, _, _ = create_position_encoding(width, height, normalized=normalized)\nself.register_buffer(\"pos_xy\", pos_xy)\ndef forward(self, keys):\nsquared_distances = torch.sum(\ntorch.pow(self.pos_xy[None, None] - keys[:, :, :, None, None], 2.0), axis=2\n)\nheatmap = torch.exp(-squared_distances / self.heatmap_size)\nreturn heatmap\n</code></pre>"},{"location":"model/SARNN/#loss_scheduler","title":"Loss Scheduler","text":"<p>The loss scheduler is a <code>callback</code> function that gradually assigns weights to the prediction error of the attention point based on the number of epochs. It is an important feature for SARNN training. The figure below shows the weighting curve for each <code>curve_name</code> argument, where the horizontal axis represents the number of epochs and the vertical axis represents the weighting value. The decay weighting starts at 0 and gradually reaches the maximum weighting value (e.g. 0.1) at the epoch specified by <code>decay_end</code> (e.g. 100). It is important to note that the maximum weighting value is determined by the <code>__call__</code> method. This class supports five types of curves, as shown in the figure: linear, S-curve, inverse S-curve, decay, and acceleration interpolation.</p> <p></p> <p>The reason for using the error scheduler in SARNN training is to allow the CNN filters to be trained more freely in the early stages. Since the encoder and decoder weights of SARNN are randomly initialized, visual features may not be correctly extracted or learned during the initial learning phase.</p> <p>When the prediction error of attention obtained in such a situation is backpropagated, the attention point may not be correctly directed to the work object. Instead, the attention point that minimizes the \"prediction image error\" is learned. Therefore, by ignoring the prediction error of the attention point at the initial stage of learning, it is possible to obtain an attention point that focuses only on the work object. The attention point prediction error is then learned when the CNN filters have finished learning features. The <code>decay_end</code> parameter sets the learning time of the CNN, which is typically set to about 1000 epochs, but may need to be adjusted depending on the task.</p> [SOURCE] callback.py<pre><code>class LossScheduler:\ndef __init__(self, decay_end=1000, curve_name=\"s\"):\ndecay_start = 0\nself.counter = -1\nself.decay_end = decay_end\nself.interpolated_values = self.curve_interpolation(\ndecay_start, decay_end, decay_end, curve_name\n)\ndef linear_interpolation(self, start, end, num_points):\nx = np.linspace(start, end, num_points)\nreturn x\ndef s_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (t - np.sin(2 * np.pi * t) / (2 * np.pi))\nreturn x\ndef inverse_s_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (t + np.sin(2 * np.pi * t) / (2 * np.pi))\nreturn x\ndef deceleration_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (1 - np.cos(np.pi * t / 2))\nreturn x\ndef acceleration_curve_interpolation(self, start, end, num_points):\nt = np.linspace(0, 1, num_points)\nx = start + (end - start) * (np.sin(np.pi * t / 2))\nreturn x\ndef curve_interpolation(self, start, end, num_points, curve_name):\nif curve_name == \"linear\":\ninterpolated_values = self.linear_interpolation(start, end, num_points)\nelif curve_name == \"s\":\ninterpolated_values = self.s_curve_interpolation(start, end, num_points)\nelif curve_name == \"inverse_s\":\ninterpolated_values = self.inverse_s_curve_interpolation(start, end, num_points)\nelif curve_name == \"deceleration\":\ninterpolated_values = self.deceleration_curve_interpolation(start, end, num_points)\nelif curve_name == \"acceleration\":\ninterpolated_values = self.acceleration_curve_interpolation(start, end, num_points)\nelse:\nassert False, \"Invalid curve name. {}\".format(curve_name)\nreturn interpolated_values / num_points\ndef __call__(self, loss_weight):\nself.counter += 1\nif self.counter &gt;= self.decay_end:\nreturn loss_weight\nelse:\nreturn self.interpolated_values[self.counter] * loss_weight\n</code></pre>"},{"location":"model/SARNN/#bptt","title":"Backpropagation Through Time","text":"<p>We use Backpropagation Through Time (BPTT) to learn the time series of the model2. In an RNN, the internal state $h_{t}$ at each time step depends on the internal state $h_{t-1}$ at the previous time step $t-1$. In BPTT, the parameters are updated at each time step by calculating the loss at each time step and then calculating the gradients backwards. Specifically, the model takes input images $i_t$ and joint angles $a_{t}$ and outputs the next state ($\\hat i_{t+1}$, $\\hat a_{t+1}$). The mean squared error (MSE) loss between the predictions and the true values ($f_{t+1}$, $a_{t+1}$) for all sequences is computed using <code>nn.MSELoss</code>, and error propagation is performed based on the <code>loss</code> value. Since the parameters at each time step are used for all subsequent time steps, backpropagation is performed with temporal expansion.</p> <p>Lines 47-54 show that SARNN calculates not only the image loss and the joint angle loss, but also the prediction loss of the attention point. Since the true value of the attention point is not available, the bidirectional loss 3 is used to learn the attention point. Specifically, the model updates the weights to minimize the loss between the attention point $\\hat p_{t+1}$ predicted by the RNN at each time step and the attention point $p_{t+1}$ extracted by the CNN at the same time step $t+1$. Based on this bidirectional loss, the LSTM learns the time-series relationship between attention points and joint angles. This approach not only eliminates redundant image predictions, but also encourages the CNN to predict attention points that are critical for motion prediction.</p> <p>In addition, <code>loss_weights</code> assign weights to each modality loss, thus determining the focus of learning for each modality. In deep predictive learning, joint angles are learned intensively because they directly affect the robot's motion commands. However, if the image information is not adequately learned, the integration of image and joint angle learning may not occur properly, making joint angle prediction corresponding to the image information difficult. Therefore, the weighting coefficients need to be adjusted based on the model and the task. In our experience, the weighting factor is often set to 1.0 for all modalities or 0.1 for images only.</p> [SOURCE] fullBPTT.py<pre><code>class fullBPTTtrainer:\ndef __init__(self, model, optimizer, loss_weights=[1.0, 1.0], device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.loss_weights = loss_weights\nself.scheduler = LossScheduler(decay_end=1000, curve_name=\"s\")\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, ((x_img, x_joint), (y_img, y_joint)) in enumerate(data):\nx_img = x_img.to(self.device)\ny_img = y_img.to(self.device)\nx_joint = x_joint.to(self.device)\ny_joint = y_joint.to(self.device)\nstate = None\nyi_list, yv_list = [], []\ndec_pts_list, enc_pts_list = [], []\nT = x_img.shape[1]\nfor t in range(T - 1):\n_yi_hat, _yv_hat, enc_ij, dec_ij, state = self.model(\nx_img[:, t], x_joint[:, t], state\n)\nyi_list.append(_yi_hat)\nyv_list.append(_yv_hat)\nenc_pts_list.append(enc_ij)\ndec_pts_list.append(dec_ij)\nyi_hat = torch.permute(torch.stack(yi_list), (1, 0, 2, 3, 4))\nyv_hat = torch.permute(torch.stack(yv_list), (1, 0, 2))\nimg_loss = nn.MSELoss()(yi_hat, y_img[:, 1:]) * self.loss_weights[0]\njoint_loss = nn.MSELoss()(yv_hat, y_joint[:, 1:]) * self.loss_weights[1]\n# Gradually change the loss value using the LossScheluder class.\npt_loss = nn.MSELoss()(\ntorch.stack(dec_pts_list[:-1]), torch.stack(enc_pts_list[1:])\n) * self.scheduler(self.loss_weights[2])\nloss = img_loss + joint_loss + pt_loss\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / (n_batch + 1)\n</code></pre> <ol> <li> <p>Hideyuki Ichiwara, Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Contact-rich manipulation of a flexible object based on deep predictive learning using vision and tactility. In 2022 International Conference on Robotics and Automation (ICRA), 5375\u20135381. IEEE, 2022.\u00a0\u21a9</p> </li> <li> <p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.\u00a0\u21a9</p> </li> <li> <p>Hyogo Hiruma, Hiroshi Ito, Hiroki Mori, and Tetsuya Ogata. Deep active visual attention for real-time robot motion generation: emergence of tool-body assimilation and adaptive tool-use. IEEE Robotics and Automation Letters, 7(3):8550\u20138557, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"model/SARNN/#model.SARNN","title":"<code>model.SARNN</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>SARNN: Spatial Attention with Recurrent Neural Network. This model \"explicitly\" extracts positions from the image that are important to the task, such as the target object or arm position, and learns the time-series relationship between these positions and the robot's joint angles. The robot is able to generate robust motions in response to changes in object position and lighting.</p> <p>Parameters:</p> Name Type Description Default <code>rec_dim</code> <code>int</code> <p>The dimension of the recurrent state in the LSTM cell.</p> required <code>k_dim</code> <code>int</code> <p>The dimension of the attention points.</p> <code>5</code> <code>joint_dim</code> <code>int</code> <p>The dimension of the joint angles.</p> <code>14</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for the softmax function.</p> <code>0.0001</code> <code>heatmap_size</code> <code>float</code> <p>The size of the heatmap in the InverseSpatialSoftmax layer.</p> <code>0.1</code> <code>kernel_size</code> <code>int</code> <p>The size of the convolutional kernel.</p> <code>3</code> <code>activation</code> <code>str</code> <p>The name of activation function.</p> <code>'lrelu'</code> <code>im_size</code> <code>list</code> <p>The size of the input image [height, width].</p> <code>[128, 128]</code> Source code in <code>en/docs/model/src/model.py</code> <pre><code>class SARNN(nn.Module):\n#:: SARNN\n\"\"\"SARNN: Spatial Attention with Recurrent Neural Network.\n    This model \"explicitly\" extracts positions from the image that are important to the task, such as the target object or arm position,\n    and learns the time-series relationship between these positions and the robot's joint angles.\n    The robot is able to generate robust motions in response to changes in object position and lighting.\n    Arguments:\n        rec_dim (int): The dimension of the recurrent state in the LSTM cell.\n        k_dim (int, optional): The dimension of the attention points.\n        joint_dim (int, optional): The dimension of the joint angles.\n        temperature (float, optional): The temperature parameter for the softmax function.\n        heatmap_size (float, optional): The size of the heatmap in the InverseSpatialSoftmax layer.\n        kernel_size (int, optional): The size of the convolutional kernel.\n        activation (str, optional): The name of activation function.\n        im_size (list, optional): The size of the input image [height, width].\n    \"\"\"\ndef __init__(\nself,\nrec_dim,\nk_dim=5,\njoint_dim=14,\ntemperature=1e-4,\nheatmap_size=0.1,\nkernel_size=3,\nactivation=\"lrelu\",\nim_size=[128, 128],\n):\nsuper(SARNN, self).__init__()\nself.k_dim = k_dim\nif isinstance(activation, str):\nactivation = get_activation_fn(activation, inplace=True)\nsub_im_size = [im_size[0] - 3 * (kernel_size - 1), im_size[1] - 3 * (kernel_size - 1)]\nself.temperature = temperature\nself.heatmap_size = heatmap_size\n# Positional Encoder\nself.pos_encoder = nn.Sequential(\nnn.Conv2d(3, 16, 3, 1, 0),  # Convolutional layer 1\nactivation,\nnn.Conv2d(16, 32, 3, 1, 0),  # Convolutional layer 2\nactivation,\nnn.Conv2d(32, self.k_dim, 3, 1, 0),  # Convolutional layer 3\nactivation,\nSpatialSoftmax(\nwidth=sub_im_size[0], height=sub_im_size[1], temperature=self.temperature, normalized=True\n),  # Spatial Softmax layer\n)\n# Image Encoder\nself.im_encoder = nn.Sequential(\nnn.Conv2d(3, 16, 3, 1, 0),  # Convolutional layer 1\nactivation,\nnn.Conv2d(16, 32, 3, 1, 0),  # Convolutional layer 2\nactivation,\nnn.Conv2d(32, self.k_dim, 3, 1, 0),  # Convolutional layer 3\nactivation,\n)\nrec_in = joint_dim + self.k_dim * 2\nself.rec = nn.LSTMCell(rec_in, rec_dim)  # LSTM cell\n# Joint Decoder\nself.decoder_joint = nn.Sequential(nn.Linear(rec_dim, joint_dim), activation)  # Linear layer and activation\n# Point Decoder\nself.decoder_point = nn.Sequential(\nnn.Linear(rec_dim, self.k_dim * 2), activation\n)  # Linear layer and activation\n# Inverse Spatial Softmax\nself.issm = InverseSpatialSoftmax(\nwidth=sub_im_size[0], height=sub_im_size[1], heatmap_size=self.heatmap_size, normalized=True\n)\n# Image Decoder\nself.decoder_image = nn.Sequential(\nnn.ConvTranspose2d(self.k_dim, 32, 3, 1, 0),  # Transposed Convolutional layer 1\nactivation,\nnn.ConvTranspose2d(32, 16, 3, 1, 0),  # Transposed Convolutional layer 2\nactivation,\nnn.ConvTranspose2d(16, 3, 3, 1, 0),  # Transposed Convolutional layer 3\nactivation,\n)\ndef forward(self, xi, xv, state=None):\n\"\"\"\n        Forward pass of the SARNN module.\n        Predicts the image, joint angle, and attention at the next time based on the image and joint angle at time t.\n        Predict the image, joint angles, and attention points for the next state (t+1) based on\n        the image and joint angles of the current state (t).\n        By inputting the predicted joint angles as control commands for the robot,\n        it is possible to generate sequential motion based on sensor information.\n        Arguments:\n            xi (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n            xv (torch.Tensor): Input vector tensor of shape (batch_size, input_dim).\n            state (tuple, optional): Initial hidden state and cell state of the LSTM cell.\n        Returns:\n            y_image (torch.Tensor): Decoded image tensor of shape (batch_size, channels, height, width).\n            y_joint (torch.Tensor): Decoded joint prediction tensor of shape (batch_size, joint_dim).\n            enc_pts (torch.Tensor): Encoded points tensor of shape (batch_size, k_dim * 2).\n            dec_pts (torch.Tensor): Decoded points tensor of shape (batch_size, k_dim * 2).\n            rnn_hid (tuple): Tuple containing the hidden state and cell state of the LSTM cell.\n        \"\"\"\n# Encode input image\nim_hid = self.im_encoder(xi)\nenc_pts, _ = self.pos_encoder(xi)\n# Reshape encoded points and concatenate with input vector\nenc_pts = enc_pts.reshape(-1, self.k_dim * 2)\nhid = torch.cat([enc_pts, xv], -1)\nrnn_hid = self.rec(hid, state)  # LSTM forward pass\ny_joint = self.decoder_joint(rnn_hid[0])  # Decode joint prediction\ndec_pts = self.decoder_point(rnn_hid[0])  # Decode points\n# Reshape decoded points\ndec_pts_in = dec_pts.reshape(-1, self.k_dim, 2)\nheatmap = self.issm(dec_pts_in)  # Inverse Spatial Softmax\nhid = torch.mul(heatmap, im_hid)  # Multiply heatmap with image feature `im_hid`\ny_image = self.decoder_image(hid)  # Decode image\nreturn y_image, y_joint, enc_pts, dec_pts, rnn_hid\n</code></pre>"},{"location":"model/SARNN/#model.SARNN.forward","title":"<code>forward(xi, xv, state=None)</code>","text":"<p>Forward pass of the SARNN module. Predicts the image, joint angle, and attention at the next time based on the image and joint angle at time t. Predict the image, joint angles, and attention points for the next state (t+1) based on the image and joint angles of the current state (t). By inputting the predicted joint angles as control commands for the robot, it is possible to generate sequential motion based on sensor information.</p> <p>Parameters:</p> Name Type Description Default <code>xi</code> <code>torch.Tensor</code> <p>Input image tensor of shape (batch_size, channels, height, width).</p> required <code>xv</code> <code>torch.Tensor</code> <p>Input vector tensor of shape (batch_size, input_dim).</p> required <code>state</code> <code>tuple</code> <p>Initial hidden state and cell state of the LSTM cell.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_image</code> <code>torch.Tensor</code> <p>Decoded image tensor of shape (batch_size, channels, height, width).</p> <code>y_joint</code> <code>torch.Tensor</code> <p>Decoded joint prediction tensor of shape (batch_size, joint_dim).</p> <code>enc_pts</code> <code>torch.Tensor</code> <p>Encoded points tensor of shape (batch_size, k_dim * 2).</p> <code>dec_pts</code> <code>torch.Tensor</code> <p>Decoded points tensor of shape (batch_size, k_dim * 2).</p> <code>rnn_hid</code> <code>tuple</code> <p>Tuple containing the hidden state and cell state of the LSTM cell.</p> Source code in <code>en/docs/model/src/model.py</code> <pre><code>def forward(self, xi, xv, state=None):\n\"\"\"\n    Forward pass of the SARNN module.\n    Predicts the image, joint angle, and attention at the next time based on the image and joint angle at time t.\n    Predict the image, joint angles, and attention points for the next state (t+1) based on\n    the image and joint angles of the current state (t).\n    By inputting the predicted joint angles as control commands for the robot,\n    it is possible to generate sequential motion based on sensor information.\n    Arguments:\n        xi (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n        xv (torch.Tensor): Input vector tensor of shape (batch_size, input_dim).\n        state (tuple, optional): Initial hidden state and cell state of the LSTM cell.\n    Returns:\n        y_image (torch.Tensor): Decoded image tensor of shape (batch_size, channels, height, width).\n        y_joint (torch.Tensor): Decoded joint prediction tensor of shape (batch_size, joint_dim).\n        enc_pts (torch.Tensor): Encoded points tensor of shape (batch_size, k_dim * 2).\n        dec_pts (torch.Tensor): Decoded points tensor of shape (batch_size, k_dim * 2).\n        rnn_hid (tuple): Tuple containing the hidden state and cell state of the LSTM cell.\n    \"\"\"\n# Encode input image\nim_hid = self.im_encoder(xi)\nenc_pts, _ = self.pos_encoder(xi)\n# Reshape encoded points and concatenate with input vector\nenc_pts = enc_pts.reshape(-1, self.k_dim * 2)\nhid = torch.cat([enc_pts, xv], -1)\nrnn_hid = self.rec(hid, state)  # LSTM forward pass\ny_joint = self.decoder_joint(rnn_hid[0])  # Decode joint prediction\ndec_pts = self.decoder_point(rnn_hid[0])  # Decode points\n# Reshape decoded points\ndec_pts_in = dec_pts.reshape(-1, self.k_dim, 2)\nheatmap = self.issm(dec_pts_in)  # Inverse Spatial Softmax\nhid = torch.mul(heatmap, im_hid)  # Multiply heatmap with image feature `im_hid`\ny_image = self.decoder_image(hid)  # Decode image\nreturn y_image, y_joint, enc_pts, dec_pts, rnn_hid\n</code></pre>"},{"location":"model/dataloader/","title":"Dataloader","text":"<p>EIPL provides a <code>MultimodalDataset</code> class for learning robot motions, which inherits from the Dataset class. This class returns a pair of input data, <code>x_data</code>, and the corresponding true value, <code>y_data</code>, for each epoch. The input data, <code>x_data</code>, consists of pairs of images and joint angles, and data augmentation is applied during each epoch. The input images are randomly adjusted for brightness, contrast, etc. to improve robustness to changes in lighting conditions, while Gaussian noise is added to the input joint angles to improve robustness to position errors. On the other hand, no noise is added to the original data. The model learns to handle noiseless situations (internal representation) from input data mixed with noise, allowing robust motion generation even in the presence of real-world noise during inference.</p> <p>The following source code shows how to use the <code>MultimodalDataset</code> class with an example of an object grasping task collected by AIREC. By providing 5-dimensional time series image data [number of data, time series length, channel, height, width] and 3-dimensional time series joint angle data [number of data, time series length, number of joints] to the <code>MultimodalDataset</code> class, data augmentation and other operations are performed automatically. Note that the <code>SampleDownloader</code>, which is used to download the sample dataset, is not mandatory. You can use functions like <code>numpy.load</code> or others to load your own datasets directly.</p> How to use dataloader<pre><code>from eipl.data import SampleDownloader, MultimodalDataset\n# Download and normalize sample data\ngrasp_data = SampleDownloader(\"airec\", \"grasp_bottle\", img_format=\"CHW\")\nimages, joints = grasp_data.load_norm_data(\"train\", vmin=0.1, vmax=0.9)\n# Give the image and joint angles to the Dataset class\nmulti_dataset = MultimodalDataset(images, joints)\n# Return input/true data as return value.\nx_data, y_data = multi_dataset[1]\n</code></pre> <p>The following figure shows the robot camera images returned by the <code>MultimodalDataset</code> class. From left to right, the images show the original image, the image with noise, and the robot joint angles. Random noise is added to the image at each epoch, allowing the model to learn from a variety of visual situations. The black dotted lines represent the original joint angles, while the colored lines represent the joint angles with Gaussian noise.</p> <p></p> <p>Note</p> <p>If you are unable to obtain the dataset due to a proxy or any other reason, you can manually download the dataset from here and save it in the ~/.eipl/ folder.</p> <pre><code>```bash            \n$ cd ~/\n$ mkdir -p .eipl/airec/\n$ cd .eipl/airec/\n$ # copy grasp_bottle.tar to ~/.eipl/airec/ directory\n$ tar xvf grasp_bottle.tar\n$ ls grasp_bottle/*\ngrasp_bottle/joint_bounds.npy\n```\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset","title":"<code>dataloader.MultimodalDataset</code>","text":"<p>         Bases: <code>Dataset</code></p> <p>This class is used to train models that deal with multimodal data (e.g., images, joints), such as CNNRNN/SARNN.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>numpy array</code> <p>Set of images in the dataset, expected to be a 5D array [data_num, seq_num, channel, height, width].</p> required <code>joints</code> <code>numpy array</code> <p>Set of joints in the dataset, expected to be a 3D array [data_num, seq_num, joint_dim].</p> required <code>stdev</code> <code>float</code> <p>Set the standard deviation for normal distribution to generate noise.</p> <code>0.02</code> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>class MultimodalDataset(Dataset):\n#:: MultimodalDataset\n\"\"\"\n    This class is used to train models that deal with multimodal data (e.g., images, joints), such as CNNRNN/SARNN.\n    Args:\n        images (numpy array): Set of images in the dataset, expected to be a 5D array [data_num, seq_num, channel, height, width].\n        joints (numpy array): Set of joints in the dataset, expected to be a 3D array [data_num, seq_num, joint_dim].\n        stdev (float, optional): Set the standard deviation for normal distribution to generate noise.\n    \"\"\"\ndef __init__(self, images, joints, stdev=0.02):\n\"\"\"\n        The constructor of Multimodal Dataset class. Initializes the images, joints, and transformation.\n        Args:\n            images (numpy array): The images data, expected to be a 5D array [data_num, seq_num, channel, height, width].\n            joints (numpy array): The joints data, expected to be a 3D array [data_num, seq_num, joint_dim].\n            stdev (float, optional): The standard deviation for the normal distribution to generate noise. Defaults to 0.02.\n        \"\"\"\nself.stdev = stdev\nself.images = images\nself.joints = joints\nself.transform = transforms.ColorJitter(contrast=0.5, brightness=0.5, saturation=0.1)\ndef __len__(self):\n\"\"\"\n        Returns the number of the data.\n        \"\"\"\nreturn len(self.images)\ndef __getitem__(self, idx):\n\"\"\"\n        Extraction and preprocessing of images and joints at the specified indexes.\n        Args:\n            idx (int): The index of the element.\n        Returns:\n            dataset (list): A list containing lists of transformed and noise added image\n                            and joint (x_img, x_joint) and the original image and joint (y_img, y_joint).\n        \"\"\"\ny_img = self.images[idx]\ny_joint = self.joints[idx]\nx_img = self.transform(self.images[idx])\nx_img = x_img + torch.normal(mean=0, std=self.stdev, size=x_img.shape)\nx_joint = self.joints[idx] + torch.normal(mean=0, std=self.stdev, size=y_joint.shape)\nreturn [[x_img, x_joint], [y_img, y_joint]]\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Extraction and preprocessing of images and joints at the specified indexes.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the element.</p> required <p>Returns:</p> Name Type Description <code>dataset</code> <code>list</code> <p>A list containing lists of transformed and noise added image             and joint (x_img, x_joint) and the original image and joint (y_img, y_joint).</p> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>def __getitem__(self, idx):\n\"\"\"\n    Extraction and preprocessing of images and joints at the specified indexes.\n    Args:\n        idx (int): The index of the element.\n    Returns:\n        dataset (list): A list containing lists of transformed and noise added image\n                        and joint (x_img, x_joint) and the original image and joint (y_img, y_joint).\n    \"\"\"\ny_img = self.images[idx]\ny_joint = self.joints[idx]\nx_img = self.transform(self.images[idx])\nx_img = x_img + torch.normal(mean=0, std=self.stdev, size=x_img.shape)\nx_joint = self.joints[idx] + torch.normal(mean=0, std=self.stdev, size=y_joint.shape)\nreturn [[x_img, x_joint], [y_img, y_joint]]\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset.__init__","title":"<code>__init__(images, joints, stdev=0.02)</code>","text":"<p>The constructor of Multimodal Dataset class. Initializes the images, joints, and transformation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>numpy array</code> <p>The images data, expected to be a 5D array [data_num, seq_num, channel, height, width].</p> required <code>joints</code> <code>numpy array</code> <p>The joints data, expected to be a 3D array [data_num, seq_num, joint_dim].</p> required <code>stdev</code> <code>float</code> <p>The standard deviation for the normal distribution to generate noise. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>def __init__(self, images, joints, stdev=0.02):\n\"\"\"\n    The constructor of Multimodal Dataset class. Initializes the images, joints, and transformation.\n    Args:\n        images (numpy array): The images data, expected to be a 5D array [data_num, seq_num, channel, height, width].\n        joints (numpy array): The joints data, expected to be a 3D array [data_num, seq_num, joint_dim].\n        stdev (float, optional): The standard deviation for the normal distribution to generate noise. Defaults to 0.02.\n    \"\"\"\nself.stdev = stdev\nself.images = images\nself.joints = joints\nself.transform = transforms.ColorJitter(contrast=0.5, brightness=0.5, saturation=0.1)\n</code></pre>"},{"location":"model/dataloader/#dataloader.MultimodalDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of the data.</p> Source code in <code>en/docs/model/src/dataloader.py</code> <pre><code>def __len__(self):\n\"\"\"\n    Returns the number of the data.\n    \"\"\"\nreturn len(self.images)\n</code></pre>"},{"location":"model/test/","title":"Inference","text":""},{"location":"model/test/#offline-inference","title":"Offline inference","text":"<p>To verify that SARNN has been trained correctly, use the test program <code>test.py</code>. The argument <code>filename</code> should be the path of the trained weights file, and <code>idx</code> is the index of the data to be visualized. The input_param is a mixing coefficient that produces stable behavior against real-world noise. It mixes the sensor information at a given time with the model predictions at the previous time $t-1$ in a certain ratio and feeds it as input to the model. This process can be seen as a low-pass filter, where the predicted values from the previous time can complement the prediction of stable motion commands, even if the robot's sensor values are noisy. It is important to note that if the mixing coefficient is too small, it becomes difficult to adjust the motion based on real sensor information, and the robustness to position changes decreases.</p> <p><pre><code>$ cd eipl/tutorials/sarnn/\n$ python3 ./bin/test.py --filename ./log/20230521_1247_41/SARNN.pth --idx 4 --input_param 1.0\n\nimages shape:(187, 128, 128, 3), min=0, max=255\njoints shape:(187, 8), min=-0.8595600128173828, max=1.8292399644851685\nloop_ct:0, joint:[ 0.00226304 -0.7357931  -0.28175825  1.2895856   0.7252841   0.14539993\n-0.0266939   0.00422328]\nloop_ct:1, joint:[ 0.00307412 -0.73363686 -0.2815826   1.2874944   0.72176594  0.1542334\n-0.02719587  0.00325996]\n.\n.\n.\n\n$ ls output/\nSARNN_20230521_1247_41_4_1.0.gif\n</code></pre> The following figure shows the inference results at an untaught position (point D). From left to right are the input image, the predicted image, and the predicted joint angles (dotted lines represent the true values). The blue points in the input image represent the POIs (Point of Interest) extracted from the image, while the red points represent the POIs predicted by the RNN. This indicates that the joint angle is predicted while focusing on the robot hand and the grasped object.</p> <p></p>"},{"location":"model/test/#pca","title":"Principal Component Analysis","text":"<p>In deep predictive learning, it is recommended to visualize the internal representation using Principal Component Analysis (PCA)1 to preliminarily examine the generalization performance of the trained model. By embedding motion into the internal state of the RNN and ensuring that the internal state is self-organized and structured for each learning motion, we can achieve generalization motion with a small amount of data. To verify how the sensorimotor information (images and joint angles) is represented, we use PCA to compress the internal state of the RNN into a lower dimension and visualize the elements that represent the characteristics of the data, specifically the first through third principal components.</p> <p>The following code snippet demonstrates the inference and PCA process. First, the test data is fed into the model and the internal state <code>state</code> of the RNN at each time step is stored as a list. In the case of LSTM, the <code>hidden state</code> and the <code>cell state</code> are returned as <code>state</code>. For visualization and analysis purposes we use the <code>hidden state</code>. Next, we reshape the state from [number of data, time series length, number of state dimensions] to [number of data x time series length, number of state dimensions] to compare the internal state at each object position. Finally, we apply PCA to compress the high-dimensional state into low-dimensional information (3 dimensions), as shown in line 12. By restoring the compressed principal component pca_val to its original form [number of data, time series length, 3 dim], we can visualize the relationship between object position and internal state by assigning a unique color to each object position and plotting the points in 3D space.</p> [SOURCE] test_pca_rnn.py<pre><code>states = tensor2numpy( states )\n# Reshape the state from [N,T,D] to [-1,D] for PCA of RNN.\n# N is the number of datasets\n# T is the sequence length\n# D is the dimension of the hidden state\nN,T,D  = states.shape\nstates = states.reshape(-1,D)\n# plot pca\nloop_ct = float(360)/T\npca_dim = 3\npca     = PCA(n_components=pca_dim).fit(states)\npca_val = pca.transform(states)\n# Reshape the states from [-1, pca_dim] to [N,T,pca_dim] to\n# visualize each state as a 3D scatter.\npca_val = pca_val.reshape( N, T, pca_dim )\nfig = plt.figure(dpi=60)\nax = fig.add_subplot(projection='3d')\ndef anim_update(i):\nax.cla()\nangle = int(loop_ct * i)\nax.view_init(30, angle)\nc_list = ['C0','C1','C2','C3','C4']\n</code></pre> <p>Use the <code>test_pca_sarnn.py</code> program to visualize the internal state using PCA. The filename argument should be the path to the weight file.</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ python3 ./bin/test_pca_sarnn.py --filename log/20230521_1247_41/SARNN.pth\n$ ls output/\nPCA_SARNN_20230521_1247_41.gif\n</code></pre> <p>The figure below shows the inference results of SARNN. Each dotted line represents the time evolution of the internal state. The color of each attractor corresponds to the object position: blue, orange, and green correspond to the teaching positions A, C, and E, while red and purple correspond to the untrained positions B and D. The self-organization of the attractors based on the object position indicates that the behavior is learned and memorized according to the object position. In particular, the attractors at the untrained positions are generated between the teaching positions, allowing the generation of interpolated movements by teaching grasping movements with different object positions multiple times.</p> <p></p>"},{"location":"model/test/#online","title":"Motion Generation","text":"<p>The following pseudocode describes an online motion generation method using a real robot. The robot can generate sequential motions based on sensor information by repeating steps 2-5 at a specified sampling rate.</p> <ol> <li> <p>Load model (line 23)</p> <p>After defining the model, load the trained weights.</p> </li> <li> <p>Retrieve and normalize sensor information (line 38)</p> <p>Retrieve the robot's sensor information and perform the normalization process. For example, if you are using ROS, subscribe to the image and joint angles and assign them to the <code>raw_image</code> and <code>raw_joint</code> variables.</p> </li> <li> <p>Inference (line 51)</p> <p>Predict the image <code>y_image</code> and joint angle <code>y_joint</code> for the next time step using the normalized image <code>x_img</code> and joint angle <code>x_joint</code>.</p> </li> <li> <p>Send command (line 61)</p> <p>By using the predicted joint angle <code>pred_joint</code> as the robot's motor command, the robot can generate sequential motions. When using ROS, publish the joint angles to the motors to control each motor based on the motor command.</p> </li> <li> <p>Sleep (line 65)</p> <p>Finally, insert a sleep process to adjust the timing and perform inference at the specified sampling rate. The sampling rate should be the same as that used during training data collection.</p> </li> </ol> online.py<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(\"--model_pth\", type=str, default=None)\nparser.add_argument(\"--input_param\", type=float, default=0.8)\nargs = parser.parse_args()\n# restore parameters\ndir_name = os.path.split(args.model_pth)[0]\nparams = restore_args(os.path.join(dir_name, \"args.json\"))\n# load dataset\nminmax = [params[\"vmin\"], params[\"vmax\"]]\njoint_bounds = np.load(os.path.join(os.path.expanduser(\"~\"), \".eipl/grasp_bottle/joint_bounds.npy\"))\n# define model\nmodel = SARNN(\nrec_dim=params[\"rec_dim\"],\njoint_dim=8,\nk_dim=params[\"k_dim\"],\nheatmap_size=params[\"heatmap_size\"],\ntemperature=params[\"temperature\"],\n)\n# load weight\nckpt = torch.load(args.model_pth, map_location=torch.device(\"cpu\"))\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\n# Inference\n# Set the inference frequency; for a 10-Hz in ROS system, set as follows.\nfreq = 10  # 10Hz\nrate = rospy.Rate(freq)\nimage_list, joint_list = [], []\nstate = None\nnloop = 200  # freq * 20 sec\nfor loop_ct in range(nloop):\nstart_time = time.time()\n# load data and normalization\nraw_images, raw_joint = robot.get_sensor_data()\nx_img = raw_images[loop_ct].transpose(2, 0, 1)\nx_img = torch.Tensor(np.expand_dims(x_img, 0))\nx_img = normalization(x_img, (0, 255), minmax)\nx_joint = torch.Tensor(np.expand_dims(raw_joint, 0))\nx_joint = normalization(x_joint, joint_bounds, minmax)\n# closed loop\nif loop_ct &gt; 0:\nx_img = args.input_param * x_img + (1.0 - args.input_param) * y_image\nx_joint = args.input_param * x_joint + (1.0 - args.input_param) * y_joint\n# predict rnn\ny_image, y_joint, state = rnn_model(x_img, x_joint, state)\n# denormalization\npred_image = tensor2numpy(y_image[0])\npred_image = deprocess_img(pred_image, cae_params[\"vmin\"], cae_params[\"vmax\"])\npred_image = pred_image.transpose(1, 2, 0)\npred_joint = tensor2numpy(y_joint[0])\npred_joint = normalization(pred_joint, minmax, joint_bounds)\n# send pred_joint to robot\n# send_command(pred_joint)\npub.publish(pred_joint)\n# Sleep to infer at set frequency.\n# ROS system\nrate.sleep()\n</code></pre> <ol> <li> <p>Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933.\u00a0\u21a9</p> </li> </ol>"},{"location":"model/train/","title":"Train","text":""},{"location":"model/train/#files","title":"Files","text":"<p>Use the programs in the tutorial/SARNN folder of the EIPL repository to train SARNN. Each folder and each program has a specific role:</p> <ul> <li>bin/train.py: Program to load data, train and save models.</li> <li>bin/test.py: Program for offline inference of models using test data (images and joint angles) and visualization of inference results.</li> <li>bin/test_pca_sarnn.py: Program to visualize the internal state of the RNN using Principal Component Analysis.</li> <li>libs/fullBPTT.py: Backpropagation class for time series learning.</li> <li>log: Folder for storing weights, learning curves, and parameter information.</li> <li>output: Folder for storing inference results.</li> </ul>"},{"location":"model/train/#train","title":"Trainig","text":"<p>The main program <code>train.py</code> is used to train SARNN. When the program is executed, the weights (pth) and Tensorboard log files are saved in the <code>log</code> folder. The program allows users to specify necessary training parameters such as model type, number of epochs, batch size, learning rate, and optimization method using command line arguments. It also uses the EarlyStopping library to determine when to stop training early and save weights when the test error is minimized. For a detailed explanation of how the program works, please refer to the comments in the [code] (https://github.com/ogata-lab/eipl/blob/master/eipl/tutorials/sarnn/bin/train.py).</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ python3 ./bin/train.py\n[INFO] Set tag = \"20230521_1247_41\"\n================================\nbatch_size : 5\ndevice : 0\nepoch : 100000\nheatmap_size : 0.1\nimg_loss : 0.1\njoint_loss : 1.0\nk_dim : 5\nlog_dir : log/\nlr : 0.001\nmodel : sarnn\noptimizer : adam\npt_loss : 0.1\nrec_dim : 50\nstdev : 0.02\ntag : \"20230521_1247_41\"\ntemperature : 0.0001\nvmax : 1.0\nvmin : 0.0\n================================\n12%|\u2588\u2588\u2588\u2588          | 11504/100000 [14:46:53&lt;114:10:44,  4.64s/it, train_loss=0.000251, test_loss=0.000316]\n</code></pre>"},{"location":"model/train/#tensorboard","title":"Learning Curves","text":"<p>You can check the training progress of the model using TensorBoard. By specifying the log folder where the weights are stored with the <code>logdir</code> argument, you can visualize the learning curve in your browser, as shown in the figure below. If there is a tendency for overfitting in the early stages of training, it may be due to anomalies in the training data or model, or in the initial weights (seeds). Countermeasures include checking the normalization range of the training data, reviewing the model structure, and retraining with different seed values. For specific instructions on how to use TensorBoard, please refer to the documentation linked [here] (https://www.tensorflow.org/tensorboard).</p> <pre><code>$ cd eipl/tutorials/sarnn/\n$ tensorboard --logdir=./log/\nTensorFlow installation not found - running with reduced feature set.\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.12.1 at http://localhost:6006/ (Press CTRL+C to quit)\n</code></pre> <p></p>"},{"location":"robot/overview/","title":"Overview","text":"<p>Here we will describe a sequence of procedures from motion teaching to motion generation using Open Manpulator. The following five points will be covered:</p> <p>Note</p> <p>Coming soon.</p> <ol> <li> <p>Motion Teaching</p> <p>In this step, the motion is taught to the Open Manipulator robot. Different methods can be used to teach the motion, such as using a leader-follower system or a joystick.</p> </li> <li> <p>Data Collection</p> <p>After motion teaching, data collection is performed to capture the robot's sensor information, including joint angles, camera images, and other relevant data. This data is used to train the motion generation model.</p> </li> <li> <p>Data Set Preparation</p> <p>Once the data is collected, it must be prepared in a suitable format for training the motion generation model. This includes organizing the data, performing any necessary preprocessing steps such as normalization or resizing, and dividing the data into training and test sets.</p> </li> <li> <p>Model Training</p> <p>With the prepared data set, the motion generation model is trained using machine learning techniques. This typically involves feeding the input data, such as joint angles and camera images, into the model and adjusting the model parameters to minimize the prediction error. </p> </li> <li> <p>Motion Generation</p> <p>After the model is trained, it can be used to generate motion. By providing the desired input, such as target joint angles or desired end-effector positions, the model can generate appropriate motion commands for the robot. These motion commands can be sent to the robot's actuators, allowing it to perform the desired motions.</p> </li> </ol>"},{"location":"simulator/dataset/","title":"Motion Teaching","text":"<p>Here we describe a sequence of procedures from motion teaching to motion generation using robot simulator Robosuite. </p>"},{"location":"simulator/dataset/#setup","title":"Setup","text":"<p>Clone the robosuite repository <pre><code>$ git clone https://github.com/ARISE-Initiative/robosuite.git\n$ cd robosuite\n</code></pre></p> <p>Install the requirements with <pre><code>$ pip3 install -r requirements.txt\n$ pip3 install -r requirements-extra.txt\n</code></pre></p>"},{"location":"simulator/dataset/#teleoperation","title":"Teleoperation","text":"<p>Teach object grasping motions to the robot using keyboard teleoperation. The argument <code>pos</code> is the position of the object, and <code>ep_dir</code> is the save directory. We provide a <code>demonstration dataset</code> to eliminate the hassle of data collection. For more information, click here.</p> <pre><code>$ cd eipl/tutorials/robosuite/simulator\n$ ls\n2_resave.sh  3_check_data.sh  bin  data  libs  output  README\n$ python3 ./bin/1_teaching.py --pos target_position --ep_dir save_dir\n\n# e.g.\n$ python3 ./bin/1_teaching.py --pos -0.2 --ep_dir ./data/raw_data/Pos1_1\n\n[robosuite WARNING] No private macro file found! (__init__.py:7)\n[robosuite WARNING] It is recommended to use a private macro file (__init__.py:8)\n[robosuite WARNING] To setup, run: python /home/ito/Downloads/robosuite/robosuite/scripts/setup_macros.py (__init__.py:9)\nKeys        Command\nspacebar    toggle gripper (open/close)\nw-a-s-d     move arm horizontally in x-y plane\nr-f         move arm vertically\nz-x         rotate arm about x-axis\nt-g         rotate arm about y-axis\nc-v         rotate arm about z-axis\n[           start recording\n]           stop recording (not recommended)\nCollecting demonstration data ...\n</code></pre> <p>The collected data (model.xml and state.npz) are stored in the <code>ep_dir</code> folder (e.g. ./data/raw_data/). <pre><code>$ ls ./data/raw_data/\nPos1_1  Pos1_2  Pos1_3  Pos1_4  Pos2_1  Pos2_2  Pos2_3  Pos2_4 ...\n$ cd ./data/raw_data/Pos1_1\nmodel.xml  state.npz\n</code></pre></p>"},{"location":"simulator/dataset/#data-re-saving","title":"Data Re-Saving","text":"<p>The following two processes are performed. The first is the removal of guide lines. During keyboard teleoperation, green guide lines appear in the image data. By playback of the collected data, the image data without the guide line is saved. The second is downsampling. During tele-operation, the collected data sequence is long (default is 600 step) because the robot is controlled with a high frequency to teach fine movements. Therefore, sensor data is recollected every 5 step to downsample the sequence length to 120 step.</p> <pre><code># Re-save the files one by one.\n$ python3 ./bin/2_resave.py --ep_dir ./data/raw_data/Pos1_1/\n\n# Re-save all files in the specified folder (default is './data/raw_data')  at once.\n$ bash 2_resave.sh\n</code></pre> <p>Check whether the motion generated by playback is correct. Tasks rarely fail due to downsampling. If the playback data fails the task, the following error is displayed <code>[ERROR] This data set has failed task during playback.</code>. If the task is successful, the gif animation is saved in the output folder.</p> <pre><code># Re-save the files one by one.\n$ python3 ./bin/3_check_playback_data.py ./data/raw_data/Pos1_1/state_resave.npz\n$ ls ./output/\nPos1_1_image_joint_ani.gif\n\n# Re-save all files in the specified folder (default is './data/raw_data')  at once.\n$ bash 3_check_data.sh\n</code></pre>"},{"location":"simulator/dataset/#generate-dataset","title":"Generate Dataset","text":"<p>If the playbacked file was stored in <code>./data/raw_data/</code>, training/test data will be automatically generated by the following command.</p> <pre><code>$ python3 ./bin/4_generate_dataset.py\n$ ls ./data/\njoint_bounds.npy  pose_bounds.npy  raw_data  test  train\n$ ls ./data/train/\nimages.npy  joints.npy  poses.npy\n$ ls ./data/test/\nimages.npy  joints.npy  poses.npy\n</code></pre> <p>Check for proper normalization range of joint angles. The visual image of the robot, raw joint angles, and normalized joint angle data are saved as a gif animation in the output folder.</p> <pre><code>$ python3 ./bin/5_check_dataset.py\nload test data, index number is 0\nJoint: shape=(18, 90, 8), min=-2.64, max=3.18\nNorm joint: shape=(18, 90, 8), min=0.1, max=0.9\n\n$ ls ./output/\ncheck_dataset_0.gif\n</code></pre>"},{"location":"simulator/dataset/#demonstration-dataset","title":"Demonstration Dataset","text":"<p>Download the demonstration dataset from this link and extract it into <code>./data/raw_data/</code> to eliminate the need for data collection. Note that after downloading the file, you must perform Step 2 or later.</p>"},{"location":"simulator/rt_control/","title":"Realtime Motion Generation","text":"<p>Move to the simulator folder and run the realtime motion generation program <code>6_rt_control.py</code> with the weight file as an argument. This program repeats 10 times the object grasping operation placed at random positions.</p> <pre><code>$ cd ../simulator/\n$ python3 ./bin/6_rt_control.py ../sarnn/log/YEAR_DAY_TIME/SARNN.pth\n[robosuite WARNING] No private macro file found! (macros.py:53)\n[robosuite WARNING] It is recommended to use a private macro file (macros.py:54)\n[robosuite WARNING] To setup, run: python /home/ito/var/robosuite/robosuite/scripts/setup_macros.py (macros.py:55)\n[1/10] Task succeeded!\n[2/10] Task succeeded!\n</code></pre>"},{"location":"simulator/train/","title":"Model Training","text":""},{"location":"simulator/train/#train","title":"Train","text":"<p>Move to the <code>sarnn folder</code> and start training the model. The trained weights are saved in the log folder. If you want to perform validation without training the model, download the trained weights from here and save it in the log folder.</p> <pre><code>$ cd ../sarnn/\n$ python3 ./bin/train.py\n[INFO] Set tag = YEAR_DAY_TIME\n================================\nbatch_size : 5\ncompile : False\ndevice : 0\nepoch : 10000\nheatmap_size : 0.1\nimg_loss : 0.1\njoint_loss : 1.0\nk_dim : 5\nlog_dir : log/\nlr : 0.0001\nmodel : sarnn\nn_worker : 8\noptimizer : adam\npt_loss : 0.1\nrec_dim : 50\nstdev : 0.1\ntag : YEAR_DAY_TIME\ntemperature : 0.0001\nvmax : 1.0\nvmin : 0.0\n================================\n96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9551/10000 [1:56:01&lt;05:31,  1.35it/s, train_loss=0.00066, test_loss=0.00111]\n</code></pre>"},{"location":"simulator/train/#test","title":"Test","text":"<p>Specifying a weight file as the argument of <code>test.py</code> will save a gif animation of the predicted image, attention points, and predicted joint angles in the output folder.</p> <pre><code>$ python3 ./bin/test.py --filename ./log/YEAR_DAY_TIME/SARNN.pth\n$ ls ./output/\n$ SARNN_YEAR_DAY_TIME_0.gif\n</code></pre>"},{"location":"simulator/train/#visualization-of-internal-representation-using-pca","title":"Visualization of internal representation using PCA","text":"<p>Specifying a weight file as the argument of <code>test_pca_sarnn.py</code> will save the internal representation of the RNN as a gif animation.</p> <pre><code>$ python3 ./bin/test_pca_sarnn.py ./log/YEAR_DAY_TIME/SARNN.pth\n$ ls ./output/\n$ PCA_SARNN_YEAR_DAY_TIME.gif\n</code></pre>"},{"location":"teach/dataset/","title":"Generate dataset","text":""},{"location":"teach/dataset/#download","title":"Download","text":"<p>In this section, we generate a dataset for deep predictive learning using sensor information obtained during object grasping training with AIREC. We provide instructions on how to extract specific files from multiple rosbag data and save them in npz format using the provided sample data and scripts. Please follow the instructions below to download and extract the files.</p> <pre><code>$ mkdir ~/tmp\n$ cd tmp\n$ wget https://dl.dropboxusercontent.com/s/90wkfttf9w0bz0t/rosbag.tar\n$ tar xvf rosbag.tar\n$ cd rosbag\n$ ls\n1_rosbag2npz.py  2_make_dataset.py  3_check_data.py  bag  data  output  utils.py\n</code></pre>"},{"location":"teach/dataset/#files","title":"Files","text":"<p>The downloaded file contains the following files. Users can generate datasets from rosbag data by running the programs in numerical order.</p> <ul> <li>1_rosbag2npy.py: Extracts the specified information (topic data) from rosbag files and converts it to the npz format.</li> <li>2_make_dataset.py: This program does three things. First, it adjusts the data length to ensure a consistent time series length for all data, regardless of the <code>--duration</code> argument set during <code>rosbag recording</code>. Second, it sorts and stores the training and test data based on a specified index. Third, it calculates the normalization parameters (upper and lower bounds) for the joint angles. For detailed information on this process, please refer to the link provided.</li> <li>3_check_data.py: This program visualizes the collected data by saving images and joint angles of the robot as gif animation. Before running the training program, it is important to check the cropping range of the images and the normalized range of the joint angles.</li> <li>utils.py: This file contains preprocessing programs (e.g. normalization) required for the dataset.</li> <li>bag: This directory stores the collected <code>rosbag</code> data.</li> <li>data: After running <code>2_make_dataset.py</code>, this directory stores the training and test data, along with the normalization parameters for the joint angles.</li> <li>output: This directory stores the visualization results, with the training data index in the filename.</li> </ul>"},{"location":"teach/dataset/#data-extraction","title":"Data Extraction","text":"<p>The following command can be used to extract specific information (topic data) from rosbag files. See the details of the arguments below:</p> <ul> <li>bag_dir: Specify the directory where the rosbag data is stored.</li> <li>freq: Since the sampling rate (Hz) of different sensors varies, the data will be extracted and saved at the specified sampling rate.</li> </ul> <pre><code>$ python3 1_rosbag2npz.py ./bag/ --freq 10\nFailed to load Python extension for LZ4 support. LZ4 compression will not be available.\n./bag/rollout_001.bag\n1664630681.9616075\n1664630682.0616074\n1664630682.1616075\n1664630682.2616074\n</code></pre> <p>Since storing all topics in the npz file consumes a significant amount of memory, this script provides an example of storing specific robot sensor information, such as camera images, joint angles, and gripper status. Lines 31-35 list the names of the topics to save, and lines 50-87 extract data from the messages of each topic and save it to a predefined list. Note that saving the camera image as it is may require a large amount of storage space, so it is recommended to resize or crop the image beforehand. Even if sampling is performed at regular intervals, the data length of the topics may differ depending on the start and end time of the rosbag recording. Therefore, after line 95, the time series length is adjusted accordingly. The program can be adapted to the user's robot by changing the topic names and the data extraction method.</p> [SOURCE] 1_rosbag2npz.py<pre><code>import os\nimport cv2\nimport glob\nimport rospy\nimport rosbag\nimport argparse\nimport numpy as np\nparser = argparse.ArgumentParser()\nparser.add_argument(\"bag_dir\", type=str)\nparser.add_argument(\"--freq\", type=float, default=10)\nargs = parser.parse_args()\nfiles = glob.glob(os.path.join(args.bag_dir, \"*.bag\"))\nfiles.sort()\nfor file in files:\nprint(file)\nsavename = file.split(\".bag\")[0] + \".npz\"\n# Open the rosbag file\nbag = rosbag.Bag(file)\n# Get the start and end times of the rosbag file\nstart_time = bag.get_start_time()\nend_time = bag.get_end_time()\n# Get the topics in the rosbag file\n# topics = bag.get_type_and_topic_info()[1].keys()\ntopics = [\n\"/torobo/joint_states\",\n\"/torobo/head/see3cam_left/camera/color/image_repub/compressed\",\n\"/torobo/left_hand_controller/state\",\n]\n# Create a rospy.Time object to represent the current time\ncurrent_time = rospy.Time.from_sec(start_time)\njoint_list = []\nfinger_list = []\nimage_list = []\nfinger_state_list = []\nprev_finger = None\nfinger_state = 0\n# Loop through the rosbag file at regular intervals (args.freq)\nfreq = 1.0 / float(args.freq)\nwhile current_time.to_sec() &lt; end_time:\nprint(current_time.to_sec())\n# Get the messages for each topic at the current time\nfor topic in topics:\nfor topic_msg, msg, time in bag.read_messages(topic):\nif time &gt;= current_time:\nif topic == \"/torobo/joint_states\":\njoint_list.append(msg.position[7:14])\nif topic == \"/torobo/head/see3cam_left/camera/color/image_repub/compressed\":\nnp_arr = np.frombuffer(msg.data, np.uint8)\nnp_img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\nnp_img = np_img[::2, ::2]\nimage_list.append(np_img[150:470, 110:430].astype(np.uint8))\nif topic == \"/torobo/left_hand_controller/state\":\nfinger = np.array(msg.desired.positions[3])\nif prev_finger is None:\nprev_finger = finger\nif finger - prev_finger &gt; 0.005 and finger_state == 0:\nfinger_state = 1\nelif prev_finger - finger &gt; 0.005 and finger_state == 1:\nfinger_state = 0\nprev_finger = finger\nfinger_list.append(finger)\nfinger_state_list.append(finger_state)\nbreak\n# Wait for the next interval\ncurrent_time += rospy.Duration.from_sec(freq)\nrospy.sleep(freq)\n# Close the rosbag file\nbag.close()\n# Convert list to array\njoints = np.array(joint_list, dtype=np.float32)\nfinger = np.array(finger_list, dtype=np.float32)\nfinger_state = np.array(finger_state_list, dtype=np.float32)\nimages = np.array(image_list, dtype=np.uint8)\n# Get shorter lenght\nshorter_length = min(len(joints), len(images), len(finger), len(finger_state))\n# Trim\njoints = joints[:shorter_length]\nfinger = finger[:shorter_length]\nimages = images[:shorter_length]\nfinger_state = finger_state[:shorter_length]\n# Save\nnp.savez(savename, joints=joints, finger=finger, finger_state=finger_state, images=images)\n</code></pre>"},{"location":"teach/dataset/#dataset-preparation","title":"Dataset Preparation","text":"<p>The following command generates training and test data from the npz file converted in the previous section.</p> <pre><code>$ python3 2_make_dataset.py\n./bag/rollout_001.npz\n./bag/rollout_002.npz\n./bag/rollout_003.npz\n./bag/rollout_004.npz\n./bag/rollout_005.npz\n</code></pre> <p>This program consists of three steps, and all generated data is stored in the <code>data</code> folder. First, all data is loaded using the <code>load_data</code> function. The operations performed in lines 21, 22, 28, and 29 are as follows</p> <ul> <li>resize_img: Resizes the image to the given size. This function supports time series images based on the <code>cv2.resize</code> function.</li> <li>cos_interpolation: To facilitate learning and prediction of rapidly changing 0/1 binary data, such as robot hand open/close commands, cosine interpolation is used to transform the data into smooth open/close commands. See the provided link for more information.</li> <li>list_to_numpy: Even if you specify a storage time <code>--duration</code> for the <code>rosbag record</code>, the sequence length of all rosbag data may not be the same due to the timing of the ROS system execution. Therefore, the data length is standardized and formatted by performing padding processing according to the longest sequence.</li> </ul> <p>Lines 43-46 sort the training and test data based on user-specified indices (lines 36 and 37). The relationship between the training positions and the indexes is shown in the table below. The positions A-E in the table represent the object position. Four training data points were collected for each teaching position, and one test data point was collected for all positions. A total of 15 data points were collected. When evaluating the model using only the test data collected at the teaching positions, it can be difficult to observe generalization behavior at untaught positions due to overlearning at the teaching positions. Therefore, it is important to include even a small amount of untrained items in the test data to obtain generalization performance across items.</p> <p>Finally, in lines 49-50, the upper and lower limits of each joint angle are calculated and stored as normalization parameters for the joint angles. For more information on why the upper and lower limits of the joint angles are calculated, see the link provided.</p> Position A B C D E train 0,1,2,3 None 5,6,7,8 None 10,11,12,13 test 4 15 9 16 14 [SOURCE] 2_make_dataset.py<pre><code>import os\nimport cv2\nimport glob\nimport argparse\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom eipl.utils import resize_img, calc_minmax, list_to_numpy, cos_interpolation\ndef load_data(dir):\njoints = []\nimages = []\nseq_length = []\nfiles = glob.glob(os.path.join(dir, \"*.npz\"))\nfiles.sort()\nfor filename in files:\nprint(filename)\nnpz_data = np.load(filename)\nimages.append(resize_img(npz_data[\"images\"], (128, 128)))\nfinger_state = cos_interpolation(npz_data[\"finger_state\"])\n_joints = np.concatenate((npz_data[\"joints\"], finger_state), axis=-1)\njoints.append(_joints)\nseq_length.append(len(_joints))\nmax_seq = max(seq_length)\nimages = list_to_numpy(images, max_seq)\njoints = list_to_numpy(joints, max_seq)\nreturn images, joints\nif __name__ == \"__main__\":\n# dataset index\ntrain_list = [0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13]\ntest_list = [4, 9, 14, 15, 16]\n# load data\nimages, joints = load_data(\"./bag/\")\n# save images and joints\nnp.save(\"./data/train/images.npy\", images[train_list].astype(np.uint8))\nnp.save(\"./data/train/joints.npy\", joints[train_list].astype(np.float32))\nnp.save(\"./data/test/images.npy\", images[test_list].astype(np.uint8))\nnp.save(\"./data/test/joints.npy\", joints[test_list].astype(np.float32))\n# save joint bounds\njoint_bounds = calc_minmax(joints)\nnp.save(\"./data/joint_bounds.npy\", joint_bounds)\n</code></pre>"},{"location":"teach/dataset/#visualization","title":"Visualization","text":"<p>The following command saves the image and joint angles of the robot as a GIF animation. The <code>idx</code> argument represents the index of the data to visualize. The result will display the range of joint angles within the normalized range specified by the user, from [-0.92, 1.85] to [0.1, 0.9]. The figure below shows the actual GIF animation generated, with the camera image, robot joint angles, and normalized robot joint angles displayed from left to right. If the cropping or normalization range of the joint angles differs from the expected range, it is most likely that errors have occurred in the \"resize_img\" and \"calc_minmax\" processes described in the previous section.</p> <pre><code>$ python3 3_check_data.py --idx 4\nload test data, index number is 4\nJoint: shape=(5, 187, 8), min=-0.92, max=1.85\nNorm joint: shape=(5, 187, 8), min=0.1, max=0.9\n</code></pre> <p></p> [SOURCE] 3_check_data.py<pre><code>import argparse\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib.animation as anim\nfrom eipl.utils import normalization\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--idx\", type=int, default=0)\nargs = parser.parse_args()\nidx = int(args.idx)\njoints = np.load(\"./data/test/joints.npy\")\njoint_bounds = np.load(\"./data/joint_bounds.npy\")\nimages = np.load(\"./data/test/images.npy\")\nN = images.shape[1]\n# normalized joints\nminmax = [0.1, 0.9]\nnorm_joints = normalization(joints, joint_bounds, minmax)\n# print data information\nprint(\"load test data, index number is {}\".format(idx))\nprint(\"Joint: shape={}, min={:.3g}, max={:.3g}\".format(joints.shape, joints.min(), joints.max()))\nprint(\n\"Norm joint: shape={}, min={:.3g}, max={:.3g}\".format(\nnorm_joints.shape, norm_joints.min(), norm_joints.max()\n)\n)\n# plot images and normalized joints\nfig, ax = plt.subplots(1, 3, figsize=(14, 5), dpi=60)\ndef anim_update(i):\nfor j in range(3):\nax[j].cla()\n# plot image\nax[0].imshow(images[idx, i, :, :, ::-1])\nax[0].axis(\"off\")\nax[0].set_title(\"Image\")\n# plot joint angle\nax[1].set_ylim(-1.0, 2.0)\nax[1].set_xlim(0, N)\nax[1].plot(joints[idx], linestyle=\"dashed\", c=\"k\")\nfor joint_idx in range(8):\nax[1].plot(np.arange(i + 1), joints[idx, : i + 1, joint_idx])\nax[1].set_xlabel(\"Step\")\nax[1].set_title(\"Joint angles\")\n# plot normalized joint angle\nax[2].set_ylim(0.0, 1.0)\nax[2].set_xlim(0, N)\nax[2].plot(norm_joints[idx], linestyle=\"dashed\", c=\"k\")\nfor joint_idx in range(8):\nax[2].plot(np.arange(i + 1), norm_joints[idx, : i + 1, joint_idx])\nax[2].set_xlabel(\"Step\")\nax[2].set_title(\"Normalized joint angles\")\nani = anim.FuncAnimation(fig, anim_update, interval=int(N / 10), frames=N)\nani.save(\"./output/check_data_{}.gif\".format(idx))\n</code></pre>"},{"location":"teach/overview/","title":"Overview","text":"<p>This section provides instructions on how to create a dataset for deep predictive learning using robot sensor data collected by the ROS system. For better understanding, it is recommended to download the collected data and scripts (1.3GB) and follow the instructions to run them.</p>"},{"location":"teach/overview/#task","title":"Experimental Task","text":"<p>AIREC (AI-driven Robot for Embrace and Care), a humanoid robot developed by Tokyo Robotics, is used to teach object grasping. The figure below shows an overview of the task. The generalization performance is evaluated by comparing the object grasping experience at the teaching positions (three circled points) with the untaught positions (two points) shown in the figure. Training data are collected four times for each teaching position, for a total of 12 data points. Test data is collected once for each of the five positions, including the untaught positions, for a total of five data</p> <p></p>"},{"location":"teach/overview/#teaching","title":"Motion Teaching","text":"<p>AIREC is a robotic system that enables bilateral teleoperation, as shown below. The operator can teach a multi-degree of freedom robot more intuitively by instructing its motion based on the visual image of the robot displayed on the monitor and receiving force feedback from the robot. During the task teaching process using the teleoperation device, sensor information such as joint angles, camera images, torque information, etc. is stored in the \"rosbag\" format and a dataset is created for the deep predictive learning model in the following sections.</p> <p>Note that it is possible to teach motion to a robot without using such specialized equipment.  The Real Robot Application section describes two motion teaching methods using OpenManipulator: the leader-follower system and joystick control.</p> <p> <p></p>"},{"location":"teach/setup/","title":"Setup","text":""},{"location":"teach/setup/#ros","title":"ROS","text":"<p>In this section, we use the rospy and rosbag packages to extract data from <code>rosbag</code> files. If you are creating datasets in an environment with ROS installed, the following procedure is not necessary. Please skip to the next chapter.</p>"},{"location":"teach/setup/#pyenv","title":"pyenv","text":"<p>On the other hand, if you want to use rospy and other software on a PC without a ROS environment, you can use the rospypi/simple package. This package allows the use of binary packages such as rospy and tf2 without the need for a full ROS installation. Furthermore, since it is compatible with Linux, Windows, and MacOS, you can easily analyze the collected data in your own PC environment. To avoid conflicts with existing Python environments, it is recommended to create a virtual environment using venv. The following procedure outlines the steps to create an environment for the rospypi/simple library using venv.</p> <pre><code>$ python3 -m venv ~/.venv/rosbag\n$ source ~/.venv/rosbag/bin/activate\n$ pip install -U pip\n$ pip install --extra-index-url https://rospypi.github.io/simple/ rospy rosbag\n$ pip install matplotlib numpy opencv-python\n</code></pre> <p>Note</p> <p>The authors have not been able to confirm that the rospypi/simple library can handle all types of message data. In particular, custom ROS messages have not been tested. Therefore, if a program cannot be executed correctly in a virtual environment, it should be run in a ROS environment.</p>"},{"location":"tips/augmentation/","title":"Image Augmentation","text":""},{"location":"tips/augmentation/#transformsrandomaffine","title":"transforms.RandomAffine","text":"<p>transforms.RandomAffine is a function that applies a random affine transformation to an image. Affine transformations can translate, rotate, scale, or distort an image. The figure below shows the result of translating an image vertically and horizontally. When affine transforms are used during AutoEncoder training, they allow for the expression (extraction) of object position information as image features, allowing for appropriate reconstruction even for untaught positions.</p> <p></p>"},{"location":"tips/augmentation/#transformsrandomverticalflip","title":"transforms.RandomVerticalFlip","text":"<p>transforms.RandomVerticalFlip is a function that randomly flips the input image vertically to increase data diversity.</p> <p></p>"},{"location":"tips/augmentation/#transformsrandomhorizontalflip","title":"transforms.RandomHorizontalFlip","text":"<p>transforms.RandomHorizontalFlip is a function that randomly flips the input image horizontally and can be combined with <code>RandomVerticalFlip</code> to improve the generalization performance of the model.</p> <p></p>"},{"location":"tips/augmentation/#transformscolorjitter","title":"transforms.ColorJitter","text":"<p>transforms.ColorJitter is a function that applies random color transformations to an input image, allowing adjustments to its brightness, contrast, saturation, and hue. The figure below illustrates the effects of such transformations.</p> <p></p>"},{"location":"tips/augmentation/#gridmask","title":"GridMask","text":"<p>GridMask is a method used to increase the diversity of the training data by masking parts of the image with a grid-like pattern1. As shown in the figure below, this technique aims to improve the generalization performance of the model by training it on image data where certain parts are missing. When applied to a SARNN model, the missing parts of the image do not attract attention, allowing the model to learn spatial attention, which is crucial for motion prediction. The source code for GridMask is available here.</p> <p></p> <ol> <li> <p>Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Gridmask data augmentation. arXiv preprint arXiv:2001.04086, 2020.\u00a0\u21a9</p> </li> </ol>"},{"location":"tips/normalization/","title":"Pre-process","text":""},{"location":"tips/normalization/#joint_norm","title":"Joint Angle Normalization","text":"<p>Since the robot's camera image consists of integer values representing 256 shades (0-255 = 8 bits), it is necessary to normalize the values to fit within a specified range, such as [0.0, 1.0], instead of the original range of [0, 255]. On the other hand, the possible range of robot joint angles can vary depending on factors such as joint structure, range of motion, and teaching task. The simplest normalization method is to normalize the joint angles based on the maximum and minimum values of the training data. However, with this approach, the normalization range is affected by joints with large movements, making it difficult to learn fine movements accurately. To solve this problem, the joint angles are normalized based on the maximum and minimum values of each individual joint, allowing fine movements to be emphasized and learned more effectively.</p> <p>The figure below shows the results of joint angle normalization during object grasping. From left to right, the results show the normalization based on the maximum and minimum values of the collected raw data and training data (overall normalization), and the normalization based on the maximum and minimum values of each joint angle (joint normalization). In the case of overall normalization, the waveform after normalization shows minimal changes compared to the raw data, and the range of joint angles is simply converted to a scale from 0.0 to 1.0. However, when joint normalization is applied, both the coarse motion (e.g., represented by the gray waveform) and the fine motion (represented by the blue waveform) become more pronounced due to the normalization performed on each individual joint. This allows for more accurate learning of the robot's motions, surpassing the capabilities of global normalization alone.</p> <p></p> <p>Note</p> <p>Depending on the task, certain joints may have minimal or no motion. If joint normalization is applied to such joints, the resulting waveforms after normalization may be severely distorted, which can negatively affect the learning process. It is advisable to inspect the waveforms after normalization and manually adjust the normalization range if distorted joint waveforms are present. It is important to note that joint normalization is not suitable for inherently noisy data, such as torque and current values.</p>"},{"location":"tips/normalization/#cos-interpolation","title":"Cosine Interpolation","text":"<p>In cases where the model is learning data represented as ON/OFF states, such as robot hand open/close commands or signals from a Position Sensitive Detector (PSD) sensor, it may be beneficial to apply smoothing beforehand to facilitate learning. The following example shows the effect of applying smoothing using cosine interpolation to the original data (represented by a blue square wave). The degree of smoothness can be adjusted by modifying the <code>step_size</code> argument.</p> <p></p>"},{"location":"zoo/CAE-RNN/","title":"CAE-RNN","text":"<p>CAE-RNN is a motion generation model that consists of an image feature extraction part and a time series learning part, which aims to learn the robot's sensorimotor information1 2. The network structure of the CAE-RNN model is shown in the figure below. It consists of a Convolutional Auto-Encoder (CAE), which is responsible for extracting image features from the robot's visual information, and a Recurrent Neural Network (RNN), which learns the time-series information of the robot's joint angles and image features. CAE-RNN enables independent training of the image feature extraction part and the time series learning part, which are sequentially trained first with CAE and then with RNN. By learning a variety of sensorimotor information, CAE-RNN can extract image features such as the position and shape of flexible objects, which are traditionally difficult to recognize, and generate corresponding motions. This section provides a step-by-step explanation of the implementation, training, inference, and analysis of internal representation analysis.</p> <p></p>"},{"location":"zoo/CAE-RNN/#cae","title":"CAE","text":""},{"location":"zoo/CAE-RNN/#cae_overview","title":"Overview","text":"<p>Since visual images are high-dimensional compared to the robot's motion information, it is essential to align the dimensions of each modality to properly learn sensorimotor information. In addition, to capture the relationship between object position and motion, it is necessary to extract low-dimensional image features (such as position, color, shape, etc.) from the high-dimensional visual images. To achieve this, a Convolutional Auto-Encoder (CAE) is used to extract image features. The figure below focuses specifically on the CAE network structure within CAE-RNN, which consists of an encoder responsible for extracting image features from the robot's visual information ($i_t$) and a decoder that reconstructs the image ($\\hat i_t$) from the extracted features. By updating the weights of each layer to minimize the error between input and output values, the layer with the fewest number of neurons (known as the bottleneck layer) in the middle layer can abstractly represent the input information.</p> <p></p>"},{"location":"zoo/CAE-RNN/#cae_files","title":"Files","text":"<p>The programs and folders used in CAE are as follows:</p> <ul> <li>bin/train.py: This program is used to load data, train models, and save the trained models.</li> <li>bin/test.py: This program performs off-line inference of models using test data (images and joint angles) and visualizes the inference results.</li> <li>bin/extract.py\uff1aThis program calculates and stores the image features extracted by the CAE, as well as the upper and lower bounds for normalization.</li> <li>libs/trainer.py\uff1aThis is the backpropagation class for the CAE, responsible for training the model.</li> <li>log: This folder is used to store the weights, learning curves, and parameter information.</li> <li>output: This folder stores the inference results.</li> <li>data\uff1aThis folder stores the RNN training data, including joint angles, image features, and normalization information.</li> </ul>"},{"location":"zoo/CAE-RNN/#cae_model","title":"CAE Model","text":"<p>CAE consists of a convolution layer, a transposed convolution layer, and a linear layer. By using the convolutional layer (CNN) to extract image features, CAE can handle high-dimensional information with fewer parameters compared to AutoEncoder3, which only consists of a linear layer. In addition, CNN can extract different image features by convolving with shifting filters. The pooling layer, usually applied after CNN, is often used in image recognition and other fields to reduce the dimensionality of the input data. However, while it simultaneously achieves position invariance and information compression, there is a problem of losing spatial structure information in the image4. Since spatial position information of manipulated objects and robot hands is crucial for robot motion generation, dimensional compression is performed using the convolution stride (the interval at which the CNN filter is applied) instead of the pooling layer.</p> <p>The following program shows the CAE model capable of extracting image features with the specified dimension <code>feat_dim</code> from a 128x128 pixel color image. This model uses a simple network structure to help understand the overall architecture and implementation of CAE.</p> [SOURCE] BasicCAE.py<pre><code>class BasicCAE(nn.Module):\ndef __init__(self, feat_dim=10):\nsuper(BasicCAE, self).__init__()\n# encoder\nself.encoder = nn.Sequential(\nnn.Conv2d(3, 64, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(64, 32, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(32, 16, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(16, 12, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(12, 8, 3, 2, 1),\nnn.Tanh(),\nnn.Flatten(),\nnn.Linear(8 * 4 * 4, 50),\nnn.Tanh(),\nnn.Linear(50, feat_dim),\nnn.Tanh(),\n)\n# decoder\nself.decoder = nn.Sequential(\nnn.Linear(feat_dim, 50),\nnn.Tanh(),\nnn.Linear(50, 8 * 4 * 4),\nnn.Tanh(),\nnn.Unflatten(1, (8, 4, 4)),\nnn.ConvTranspose2d(8, 12, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(12, 16, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(16, 32, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(32, 64, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(64, 3, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\n)\ndef forward(self, x):\nreturn self.decoder(self.encoder(x))\n</code></pre> <p>By incorporating the <code>ReLU</code> function and <code>Batch Normalization</code>5, it is possible to increase the expressiveness of each layer, mitigate gradient loss, and improve the efficiency and stability of the learning process. This software library already contains CAE models that use <code>Batch Normalization</code>, which can be loaded as shown below. The difference between BasicCAENE and CAEBN lies in the structure of the model (parameter size), more details can be found in the source code. Please note that the implemented model expects input in the format of a 128x128 pixel color image. If you want to input images of different sizes, you will need to adjust the parameters accordingly.</p> <pre><code>from eipl.model import BasicCAENE, CAEBN\n</code></pre>"},{"location":"zoo/CAE-RNN/#cae_bp","title":"Back Propagation","text":"<p>During the CAE learning process, the robot's camera images ($i_t$) are used as input to generate reconstructed images ($\\hat i_t$). In this process, the model parameters are updated using the backpropagation method 6 to minimize the error between the input and reconstructed images. In lines 27-33 of the code, the batch of input images <code>xi</code> is passed through the model to obtain the reconstructed images <code>yi_hat</code>. The mean square error (nn.MSELoss) between the reconstructed images and the corresponding true images <code>yi</code> is then computed, and the error value <code>loss</code> is used to propagate the gradients through the model for parameter updates. This autoregressive learning approach eliminates the need for the complex manual design of image features typically required in conventional robotics. It is worth noting that data augmentation techniques, such as randomly adjusting brightness, contrast, and position, are used during training to extract image features that are robust to various real-world noise sources.</p> [SOURCE] trainer.py<pre><code>class Trainer:\ndef __init__(self, model, optimizer, device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, (xi, yi) in enumerate(data):\nxi = xi.to(self.device)\nyi = yi.to(self.device)\nyi_hat = self.model(xi)\nloss = nn.MSELoss()(yi_hat, yi)\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / n_batch\n</code></pre>"},{"location":"zoo/CAE-RNN/#training","title":"Training","text":"<p>We use the <code>Model</code>, the <code>Trainer Class</code>, and the pre-implemented main program <code>train.py</code> to train the CAE model. When the program is run, a folder with a name indicating the execution date and time of execution (e.g., 20230427_1316_29) is created in the <code>log</code> folder. This folder will contain the trained weights (pth) and the TensorBoard log file. The program allows command line arguments to specify essential training parameters, such as the model type, number of epochs, batch size, learning rate, and optimization method. The EarlyStopping library is also used to determine when to stop training early and to save the weights when the test error is minimized (<code>save_ckpt=True</code>). For a detailed explanation of how the program works, please refer to the comments inside the code.</p> <pre><code>$ cd eipl/zoo/cae/\n$ python3 ./bin/train.py\n[INFO] Set tag = 20230427_1316_29\n================================\nbatch_size : 128\ndevice : 0\nepoch : 100000\nfeat_dim : 10\nlog_dir : log/\nlr : 0.001\nmodel : CAE\noptimizer : adam\nstdev : 0.02\ntag : 20230427_1316_29\nvmax : 1.0\nvmin : 0.0\n================================\n0%|               | 11/100000 [00:40&lt;101:55:18,  3.67s/it, train_loss=0.0491, test_loss=0.0454]\n</code></pre>"},{"location":"zoo/CAE-RNN/#inference","title":"Inference","text":"<p>Use the <code>test.py</code> program to verify that the CAE has been trained correctly. The argument <code>filename</code> is the path to the file containing the trained weights, and <code>idx</code> is the index of the data to be visualized. The lower (top) figure shows the inference results of the CAEBN model using this program, with the input image on the left and the reconstructed image on the right. Since the robot hand and the grasping object are reconstructed in the \"untaught position\", which is important for the generation of the robot motion, it can be assumed that the image features represent information such as the object's position and shape. The lower figure (bottom) is also an example of failure, showing that the object is not adequately predicted by the basic CAE model with a simple network structure. In this case, it is necessary to adjust the method of the optimization algorithm, the learning rate, the loss function, and the structure of the model.</p> <pre><code>$ cd eipl/zoo/cae/\n$ python3 ./bin/test.py --filename ./log/20230424_1107_01/CAEBN.pth --idx 4\n$ ls output/\nCAEBN_20230424_1107_01_4.gif\n</code></pre> <p></p> <p></p>"},{"location":"zoo/CAE-RNN/#extract-image-features","title":"Extract Image Features","text":"<p>To prepare for time series learning of image features and robot joint angles with RNN, you need to extract the image features from CAE. Running the following program will save the image features and joint angles of the training and test data in the <code>data</code> folder in npy format. Make sure that the number of data and the length of the time series for the extracted image features and joint angles are the same. The joint angles are saved again to make it easier to load the dataset during RNN training.</p> <pre><code>$ cd eipl/zoo/cae/\n$ python3 ./bin/extract.py --filename ./log/20230424_1107_01/CAEBN.pth\n[INFO] train data\n==================================================\nShape of joints angle: torch.Size([12, 187, 8])\nShape of image feature: (12, 187, 10)\n==================================================\n[INFO] test data\n==================================================\nShape of joints angle: torch.Size([5, 187, 8])\nShape of image feature: (5, 187, 10)\n==================================================\n$ ls ./data/*\ndata/test:\nfeatures.npy  joints.npy\n\ndata/train:\nfeatures.npy  joints.npy\n</code></pre> <p>The following code snippet is part of the source code of <code>extract.py</code>, which performs the extraction and saving of the image features. In the fourth line, the encoder process of CAE is executed and the resulting low-dimensional image features are returned. The extracted image features from CAE are then normalized to fit within the range specified by the user and used for RNN training. When <code>tanh</code> is used as the activation function for the model, the upper and lower bounds of the image features (<code>feat_bounds</code>) remain constant (-1.0 to 1.0). However, for CAEBN, which uses <code>ReLU</code> as the activation function, the upper and lower bounds of the image features are indeterminate. Therefore, in line 25, the upper and lower bounds of the image features are determined by calculating the maximum and minimum values from the extracted image features of both the training and test data.</p> [SOURCE] extract.py<pre><code>    # extract image feature\nfeature_list = []\nfor i in range(N):\n_features = model.encoder(images[i])\nfeature_list.append( tensor2numpy(_features) )\nfeatures = np.array(feature_list)\nnp.save('./data/joint_bounds.npy', joint_bounds )\nnp.save('./data/{}/features.npy'.format(data_type), features )\nnp.save('./data/{}/joints.npy'.format(data_type), joints )\nprint_info('{} data'.format(data_type))\nprint(\"==================================================\")\nprint('Shape of joints angle:',  joints.shape)\nprint('Shape of image feature:', features.shape)\nprint(\"==================================================\")\nprint()\n# save features minmax bounds\nfeat_list = []\nfor data_type in ['train', 'test']:\nfeat_list.append( np.load('./data/{}/features.npy'.format(data_type) ) )\nfeat = np.vstack(feat_list)\nfeat_minmax = np.array( [feat.min(), feat.max()] )\nnp.save('./data/feat_bounds.npy', feat_minmax )\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn","title":"RNN","text":""},{"location":"zoo/CAE-RNN/#rnn_overview","title":"Overview","text":"<p>A Recurrent Neural Network (RNN) is used to integrate and learn the robot's sensorimotor information of the robot. The figure below shows the network structure of the RNN within CAE-RNN, which takes image features ($f_t$) and joint angles ($a_t$) at time $t$ and predicts them at the next time step $t+1$.</p> <p></p>"},{"location":"zoo/CAE-RNN/#rnn_files","title":"Files","text":"<p>The programs and folders used in RNN are as follows:</p> <ul> <li>bin/train.py: Program to load data, train and save models.</li> <li>bin/test.py: Program for offline inference of models using test data (images and joint angles) and visualization of inference results.</li> <li>bin/test_pca_cnnrnn.py: Program to visualize the internal state of an RNN using principal component analysis.</li> <li>libs/fullBPTT.py: Backpropagation class for time series learning.</li> <li>bin/rt_predict.py: Program that integrates the trained CAE and RNN models to predict motor commands based on images and joint angles.</li> <li>libs/dataloader.py: DataLoader for RNN providing image features and joint angles.</li> <li>log: Folder for storing weights, learning curves, and parameter information.</li> <li>output: Folder for storing inference results.</li> </ul>"},{"location":"zoo/CAE-RNN/#rnn_model","title":"RNN Model","text":"<p>RNN is a type of neural network that can learn and infer time series data, allowing sequential state changes based on input values for time series prediction. However, vanilla RNN is susceptible to gradient loss during backpropagation. To address this problem, Long Short-Term Memory (LSTM) and Multiple Timescales RNN (MTRNN) have been proposed.</p> <p>In this context, we describe a method for learning the integrated sensorimotor information of a robot using LSTM. LSTM includes three gates (input gate, forget gate, and output gate), each with its own weights and biases. The $h_{t-1}$ gate learns detailed changes in the time series as short-term memory, and the $c_{t-1}$ gate learns features of the entire time series as long-term memory. These gates allow the LSTM to remember and forget past information. The following example shows the implementation. The input value <code>x</code>, which is a combination of low-dimensional image features and robot joint angles previously extracted by CAE, is fed into the LSTM. LSTM then outputs the predicted value <code>y_hat</code> for the image features and robot joint angles at the next time step based on its internal state.</p> [SOURCE] BasicRNN.py<pre><code>class BasicLSTM(nn.Module):\ndef __init__(self, in_dim, rec_dim, out_dim, activation=\"tanh\"):\nsuper(BasicLSTM, self).__init__()\nif isinstance(activation, str):\nactivation = get_activation_fn(activation)\nself.rnn = nn.LSTMCell(in_dim, rec_dim)\nself.rnn_out = nn.Sequential(nn.Linear(rec_dim, out_dim), activation)\ndef forward(self, x, state=None):\nrnn_hid = self.rnn(x, state)\ny_hat = self.rnn_out(rnn_hid[0])\nreturn y_hat, rnn_hid\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_bptt","title":"Backpropagation Through Time","text":"<p>Backpropagation Through Time (BPTT) is used as the error backpropagation algorithm for time series learning. The details of BPTT have already been described in SARNN, please see here.</p> [SOURCE] fullBPTT.py<pre><code>class fullBPTTtrainer:\ndef __init__(self, model, optimizer, device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, (x, y) in enumerate(data):\nx = x.to(self.device)\ny = y.to(self.device)\nstate = None\ny_list = []\nT = x.shape[1]\nfor t in range(T - 1):\ny_hat, state = self.model(x[:, t], state)\ny_list.append(y_hat)\ny_hat = torch.permute(torch.stack(y_list), (1, 0, 2))\nloss = nn.MSELoss()(y_hat, y[:, 1:])\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / (n_batch + 1)\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_dataloader","title":"Dataloader","text":"<p>We describe the <code>DataLoader</code> for integrating the image features and robot joint angles extracted by CAE with RNN. As shown in lines 15 and 16, Gaussian noise is added to the input data. By training the model to minimize the error between the predicted values and the original data, the robot can generate appropriate motion commands even in the presence of real-world noise.</p> [SOURCE] dataloader.py<pre><code>class TimeSeriesDataSet(Dataset):\ndef __init__(self, feats, joints, minmax=[0.1, 0.9], stdev=0.02):\nself.stdev = stdev\nself.feats = torch.from_numpy(feats).float()\nself.joints = torch.from_numpy(joints).float()\ndef __len__(self):\nreturn len(self.feats)\ndef __getitem__(self, idx):\ny_feat = self.feats[idx]\ny_joint = self.joints[idx]\ny_data = torch.concat((y_feat, y_joint), axis=-1)\nx_feat = self.feats[idx] + torch.normal(mean=0, std=self.stdev, size=y_feat.shape)\nx_joint = self.joints[idx] + torch.normal(mean=0, std=self.stdev, size=y_joint.shape)\nx_data = torch.concat((x_feat, x_joint), axis=-1)\nreturn [x_data, y_data]\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_train","title":"Training","text":"<p>The main program, <code>train.py</code>, is used to train the RNN. When executing the program, the trained weights (pth) and Tensorboard log files are saved in the <code>log</code> folder. For a detailed description of how the program works, please refer to the comments within the code.</p> <pre><code>$ cd eipl/zoo/rnn/\n$ python3 ./bin/train.py --device -1\n[INFO] Set tag = 20230510_0134_03\n================================\nbatch_size : 5\ndevice : -1\nepoch : 100000\nlog_dir : log/\nlr : 0.001\nmodel : LSTM\noptimizer : adam\nrec_dim : 50\nstdev : 0.02\ntag : 20230510_0134_03\nvmax : 1.0\nvmin : 0.0\n================================\n0%|               | 99/100000 [00:25&lt;7:05:03,  3.92it/s, train_loss=0.00379, test_loss=0.00244\n</code></pre>"},{"location":"zoo/CAE-RNN/#rnn_inference","title":"Inference","text":"<p>To verify that the RNN is sufficiently trained, the test program <code>test.py</code> is used. The argument <code>filename</code> represents the path to the trained weights file, while <code>idx</code> corresponds to the index of the data to be visualized. To evaluate the generalization performance of the model, untrained position test data is provided and the predicted values are compared with the ground truth. The figure below illustrates the RNN prediction results, with the robot joint angles on the left and the image features on the right. The black dotted line represents the ground truth, while the colored line represents the predicted values. The close resemblance between them indicates that the motion learning process was performed effectively.</p> <pre><code>$ cd eipl/zoo/rnn/\n$ python3 ./bin/test.py --filename ./log/20230510_0134_03/LSTM.pth --idx 4\n$ ls output/\nLSTM_20230510_0134_03_4.gif\n</code></pre> <p></p>"},{"location":"zoo/CAE-RNN/#rnn_pca","title":"Principal Component Analysis","text":"<p>For an overview and concrete implementation of Principal Component Analysis (PCA), please refer to the link provided..</p> <pre><code>$ cd eipl/zoo/rnn/\n$ python3 ./bin/test_pca_rnn.py --filename log/20230510_0134_03/LSTM.pth\n$ ls output/\nPCA_LSTM_20230510_0134_03.gif\n</code></pre> <p>The following figure shows the visualization of the internal state of the RNN using PCA. Each dotted line represents the time evolution of the internal state of the RNN, with transitions starting from the black circle. These transition trajectories of the internal state are called attractors. The color assigned to each attractor indicates the object position, where blue, orange, and green correspond to the teaching positions A, C, and E, while red and purple correspond to the untaught positions B and D. The self-organization (alignment) of the attractors based on the object position suggests that behavior is learned and stored accordingly. In particular, the attractors at the untaught positions are generated between the teaching positions, allowing the generation of interpolated movements by repeatedly teaching and learning reaching movements with varying object positions.</p> <p></p> <ol> <li> <p>Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, and Tetsuya Ogata. Efficient multitask learning with an embodied predictive model for door opening and entry with whole-body control. Science Robotics, 7(65):eaax8177, 2022.\u00a0\u21a9</p> </li> <li> <p>Pin-Chu Yang, Kazuma Sasaki, Kanata Suzuki, Kei Kase, Shigeki Sugano, and Tetsuya Ogata. Repeatable folding task by humanoid robot worker using deep learning. IEEE Robotics and Automation Letters, 2(2):397\u2013403, 2016.\u00a0\u21a9</p> </li> <li> <p>Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.\u00a0\u21a9</p> </li> <li> <p>Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advances in neural information processing systems, 2017.\u00a0\u21a9</p> </li> <li> <p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448\u2013456. pmlr, 2015.\u00a0\u21a9</p> </li> <li> <p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533\u2013536, 1986.\u00a0\u21a9</p> </li> </ol>"},{"location":"zoo/CNNRNN/","title":"CNNRNN","text":"<p>Due to the independent training of the image feature extraction part (CAE) and the time series learning part (RNN), CAE-RNN has faced challenges in parameter adjustment and model training time. In addition, CAE extracts image features specifically designed for dimensional compression of image information, rather than features suitable for robot motion generation. To address these issues, CNN-RNN is introduced as a motion generation model that can automatically extract image features essential for motion generation by simultaneously learning (end-to-end learning) the image feature extraction part (CAE) and the time series learning part (RNN). This approach enables the robot to prioritize objects critical to the task and generate motions that are more robust to background changes compared to CAE-RNN1.</p> <p></p>"},{"location":"zoo/CNNRNN/#files","title":"Files","text":"<p>The following programs and folders are used in CNNRNN:</p> <ul> <li>bin/train.py: This program is used to load data, train models, and save the trained models.</li> <li>bin/test.py: This program performs offline inference using test data (images and joint angles) and visualizes the results of the inference.</li> <li>bin/test_pca_cnnrnn.py: This program visualizes the internal state of the RNN using Principal Component Analysis.</li> <li>libs/fullBPTT.py: This is a <code>backpropagation</code> class used for time series learning.</li> <li>log: This folder is used to store the weights, learning curves, and parameter information.</li> <li>output: This folder is used to store the results of the inference.</li> </ul>"},{"location":"zoo/CNNRNN/#model","title":"Model","text":"<p>CNNRNN is a motion generation model that can learn and perform inference on multimodal time series data. It predicts the image <code>y_image</code> and joint angle <code>y_joint</code> at the next time step $t+1$ based on the image <code>xi</code>, joint angle <code>xv</code>, and the state <code>state</code> at the time step $t$.</p> [SOURCE] CNNRNN.py<pre><code>class CNNRNN(nn.Module):\ndef __init__(self, rec_dim=50, joint_dim=8, feat_dim=10):\nsuper(CNNRNN, self).__init__()\n# Encoder\nself.encoder_image = nn.Sequential(\nnn.Conv2d(3, 64, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(64, 32, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(32, 16, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(16, 12, 3, 2, 1),\nnn.Tanh(),\nnn.Conv2d(12, 8, 3, 2, 1),\nnn.Tanh(),\nnn.Flatten(),\nnn.Linear(8 * 4 * 4, 50),\nnn.Tanh(),\nnn.Linear(50, feat_dim),\nnn.Tanh(),\n)\n# Recurrent\nrec_in = feat_dim + joint_dim\nself.rec = nn.LSTMCell(rec_in, rec_dim)\n# Decoder for joint angle\nself.decoder_joint = nn.Sequential(nn.Linear(rec_dim, joint_dim), nn.Tanh())\n# Decoder for image\nself.decoder_image = nn.Sequential(\nnn.Linear(rec_dim, 8 * 4 * 4),\nnn.Tanh(),\nnn.Unflatten(1, (8, 4, 4)),\nnn.ConvTranspose2d(8, 12, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(12, 16, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(16, 32, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(32, 64, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\nnn.ConvTranspose2d(64, 3, 3, 2, padding=1, output_padding=1),\nnn.Tanh(),\n)\ndef forward(self, xi, xv, state=None):\n# Encoder\nim_feat = self.encoder_image(xi)\nhid = torch.concat([im_feat, xv], -1)\n# Recurrent\nrnn_hid = self.rec(hid, state)\n# Decoder\ny_joint = self.decoder_joint(rnn_hid[0])\ny_image = self.decoder_image(rnn_hid[0])\nreturn y_image, y_joint, rnn_hid\n</code></pre>"},{"location":"zoo/CNNRNN/#bptt","title":"Backpropagation Through Time","text":"<p>Backpropagation Through Time (BPTT) is used as the error backpropagation algorithm for time series learning in CNNRNN. The detailed explanation of BPTT has already been provided in SARNN, please refer to that section for more information.</p> [SOURCE] fullBPTT.py<pre><code>class fullBPTTtrainer:\ndef __init__(self, model, optimizer, loss_weights=[1.0, 1.0], device=\"cpu\"):\nself.device = device\nself.optimizer = optimizer\nself.loss_weights = loss_weights\nself.model = model.to(self.device)\ndef save(self, epoch, loss, savename):\ntorch.save(\n{\n\"epoch\": epoch,\n\"model_state_dict\": self.model.state_dict(),\n\"train_loss\": loss[0],\n\"test_loss\": loss[1],\n},\nsavename,\n)\ndef process_epoch(self, data, training=True):\nif not training:\nself.model.eval()\ntotal_loss = 0.0\nfor n_batch, ((x_img, x_joint), (y_img, y_joint)) in enumerate(data):\nx_img = x_img.to(self.device)\ny_img = y_img.to(self.device)\nx_joint = x_joint.to(self.device)\ny_joint = y_joint.to(self.device)\nstate = None\nyi_list, yv_list = [], []\nT = x_img.shape[1]\nfor t in range(T - 1):\n_yi_hat, _yv_hat, state = self.model(x_img[:, t], x_joint[:, t], state)\nyi_list.append(_yi_hat)\nyv_list.append(_yv_hat)\nyi_hat = torch.permute(torch.stack(yi_list), (1, 0, 2, 3, 4))\nyv_hat = torch.permute(torch.stack(yv_list), (1, 0, 2))\nloss = self.loss_weights[0] * nn.MSELoss()(yi_hat, y_img[:, 1:]) \\\n                + self.loss_weights[1] * nn.MSELoss()(yv_hat, y_joint[:, 1:])\ntotal_loss += loss.item()\nif training:\nself.optimizer.zero_grad(set_to_none=True)\nloss.backward()\nself.optimizer.step()\nreturn total_loss / (n_batch + 1)\n</code></pre>"},{"location":"zoo/CNNRNN/#train","title":"Training","text":"<p>The main program, <code>train.py</code>, is used to train CNNRNN. When executing the program, the trained weights (pth) and Tensorboard log files are saved in the <code>log</code> folder. For a detailed understanding of the functionality of the program, please refer to the comments in the code.</p> <pre><code>$ cd eipl/zoo/cnnrnn/\n$ python3 ./bin/train.py\n[INFO] Set tag = 20230514_1958_07\n================================\nbatch_size : 5\ndevice : 0\nepoch : 100000\nfeat_dim : 10\nimg_loss : 1.0\njoint_loss : 1.0\nlog_dir : log/\nlr : 0.001\nmodel : CNNRNN\noptimizer : adam\nrec_dim : 50\nstdev : 0.02\ntag : 20230514_1958_07\nvmax : 1.0\nvmin : 0.0\n================================\n0%|               | 83/100000 [05:07&lt;99:16:42,  3.58s/it, train_loss=0.0213, test_loss=0.022\n</code></pre>"},{"location":"zoo/CNNRNN/#inference","title":"Inference","text":"<p>To ensure that CNNRNN has been trained correctly, you can use the test program <code>test.py</code>. The <code>filename</code> argument should be the path to the trained weights file, while <code>idx</code> is the index of the data you want to visualize. Additionally, <code>input_param</code> is the mixing coefficient for the inference. More details can be found in the provided documentation.</p> <pre><code>$ cd eipl/zoo/cnnrnn/\n$ python3 ./bin/test.py --filename ./log/20230514_1958_07/CNNRNN.pth --idx 4 --input_param 1.0\n\nimages shape:(187, 128, 128, 3), min=0, max=255\njoints shape:(187, 8), min=-0.8595600128173828, max=1.8292399644851685\nloop_ct:0, joint:[ 0.00226304 -0.7357931  -0.28175825  1.2895856   0.7252841   0.14539993\n-0.0266939   0.00422328]\nloop_ct:1, joint:[ 0.00307412 -0.73363686 -0.2815826   1.2874944   0.72176594  0.1542334\n-0.02719587  0.00325996]\n.\n.\n.\n\n$ ls ./output/\nCNNRNN_20230514_1958_07_4_1.0.gif\n</code></pre> <p>The figure below shows the results of inference at an untaught position. The images are presented from left to right, showing the input image, the predicted image, and the predicted joint angles (with dotted lines representing the true values). CNNRNN predicts the next time step based on the extracted image features and robot joint angles. The image features are expected to include information such as the color and position of the grasped object. It is also critical that the predicted image and robot joint angles are appropriately aligned. However, experimental results indicate that while the joint angles are accurately predicted, the predicted image consists only of the robot hand. Consequently, generating flexible movements based on object positions becomes challenging because the image features only contain information about the robot hand.</p> <p></p>"},{"location":"zoo/CNNRNN/#pca","title":"Principal Component Analysis","text":"<p>The figure below illustrates the visualization of the internal state of CNNRNN using Principal Component Analysis. Each dotted line represents the temporal evolution of CNNRNN's internal state, starting from the black circle. The color of each attractor corresponds to the object position, where blue, orange, and green represent teaching positions A, C, and E, while red and purple represent untaught positions B and D. The self-organization of attractors for each teaching position suggests the ability to generate well-learned movements at these positions. However, the attractors at the untaught positions are attracted to the attractors at the learning positions, making it impossible to generate interpolated movements. This occurs because the image features fail to extract the positional information of the grasped object.</p> <p></p>"},{"location":"zoo/CNNRNN/#improvement","title":"Model Improvement","text":"<p>In CAE-RNN, generalization performance was achieved by learning different object position information through data augmentation. In contrast, CNN-RNN learns images and joint angle information simultaneously, making it difficult to apply data augmentation to robot joint angles corresponding to changes in image position. Three potential solutions are proposed below to improve the position generalization performance of CNNRNNs.</p> <ol> <li> <p>Pre-training</p> <p>Only the CAE part of the CNNRNN is extracted and pre-trained. By learning only the image information using data augmentation, CAE can extract a variety of object position information. Then, end-to-end learning is performed using the pre-trained weights to map images to joint angles. However, since CAE needs to be pre-trained, the training time required is the same as that of CAE-RNN, resulting in minimal benefits of using CNN-RNN.</p> </li> <li> <p>Layer Normalization</p> <p>CAE-RNN used <code>BatchNormalization</code>2 as a normalization method to make CAE training stable and fast. However, BatchNormalization has the problems that learning becomes unstable when the batch of dataset is small and it is difficult to apply to recursive neural networks. Therefore, we will improve the generalization performance by using <code>Layer Normalization</code>3, which can train stably on small batches of data sets and time-series data.</p> <p>The figure below visualizes the internal state of CNNRNNLN: CNNRNN with Layer Normalization using principal component analysis. The self-organization (alignment) of attractors for each object position allows the robot to generate correct motion even at untaught positions.</p> <p></p> </li> <li> <p>Spatial Attention</p> <p>Since CAE-RNN and CNNRNN learn motion based on image features containing various information (position, color, shape, background, lighting conditions, etc.), robustness during motion generation has been a concern. To address this issue, we can improve robustness by incorporating a spatial attention mechanism that \"explicitly\" extracts spatial coordinates of important positions (target objects and arms) from images, thus improving the learning of spatial coordinates and robot joint angles. For more information on the spatial attention mechanism, see this link.</p> </li> </ol> <ol> <li> <p>Hiroshi Ito, Kenjiro Yamamoto, Hiroki Mori, Shuki Goto, and Tetsuya Ogata. Visualization of focal cues for visuomotor coordination by gradient-based methods: a recurrent neural network shifts the attention depending on task requirements. In 2020 IEEE/SICE International Symposium on System Integration (SII), 188\u2013194. IEEE, 2020.\u00a0\u21a9</p> </li> <li> <p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448\u2013456. pmlr, 2015.\u00a0\u21a9</p> </li> <li> <p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\u00a0\u21a9</p> </li> </ol>"},{"location":"zoo/MTRNN/","title":"Overview","text":"<p>MTRNN is a type of RNN consisting of a hierarchical group of neurons with different firing rates1. It consists of three layers: an input-output (IO) layer and context layers (Cf and Cs layers) with different firing rates (time constants), each with recursive inputs. The time constants increase from the Cf layer to the Cs layer, resulting in slower response speeds to the input. The input information is then passed through the Cf and Cs layers to the output layer. There is no direct connection between the IO and Cs layers, and their interaction occurs through the Cf layer. The MTRNN allows the robot to learn behaviors, where the Cf layer represents behavioral primitives and the Cs layer represents learning the combination of these primitives. Compared to LSTM, MTRNN is more interpretable and is widely used in our lab.</p> <p></p> <ol> <li> <p>Yuichi Yamashita and Jun Tani. Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment. PLoS computational biology, 4(11):e1000220, 2008.\u00a0\u21a9</p> </li> </ol>"},{"location":"zoo/MTRNN/#MTRNN.MTRNNCell","title":"<code>MTRNN.MTRNNCell</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Multiple Timescale RNN.</p> <p>Implements a form of Recurrent Neural Network (RNN) that operates with multiple timescales. This is based on the idea of hierarchical organization in human cognitive functions.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>fast_dim</code> <code>int</code> <p>Number of fast context neurons.</p> required <code>slow_dim</code> <code>int</code> <p>Number of slow context neurons.</p> required <code>fast_tau</code> <code>float</code> <p>Time constant value of fast context.</p> required <code>slow_tau</code> <code>float</code> <p>Time constant value of slow context.</p> required <code>activation</code> <code>string</code> <p>If you set <code>None</code>, no activation is applied (ie. \"linear\" activation: <code>a(x) = x</code>).</p> <code>'tanh'</code> <code>use_bias</code> <code>Boolean</code> <p>whether the layer uses a bias vector. The default is False.</p> <code>False</code> <code>use_pb</code> <code>Boolean</code> <p>whether the recurrent uses a pb vector. The default is False.</p> <code>False</code> <p>Yuichi Yamashita, Jun Tani, \"Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment.\", NeurIPS 2018. https://arxiv.org/abs/1807.03247v2</p> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>class MTRNNCell(nn.Module):\n#:: MTRNNCell\n\"\"\"Multiple Timescale RNN.\n    Implements a form of Recurrent Neural Network (RNN) that operates with multiple timescales.\n    This is based on the idea of hierarchical organization in human cognitive functions.\n    Arguments:\n        input_dim (int): Number of input features.\n        fast_dim (int): Number of fast context neurons.\n        slow_dim (int): Number of slow context neurons.\n        fast_tau (float): Time constant value of fast context.\n        slow_tau (float): Time constant value of slow context.\n        activation (string, optional): If you set `None`, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n        use_bias (Boolean, optional): whether the layer uses a bias vector. The default is False.\n        use_pb (Boolean, optional): whether the recurrent uses a pb vector. The default is False.\n    Yuichi Yamashita, Jun Tani,\n    \"Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment.\", NeurIPS 2018.\n    https://arxiv.org/abs/1807.03247v2\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nfast_dim,\nslow_dim,\nfast_tau,\nslow_tau,\nactivation=\"tanh\",\nuse_bias=False,\nuse_pb=False,\n):\nsuper(MTRNNCell, self).__init__()\nself.input_dim = input_dim\nself.fast_dim = fast_dim\nself.slow_dim = slow_dim\nself.fast_tau = fast_tau\nself.slow_tau = slow_tau\nself.use_bias = use_bias\nself.use_pb = use_pb\n# Legacy string support for activation function.\nif isinstance(activation, str):\nself.activation = get_activation_fn(activation)\nelse:\nself.activation = activation\n# Input Layers\nself.i2f = nn.Linear(input_dim, fast_dim, bias=use_bias)\n# Fast context layer\nself.f2f = nn.Linear(fast_dim, fast_dim, bias=False)\nself.f2s = nn.Linear(fast_dim, slow_dim, bias=use_bias)\n# Slow context layer\nself.s2s = nn.Linear(slow_dim, slow_dim, bias=False)\nself.s2f = nn.Linear(slow_dim, fast_dim, bias=use_bias)\ndef forward(self, x, state=None, pb=None):\n\"\"\"Forward propagation of the MTRNN.\n        Arguments:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n            state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n                   If None, initialize states to zeros.\n            pb (bool): pb vector. Used if self.use_pb is set to True.\n        Returns:\n            new_h_fast (torch.Tensor): Updated fast context state.\n            new_h_slow (torch.Tensor): Updated slow context state.\n            new_u_fast (torch.Tensor): Updated fast internal state.\n            new_u_slow (torch.Tensor): Updated slow internal state.\n        \"\"\"\nbatch_size = x.shape[0]\nif state is not None:\nprev_h_fast, prev_h_slow, prev_u_fast, prev_u_slow = state\nelse:\ndevice = x.device\nprev_h_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_h_slow = torch.zeros(batch_size, self.slow_dim).to(device)\nprev_u_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_u_slow = torch.zeros(batch_size, self.slow_dim).to(device)\n# Update of fast internal state\nnew_u_fast = (1.0 - 1.0 / self.fast_tau) * prev_u_fast + 1.0 / self.fast_tau * (\nself.i2f(x) + self.f2f(prev_h_fast) + self.s2f(prev_h_slow)\n)\n# Update of slow internal state\n_input_slow = self.f2s(prev_h_fast) + self.s2s(prev_h_slow)\nif pb is not None:\n_input_slow += pb\nnew_u_slow = (1.0 - 1.0 / self.slow_tau) * prev_u_slow + 1.0 / self.slow_tau * _input_slow\n# Compute the activation for both fast and slow context states\nnew_h_fast = self.activation(new_u_fast)\nnew_h_slow = self.activation(new_u_slow)\nreturn new_h_fast, new_h_slow, new_u_fast, new_u_slow\n</code></pre>"},{"location":"zoo/MTRNN/#MTRNN.MTRNNCell.forward","title":"<code>forward(x, state=None, pb=None)</code>","text":"<p>Forward propagation of the MTRNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <code>state</code> <code>list</code> <p>Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).    If None, initialize states to zeros.</p> <code>None</code> <code>pb</code> <code>bool</code> <p>pb vector. Used if self.use_pb is set to True.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_h_fast</code> <code>torch.Tensor</code> <p>Updated fast context state.</p> <code>new_h_slow</code> <code>torch.Tensor</code> <p>Updated slow context state.</p> <code>new_u_fast</code> <code>torch.Tensor</code> <p>Updated fast internal state.</p> <code>new_u_slow</code> <code>torch.Tensor</code> <p>Updated slow internal state.</p> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>def forward(self, x, state=None, pb=None):\n\"\"\"Forward propagation of the MTRNN.\n    Arguments:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n        state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n               If None, initialize states to zeros.\n        pb (bool): pb vector. Used if self.use_pb is set to True.\n    Returns:\n        new_h_fast (torch.Tensor): Updated fast context state.\n        new_h_slow (torch.Tensor): Updated slow context state.\n        new_u_fast (torch.Tensor): Updated fast internal state.\n        new_u_slow (torch.Tensor): Updated slow internal state.\n    \"\"\"\nbatch_size = x.shape[0]\nif state is not None:\nprev_h_fast, prev_h_slow, prev_u_fast, prev_u_slow = state\nelse:\ndevice = x.device\nprev_h_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_h_slow = torch.zeros(batch_size, self.slow_dim).to(device)\nprev_u_fast = torch.zeros(batch_size, self.fast_dim).to(device)\nprev_u_slow = torch.zeros(batch_size, self.slow_dim).to(device)\n# Update of fast internal state\nnew_u_fast = (1.0 - 1.0 / self.fast_tau) * prev_u_fast + 1.0 / self.fast_tau * (\nself.i2f(x) + self.f2f(prev_h_fast) + self.s2f(prev_h_slow)\n)\n# Update of slow internal state\n_input_slow = self.f2s(prev_h_fast) + self.s2s(prev_h_slow)\nif pb is not None:\n_input_slow += pb\nnew_u_slow = (1.0 - 1.0 / self.slow_tau) * prev_u_slow + 1.0 / self.slow_tau * _input_slow\n# Compute the activation for both fast and slow context states\nnew_h_fast = self.activation(new_u_fast)\nnew_h_slow = self.activation(new_u_slow)\nreturn new_h_fast, new_h_slow, new_u_fast, new_u_slow\n</code></pre>"},{"location":"zoo/MTRNN/#MTRNN.BasicMTRNN","title":"<code>MTRNN.BasicMTRNN</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>MTRNN Wrapper Module.</p> <p>This module encapsulates the MTRNNCell, adding an output layer to it.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Number of input features.</p> required <code>fast_dim</code> <code>int</code> <p>Number of fast context neurons.</p> required <code>slow_dim</code> <code>int</code> <p>Number of slow context neurons.</p> required <code>fast_tau</code> <code>float</code> <p>Time constant value of fast context.</p> required <code>slow_tau</code> <code>float</code> <p>Time constant value of slow context.</p> required <code>out_dim</code> <code>int</code> <p>Number of output features. If None, set equal to in_dim.</p> <code>None</code> <code>activation</code> <code>string</code> <p>If you set <code>None</code>, no activation is applied (ie. \"linear\" activation: <code>a(x) = x</code>).</p> <code>'tanh'</code> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>class BasicMTRNN(nn.Module):\n#:: BasicMTRNN\n\"\"\"MTRNN Wrapper Module.\n    This module encapsulates the MTRNNCell, adding an output layer to it.\n    Arguments:\n        in_dim (int):  Number of input features.\n        fast_dim (int): Number of fast context neurons.\n        slow_dim (int): Number of slow context neurons.\n        fast_tau (float): Time constant value of fast context.\n        slow_tau (float): Time constant value of slow context.\n        out_dim (int, optional): Number of output features. If None, set equal to in_dim.\n        activation (string, optional): If you set `None`, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n    \"\"\"\ndef __init__(\nself, in_dim, fast_dim, slow_dim, fast_tau, slow_tau, out_dim=None, activation=\"tanh\"\n):\nsuper(BasicMTRNN, self).__init__()\nif out_dim is None:\nout_dim = in_dim\n# Legacy string support for activation function.\nif isinstance(activation, str):\nactivation = get_activation_fn(activation)\nself.mtrnn = MTRNNCell(\nin_dim, fast_dim, slow_dim, fast_tau, slow_tau, activation=activation\n)\n# Output of RNN\nself.rnn_out = nn.Sequential(nn.Linear(fast_dim, out_dim), activation)\ndef forward(self, x, state=None):\n\"\"\"Forward propagation of the BasicMTRNN.\n        Arguments:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n            state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n                   If None, initialize states to zeros.\n        Returns:\n            y_hat (torch.Tensor): Output tensor of shape (batch_size, out_dim).\n            rnn_hid (list): Updated states (h_fast, h_slow, u_fast, u_slow).\n        \"\"\"\nrnn_hid = self.mtrnn(x, state)\ny_hat = self.rnn_out(rnn_hid[0])\nreturn y_hat, rnn_hid\n</code></pre>"},{"location":"zoo/MTRNN/#MTRNN.BasicMTRNN.forward","title":"<code>forward(x, state=None)</code>","text":"<p>Forward propagation of the BasicMTRNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <code>state</code> <code>list</code> <p>Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).    If None, initialize states to zeros.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_hat</code> <code>torch.Tensor</code> <p>Output tensor of shape (batch_size, out_dim).</p> <code>rnn_hid</code> <code>list</code> <p>Updated states (h_fast, h_slow, u_fast, u_slow).</p> Source code in <code>en/docs/zoo/src/MTRNN.py</code> <pre><code>def forward(self, x, state=None):\n\"\"\"Forward propagation of the BasicMTRNN.\n    Arguments:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n        state (list): Previous states (h_fast, h_slow, u_fast, u_slow), each of shape (batch_size, context_dim).\n               If None, initialize states to zeros.\n    Returns:\n        y_hat (torch.Tensor): Output tensor of shape (batch_size, out_dim).\n        rnn_hid (list): Updated states (h_fast, h_slow, u_fast, u_slow).\n    \"\"\"\nrnn_hid = self.mtrnn(x, state)\ny_hat = self.rnn_out(rnn_hid[0])\nreturn y_hat, rnn_hid\n</code></pre>"},{"location":"zoo/overview/","title":"Overview","text":"<p>The newly implemented layers and models will be released gradually.</p> <ul> <li> Multiple Timescale Recurrent Neural Network</li> <li> CAE-RNN</li> <li> CNNRNN</li> <li> SARNN with image feature (comming soon)</li> <li> Stochastic Recurrent Neural Network (comming soon)</li> </ul>"}]}